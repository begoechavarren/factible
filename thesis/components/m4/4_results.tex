%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CHAPTER 4: RESULTS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}
\label{sec:results}

This section presents the experimental evaluation of Factible across 30 YouTube videos spanning diverse topics including health, science, politics, and climate. The evaluation framework follows established practices from claim detection and fact-checking research \citep{hassan2015cikm, thorne2018fever}, combining ground truth comparison with LLM-as-judge quality assessments. Every run was constrained to five claims per video, a setting chosen after analyzing the precision-recall tradeoff across multiple configurations while balancing cost and latency constraints.

%-------------------------------------------------------
\subsection{Experimental Setup}
%-------------------------------------------------------

\subsubsection{Evaluation Dataset}

The evaluation corpus consists of 30 YouTube videos manually annotated with ground truth claims. While limited in size, this dataset provides sufficient diversity for system-level evaluation, and the sample size is comparable to evaluation scales used in end-to-end fact-checking systems; for example, ClaimBuster evaluated their system on 25 presidential debates \citep{hassan2017claimbuster}. Videos were selected across three thematic categories---climate, health, and political/social issues---with 10 videos per category to ensure balanced representation. Within each category, videos were equally split between factual content (5 videos) and misinformation (5 videos), allowing evaluation of the system's performance across different truth orientations. Videos range from educational science content to political commentary, representing diverse domains and claim densities. Table~\ref{tab:dataset-stats} summarizes the dataset characteristics.

\begin{table}[H]
\centering
\caption{Evaluation dataset statistics}
\label{tab:dataset-stats}
\begin{tabular}{lr}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Total videos & 30 \\
Ground truth claims per video (mean) & 16.8 \\
Ground truth claims per video (range) & 9--35 \\
Total ground truth claims & 503 \\
System claims extracted per video & 5 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Ground Truth Annotation}

Ground truth annotations were created following a structured protocol. For each video, all factual, verifiable claims from the transcript were manually annotated. Each claim was annotated with an importance score (0.0--1.0) reflecting its centrality to the video's main argument, and a verdict label indicating the expected verification outcome (SUPPORTS, REFUTES, MIXED, or UNCLEAR). The annotation process followed guidelines from ClaimBuster's check-worthiness criteria \citep{hassan2015cikm}, prioritizing claims that are specific, verifiable, and consequential. The evaluation dataset summary, including the list of evaluated videos, annotation guidelines, and aggregate statistics, is provided in Appendix~\ref{appendix:ground-truth}. The complete claim-level annotations are available in the project repository.

\subsubsection{Evaluation Metrics}

The evaluation employs metrics at two levels: claim extraction quality and verdict accuracy.

\paragraph{Claim Alignment via Semantic Similarity}

System claims are matched to ground truth using sentence-transformer embeddings (all-MiniLM-L6-v2). Claims whose cosine similarity exceeds 0.7 count as matches; others become false positives or false negatives. A greedy pass ensures each ground truth claim pairs with at most one system claim. This semantic alignment yields the true positives needed for precision, recall, F1, and MAP calculations.

\textbf{Claim Extraction Metrics:}
\begin{itemize}
    \item \textbf{Precision@k}: Proportion of extracted claims matching any ground truth claim, using semantic similarity matching with a threshold of 0.7. This metric follows the standard information retrieval formulation used in claim detection systems \citep{hassan2017kdd}.
    \item \textbf{Recall@k}: Proportion of ground truth claims matched by the top-$k$ extracted claims \citep{hassan2017kdd}.
    \item \textbf{F1 Score}: Harmonic mean of precision and recall.
    \item \textbf{Mean Average Precision (MAP)}: Ranking quality metric from information retrieval, measuring whether important claims appear early in the extraction order \citep{hassan2017kdd}.
    \item \textbf{Recall@Important}: Recall specifically for high-importance claims (importance $\geq 0.80$).
    \item \textbf{Importance-Weighted Coverage}: Percentage of total ground truth importance mass captured by matched claims.
\end{itemize}

\begin{itemize}
    \item \textbf{Verdict Stance Accuracy}: Classification accuracy for the four-class stance problem (SUPPORTS, REFUTES, MIXED, UNCLEAR), measuring the percentage of verdicts matching ground truth stance.
\end{itemize}

\textbf{System Efficiency Metrics:}
\begin{itemize}
    \item \textbf{Latency}: End-to-end processing time per video.
    \item \textbf{Evidence Retrieval Rate}: Proportion of queries successfully retrieving evidence.
    \item \textbf{Source Reliability}: Distribution of source reliability ratings across retrieved evidence.
\end{itemize}

\subsubsection{Component Evaluation Scope}

The evaluation focuses on components where the system makes reasoning decisions that can be compared against ground truth. Specifically, the evaluation covers \textbf{Claim Extraction} (precision, recall, importance ranking) and \textbf{Verdict Generation} (stance accuracy, explanation quality) as these components employ LLM-based reasoning that can produce varying results.

Components relying on external APIs---\textbf{Transcript Extraction} (YouTube Transcript API) and \textbf{Online Search} (Serper/Google)---are not evaluated directly, as their performance depends on third-party services rather than the system's design. However, their effectiveness is implicitly covered by the end-to-end evaluation: if transcript extraction fails, no claims can be extracted; if search fails, verdict accuracy degrades. The \textbf{Query Generator} component is assessed indirectly through evidence retrieval success rates, as effective queries should yield relevant evidence.

%-------------------------------------------------------
\subsection{Precision-Recall Tradeoff Analysis}
%-------------------------------------------------------

Before presenting detailed results, this section analyzes the precision-recall tradeoff to justify the choice of the configuration of maximum claims to fetch to be 5 (\texttt{max\_claims=5}) as the primary configuration. The system was evaluated across six configurations with \texttt{max\_claims} $\in \{1, 3, 5, 7, 10, 15\}$.

\begin{table}[H]
\centering
\caption[Precision-recall tradeoff]{Precision-recall tradeoff across different \texttt{max\_claims} configurations}
\label{tab:pr-tradeoff}
\begin{tabular}{ccccc}
\toprule
\textbf{max\_claims} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{MAP} \\
\midrule
1 & 0.800 & 0.052 & 0.098 & 0.800 \\
3 & 0.822 & 0.159 & 0.264 & 0.897 \\
\textbf{5} & \textbf{0.813} & \textbf{0.262} & \textbf{0.390} & \textbf{0.870} \\
7 & 0.795 & 0.359 & 0.486 & 0.862 \\
10 & 0.769 & 0.471 & 0.573 & 0.854 \\
15 & 0.719 & 0.570 & 0.623 & 0.865 \\
\bottomrule
\end{tabular}
\end{table}

Figure~\ref{fig:pr-curve} illustrates the precision-recall tradeoff. As \texttt{max\_claims} increases, recall improves from 5.2\% to 57.0\%, while precision decreases from 82.2\% to 71.9\%. The slight precision increase from $k=1$ (80.0\%) to $k=3$ (82.2\%) indicates that the system benefits from extracting multiple high-confidence claims rather than being forced to select exactly one. Beyond $k=5$, the classic precision-recall tradeoff becomes pronounced.

\begin{figure}[H]
\centering
\includegraphics[width=0.75\textwidth]{figs/precision_recall_curve.png}
\caption[Precision-recall curve for claim extraction]{Precision-recall curve for different \texttt{max\_claims} values. The selected operating point ($k=5$) achieves 81.3\% precision at 26.2\% recall, balancing claim quality with coverage.}
\label{fig:pr-curve}
\end{figure}

The configuration \texttt{max\_claims=5} was selected as the primary operating point because it maintains high precision (81.3\%) while achieving reasonable recall (26.2\%), achieves a strong MAP score (0.870) indicating good ranking quality, and provides favorable cost and latency characteristics. Beyond \texttt{max\_claims=5}, processing time and API costs increase linearly with claim count, while precision degrades. For a fact-checking application where user trust depends on accuracy and responsiveness, balancing precision, cost, and latency is critical---presenting 5 high-quality claims efficiently is more valuable than presenting 15 claims with higher false positive rates, increased costs, and longer wait times.

%-------------------------------------------------------
\subsection{Claim Extraction Performance}
%-------------------------------------------------------

Table~\ref{tab:claim-extraction-results} presents the claim extraction results at the primary configuration (\texttt{max\_claims=5}).

\begin{table}[H]
\centering
\caption[Claim extraction performance]{Claim extraction performance (n=30 videos, \texttt{max\_claims}=5)}
\label{tab:claim-extraction-results}
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{Mean} & \textbf{Std Dev} \\
\midrule
Precision@5 & 0.813 & 0.171 \\
Recall & 0.262 & 0.088 \\
F1 Score & 0.390 & 0.111 \\
Mean Average Precision (MAP) & 0.870 & 0.146 \\
Recall@Important ($\geq$0.80) & 0.395 & 0.178 \\
Importance-Weighted Coverage & 0.292 & 0.096 \\
Importance MAE & 0.126 & 0.060 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Interpretation of Results}

The system achieves 81.3\% precision, meaning that approximately 4 out of 5 extracted claims on average match ground truth claims. This high precision indicates that the claim extractor successfully identifies legitimate factual claims rather than extracting irrelevant or fabricated statements.

The overall recall of 26.2\% reflects the constraint of extracting only 5 claims from videos averaging 16.8 ground truth claims. However, the Recall@Important metric (39.5\%) demonstrates that the system prioritizes high-importance claims---capturing nearly 40\% of the most critical claims while only extracting approximately 30\% of the total claim set. This selective extraction behavior aligns with the design goal of importance-based ranking, where claims are prioritized by their impact on the video's central argument.

The MAP score of 0.870 indicates strong ranking quality: important claims consistently appear early in the extraction order. This metric, inspired by ClaimBuster's evaluation framework \citep{hassan2017kdd}, validates that the importance scoring mechanism effectively prioritizes claims.

The importance MAE of 0.126 (on a 0--1 scale) shows that system-assigned importance scores closely approximate human judgments, with typical errors of approximately one importance tier (e.g., scoring a claim 0.7 when ground truth is 0.85).

The 26.2\% recall, while appearing low in isolation, must be interpreted in context. Given that videos contain an average of 16.8 checkable claims and the system extracts 5, a theoretical maximum recall of approximately 30\% exists under this constraint. The achieved recall of 26.2\% therefore represents strong performance relative to the configuration limit. Furthermore, the importance-weighted coverage of 29.2\% indicates that the system captures nearly one-third of the total ``importance mass'' of ground truth claims. For practical fact-checking applications where user attention is limited, presenting 5 high-quality, important claims provides more value than exhaustive but overwhelming coverage.

%-------------------------------------------------------
\subsection{Verdict Generation Performance}
%-------------------------------------------------------

Table~\ref{tab:verdict-results} presents the verdict accuracy results.

\begin{table}[H]
\centering
\caption[Verdict generation performance]{Verdict generation performance (n=30 videos)}
\label{tab:verdict-results}
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{Mean} & \textbf{Std Dev} \\
\midrule
Stance Accuracy & 73.0\% & 33.4\% \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Accuracy Distribution Analysis}

The standard deviation (0.334) in verdict accuracy is notable. Analysis of per-video results reveals a distribution skewed toward high accuracy:

\begin{itemize}
    \item \textbf{21 videos (70.0\%)} achieved high accuracy ($\geq$75\%)---the majority of extracted claims were correctly classified.
    \item \textbf{5 videos (16.7\%)} achieved medium accuracy (25--74\%)---partial verdict correctness.
    \item \textbf{4 videos (13.3\%)} achieved low accuracy ($<$25\%)---most claims were incorrectly classified.
\end{itemize}

The stance accuracy of 73.0\% substantially exceeds a random baseline: for a four-class classification problem, random guessing would achieve approximately 25\% accuracy, making this a 2.93$\times$ improvement. Moreover, unlike random classification, the system provides evidence-backed explanations that enable users to evaluate the verdict's reasoning.

With an evidence retrieval success rate of 94.7\%, the system consistently finds relevant sources for most claims. Table~\ref{tab:accuracy-retrieval-correlation} presents representative examples illustrating how verdict accuracy varies across the dataset.

\begin{table}[H]
\centering
\caption[Verdict accuracy distribution]{Verdict accuracy distribution with evidence retrieval rates (representative examples)}
\label{tab:accuracy-retrieval-correlation}
\small
\begin{tabular}{p{4.5cm}ccc}
\toprule
\textbf{Video Topic} & \textbf{Retrieval Rate} & \textbf{Verdict Acc.} & \textbf{Avg Sources} \\
\midrule
\multicolumn{4}{l}{\textit{High accuracy ($\geq$75\%) --- 21 videos total, 4 examples shown}} \\
\quad Brain Benefits of Exercise (TED) & 100\% & 100\% & 2.5 \\
\quad Climate Change (NatGeo) & 100\% & 100\% & 2.3 \\
\quad Fossil Fuels & 100\% & 100\% & 2.7 \\
\quad UK Election Results & 100\% & 75\% & 2.8 \\
\midrule
\multicolumn{4}{l}{\textit{Medium/Low accuracy ($<$75\%) --- 9 videos total, 4 examples shown}} \\
\quad Gender Wage Gap & 100\% & 50\% & 2.7 \\
\quad FBI \& January 6th & 100\% & 50\% & 2.3 \\
\quad Inflation Explainer & 60\% & 25\% & 1.8 \\
\quad A Nation of Immigrants & 100\% & 0\% & 2.9 \\
\bottomrule
\end{tabular}
\end{table}

Notably, successful evidence retrieval does not guarantee high verdict accuracy. Some politically contentious topics achieve 100\% retrieval but lower verdict accuracy, suggesting that the challenge lies not in finding evidence but in correctly synthesizing conflicting sources or matching the ground truth annotator's interpretation.

Analysis of low-accuracy videos reveals several contributing factors: (1) \textbf{contested claims}---topics with legitimate disagreement may have evidence supporting multiple stances, making definitive verdicts challenging; (2) \textbf{ground truth subjectivity}---some claims involve nuanced interpretations where reasonable annotators might disagree; and (3) \textbf{evidence-claim mismatch}---retrieved evidence may address related but not identical claims. These findings suggest that further improvements require enhanced evidence synthesis and more sophisticated handling of contested claims, rather than simply improving retrieval success.

\subsubsection{Confusion Matrix Analysis}

Figure~\ref{fig:confusion-matrix} presents the confusion matrix for verdict classification across all 122 matched claim-verdict pairs.

\begin{figure}[H]
\centering
\includegraphics[width=0.65\textwidth]{figs/confusion_matrix.png}
\caption[Verdict classification confusion matrix]{Confusion matrix for verdict stance classification across all 122 matched claim-verdict pairs. Rows represent ground truth stances; columns represent predicted stances.}
\label{fig:confusion-matrix}
\end{figure}

The class distribution in the evaluation dataset reflects the nature of real-world misinformation: SUPPORTS claims (69) outnumber REFUTES (12), MIXED (35), and UNCLEAR (6). This imbalance is not a flaw in the dataset but rather reflects how misinformation operates---even videos containing false claims typically include many factually accurate statements, which contributes to their persuasiveness and makes them harder to debunk. A video promoting health misinformation, for instance, may correctly cite scientific terminology or reference real institutions while drawing false conclusions. Similarly, the substantial proportion of MIXED verdicts (29\%) reflects claims where credible evidence exists on both sides; rather than forcing a binary true/false judgment, the system surfaces this ambiguity, encouraging users to examine the sources and draw their own informed conclusions.

The confusion matrix reveals the following patterns:

\begin{itemize}
    \item \textbf{SUPPORTS} (75.4\% recall): Errors occur when the system assigns MIXED or UNCLEAR to claims that have clear supporting evidence.

    \item \textbf{REFUTES} (83.3\% recall): The highest per-class performance demonstrates that the system reliably identifies contradicting evidence---a critical capability for fact-checking.

    \item \textbf{MIXED} (74.3\% recall): This category is inherently challenging as it requires recognizing that evidence presents both supporting and refuting elements simultaneously.

    \item \textbf{UNCLEAR} (16.7\% recall): The system tends toward definitive verdicts rather than acknowledging insufficient evidence. While this represents a limitation, the small sample size (n=6) limits the statistical reliability of this observation. The bias toward definitive verdicts reflects a design trade-off: users typically seek actionable conclusions rather than ``insufficient evidence'' responses.
\end{itemize}

The diagonal dominance confirms that the system performs substantially above the 25\% random baseline across all classes. The strong REFUTES performance is particularly noteworthy, as identifying false claims is the primary goal of fact-checking systems.

%-------------------------------------------------------
\subsection{Evidence Retrieval Performance}
%-------------------------------------------------------

\subsubsection{Retrieval Success}

Table~\ref{tab:evidence-results} summarizes evidence retrieval performance.

\begin{table}[H]
\centering
\caption[Evidence retrieval performance]{Evidence retrieval performance (n=30 videos)}
\label{tab:evidence-results}
\begin{tabular}{lr}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Evidence retrieval success rate & 94.7\% \\
Average sources per query & 1.52 \\
Average evidence items per claim & 2.41 \\
\bottomrule
\end{tabular}
\end{table}

The evidence retrieval success rate of 94.7\% indicates that the vast majority of search queries return usable evidence. When evidence is retrieved, claims receive an average of 2.41 evidence items, providing multiple perspectives for verdict synthesis.

\subsubsection{Source Reliability Distribution}

A critical aspect of fact-checking is source quality. Table~\ref{tab:source-reliability} presents the distribution of source reliability ratings across all retrieved evidence.

\begin{table}[H]
\centering
\caption[Source reliability distribution]{Source reliability distribution (n=724 total sources)}
\label{tab:source-reliability}
\begin{tabular}{lrr}
\toprule
\textbf{Reliability Rating} & \textbf{Count} & \textbf{Percentage} \\
\midrule
High & 605 & 83.6\% \\
Medium & 110 & 15.2\% \\
Low & 0 & 0.0\% \\
Unknown & 9 & 1.2\% \\
\bottomrule
\end{tabular}
\end{table}

The overwhelming majority of retrieved sources (83.6\%) receive high reliability ratings from the Media Bias/Fact Check-based assessment system \citep{mbfc2024methodology}. No sources received low reliability ratings, and only 1.2\% were classified as unknown (typically due to missing MBFC data for niche domains). This distribution reflects both the reliability scoring heuristics and the retry behavior that fetches additional results whenever the initial batch skews toward low-reliability domains.

%-------------------------------------------------------
\subsection{System Efficiency}
%-------------------------------------------------------

\subsubsection{Processing Latency}

Table~\ref{tab:latency-results} presents processing time statistics.

\begin{table}[H]
\centering
\caption[System latency]{System latency (n=30 videos)}
\label{tab:latency-results}
\begin{tabular}{lr}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Mean latency & 82.2 seconds \\
Standard deviation & 31.8 seconds \\
Minimum latency & 41.9 seconds \\
Maximum latency & 178.3 seconds \\
Total processing time (30 videos) & 41.1 minutes \\
\bottomrule
\end{tabular}
\end{table}

The mean processing time of 82.2 seconds per video enables near-interactive use for individual videos. The variance (standard deviation 31.8s) reflects multiple contributing factors:

\begin{itemize}
    \item \textbf{Video length}: Longer transcripts require more LLM tokens for claim extraction, increasing inference time.
    \item \textbf{Pipeline parameters}: The configuration parameters \texttt{max\_claims}, \texttt{max\_queries}, and \texttt{max\_results\_per\_query} directly multiply the number of downstream operations. With the evaluation configuration ($\text{max\_claims}=5$, $\text{max\_queries}=3$, $\text{max\_results}=3$), each video triggers up to $5 \times 3 \times 3 = 45$ evidence extraction operations.
    \item \textbf{Evidence retrieval success}: Videos with failed evidence retrieval complete faster (fewer web requests), while videos requiring multiple successful searches experience longer latencies.
    \item \textbf{Web scraping variability}: JavaScript-heavy sites require longer Selenium wait times, and some domains respond slower than others.
\end{itemize}

\subsubsection{Cost Analysis}

All experiments used GPT-4o-mini for LLM inference at current pricing (\$0.15 per million input tokens, \$0.60 per million output tokens). Across the 30-video evaluation corpus, the average cost per video was \$0.003, with individual videos ranging from \$0.0008 to \$0.0164 depending on transcript length, claim complexity, and evidence retrieval needs. The total cost for processing all 30 videos was \$0.09, demonstrating the practical affordability of the system for individual users. This cost-effectiveness contrasts with concerns about LLM deployment costs noted in prior work \citep{factagent2025}, showing that fact-checking systems can achieve meaningful accuracy at minimal expense when using appropriately-sized models.

%-------------------------------------------------------
\subsection{Qualitative Analysis}
%-------------------------------------------------------

\subsubsection{Successful Extraction Examples}

Table~\ref{tab:qualitative-success} presents examples of successful claim extractions demonstrating semantic matching between ground truth and system-extracted claims.

\begin{table}[H]
\centering
\caption{Examples of successful claim extraction with semantic matching}
\label{tab:qualitative-success}
\small
\begin{tabular}{p{6cm}p{6cm}c}
\toprule
\textbf{Ground Truth Claim} & \textbf{System-Extracted Claim} & \textbf{Imp.} \\
\midrule
A single workout immediately increases levels of neurotransmitters like dopamine, serotonin, and noradrenaline & A single workout increases levels of neurotransmitters like dopamine, serotonin, and noradrenaline & 0.9 \\
\midrule
HIV infects one of the immune cells that is central to the body's response to pathogens---the helper T-cell & HIV infects helper T-cells, which are central to the immune response & 0.95 \\
\midrule
Scientists at UCT have uncovered garlic's cancer fighting properties & Scientists at UCT uncovered garlic's cancer-fighting properties & 0.95 \\
\bottomrule
\end{tabular}
\end{table}

These examples demonstrate that the system successfully extracts claims while allowing minor paraphrasing and condensation. The semantic similarity matching correctly identifies these as equivalent claims despite surface-level textual differences.

\subsubsection{Error Analysis: Verdict Failures}

Analysis of verdict errors reveals systematic patterns. With high evidence retrieval success (94.7\%), the primary failure modes involve stance misclassification and handling nuanced claims where conflicting evidence requires domain expertise to synthesize correctly. Table~\ref{tab:verdict-errors} categorizes the primary error types.

\begin{table}[H]
\centering
\caption{Verdict error categories}
\label{tab:verdict-errors}
\begin{tabular}{lp{8cm}}
\toprule
\textbf{Error Type} & \textbf{Description} \\
\midrule
Evidence retrieval failure & No evidence retrieved; system defaults to UNCLEAR \\
Stance misclassification & Evidence retrieved but stance incorrectly assessed (e.g., MIXED classified as REFUTES) \\
Nuanced claims & Claims requiring domain expertise to evaluate mixed evidence \\
\bottomrule
\end{tabular}
\end{table}

Error analysis reveals that evidence retrieval success does not guarantee verdict accuracy. Two of the four lowest-accuracy videos achieved 100\% evidence retrieval but 0\% verdict accuracy, indicating that the challenge lies in evidence synthesis rather than retrieval. These cases involve politically contested claims where ground truth stances require nuanced interpretation of conflicting sources.

\subsubsection{Analysis of Low-Accuracy Cases}

Only 4 videos (13.3\%) achieved less than 25\% verdict accuracy. Detailed analysis reveals their characteristics:

\begin{table}[H]
\centering
\caption[Low-accuracy video analysis]{Low-accuracy video analysis ($<$25\% verdict accuracy)}
\label{tab:low-accuracy-videos}
\small
\begin{tabular}{p{3.5cm}lccl}
\toprule
\textbf{Video Topic} & \textbf{Type} & \textbf{Retrieval} & \textbf{Accuracy} & \textbf{Likely Cause} \\
\midrule
Trump's Historic National Emergency & Factual & 60\% & 0\% & Contested political claims \\
A Nation of Immigrants & Misinfo & 100\% & 0\% & Conflicting sources \\
Juice vs. Whole Fruit & Factual & 20\% & 20\% & Limited evidence retrieval \\
Sleep \& Teenage Brain & Factual & 100\% & 20\% & Nuanced scientific claims \\
\bottomrule
\end{tabular}
\end{table}

Key observations:
\begin{itemize}
    \item \textbf{High retrieval does not guarantee accuracy:} Two videos achieved 100\% retrieval but 0--20\% accuracy, confirming that evidence synthesis is the bottleneck for contested claims.
    \item \textbf{No single category dominates:} Low-accuracy videos span both political (2) and health (2) topics, and include both factual content (3) and misinformation (1).
    \item \textbf{Contested claims are hardest:} The common thread is claims where reasonable sources disagree or where ground truth requires nuanced interpretation.
\end{itemize}

\textbf{Successful categories:} Videos on scientific explanations (e.g., climate science, exercise benefits), health misinformation debunking (e.g., fluoride claims, detox myths), and conspiracy content (e.g., chemtrails, geoengineering) achieved high accuracy, demonstrating the system's effectiveness when evidence clearly supports or refutes claims.

%-------------------------------------------------------
\subsection{Considerations for Generative AI Systems}
%-------------------------------------------------------

It is important to contextualize these results within the unique characteristics of generative AI systems. Unlike traditional machine learning models with deterministic outputs, LLM-based systems introduce inherent variability that affects evaluation interpretation.

\subsubsection{Non-Determinism in LLM Systems}

Despite configuring all LLM calls with \texttt{temperature=0.0} to minimize output variability, complete determinism is not guaranteed. Even with zero temperature, LLM outputs may vary across runs due to:

\begin{itemize}
    \item \textbf{Floating-point precision}: GPU computation introduces subtle numerical variations that can cascade through token selection.
    \item \textbf{Model updates}: Cloud-hosted models (e.g., GPT-4o-mini) may be silently updated by providers, affecting outputs over time.
    \item \textbf{Batching effects}: Different batch sizes or concurrent requests may influence internal state.
\end{itemize}

This inherent non-determinism means that exact reproduction of results is challenging, though setting temperature to zero substantially reduces variability compared to default settings.

\subsubsection{External Dependencies and Temporal Sensitivity}

Beyond LLM variability, the system's reliance on external web search introduces additional sources of result variability:

\begin{itemize}
    \item \textbf{Search result volatility}: Web search results change over time as new content is indexed and rankings evolve.
    \item \textbf{Content availability}: Websites may become unavailable, paywalled, or block automated access.
    \item \textbf{Rate limiting}: Search APIs may throttle requests, causing some queries to fail during high-load evaluation runs.
\end{itemize}

These factors contribute to verdict accuracy variance: search results may differ between runs, and a claim that retrieved limited evidence in one execution might find more sources in another.

\subsubsection{Implications for Metric Interpretation}

Unlike traditional classification tasks where metrics are stable given fixed test data, LLM-based systems produce metrics with inherent variance. When evaluating generative AI systems, researchers should consider:

\begin{itemize}
    \item \textbf{Expected variability}: Metrics may vary by several percentage points across identical evaluation runs.
    \item \textbf{Qualitative validation}: Beyond aggregate metrics, examining individual outputs provides crucial insight into system behavior.
    \item \textbf{Temporal context}: Results reflect system performance at a specific point in time with then-current search results and model versions.
\end{itemize}

The strategies employed in this work to maximize reproducibility---temperature=0.0 for all LLM calls, fixed random seeds, comprehensive logging, and experiment versioning---represent current best practices for LLM evaluation but cannot eliminate all sources of variability.

%-------------------------------------------------------
\subsection{Comparison with Related Fact-Checking Systems}
\label{subsec:comparison}
%-------------------------------------------------------

While direct performance comparison across fact-checking systems is constrained by differences in evaluation datasets, task definitions, and domain characteristics, contextualizing these results against established benchmarks and recent approaches provides valuable perspective on the system's contributions.

\subsubsection{Foundational Benchmarks}

The FEVER dataset \citep{thorne2018fever} established the foundational benchmark for fact verification against textual sources, comprising 185,445 claims verified against Wikipedia with sentence-level evidence annotations. Their best baseline system achieved 31.87\% accuracy when requiring correct evidence retrieval, using a pipeline of TF-IDF document retrieval, sentence selection, and decomposable attention for textual entailment. Notably, 16.82\% of FEVER claims require multi-sentence reasoning, highlighting the complexity of evidence aggregation even in controlled environments. State-of-the-art systems on the FEVER leaderboard have since achieved 70--80\% accuracy, demonstrating significant progress on this benchmark.

Building on this foundation, SciFact \citep{wadden2020scifact} introduced scientific claim verification with 1,409 expert-written claims against biomedical abstracts. Their VERISCI baseline achieved 46.5\% F1 on the open retrieval task (AbstractLabel+Rationale), demonstrating that domain-specific adaptation---combining FEVER pretraining with in-domain fine-tuning---substantially improves performance over zero-shot approaches (36.4\% F1). Their error analysis identified five key reasoning challenges: scientific background knowledge, directionality, numerical reasoning, cause-and-effect relationships, and coreference resolution---challenges that parallel the difficulties Factible encounters with YouTube content.

\subsubsection{Post-hoc Verification and Attribution Approaches}

The RARR framework \citep{gao2023rarr} introduced iterative research-and-revision for improving attribution in LLM outputs. RARR employs a two-stage pipeline: a \textit{research stage} that generates queries and retrieves evidence from web sources, and a \textit{revision stage} that uses agreement models to detect disagreements between claims and evidence before making targeted edits. On diverse QA datasets including Natural Questions, StrategyQA, and QReCC, RARR achieved F1 scores (combining attribution and preservation) ranging from 43\% to 68\%.

A key insight from RARR relevant to this work is the tension between attribution improvement and content preservation. RARR found that editing to improve attribution while changing only 10--20\% of text represents a careful balance---aggressive editing sacrifices the original intent while minimal editing leaves unattributed claims intact. Factible faces an analogous trade-off: the system prioritizes direct verification verdicts (Supported, Refuted, Insufficient Evidence, Unverifiable) over claim revision, trading the flexibility of correction for clearer actionable outputs suited to users assessing video content truthfulness.

VeriScore \citep{song2024veriscore} and FIRE \citep{xie2025fire} extended this direction by introducing confidence-based retrieval triggers that reduce computational costs without sacrificing accuracy. FIRE's iterative retrieval-and-verification approach, which triggers external retrieval only when model confidence falls below a threshold, parallels Factible's confidence-based handling of borderline claims.

\subsubsection{Multi-Agent Architectures}

Recent work has demonstrated the effectiveness of multi-agent LLM architectures for fact verification. FactAgent \citep{factagent2025} employs a four-agent pipeline comprising Input Ingestion, Query Generation, Evidence Retrieval, and Verdict Prediction---an architecture closely resembling Factible's. Evaluated on HoVER, FEVEROUS, and SciFact-Open benchmarks with GPT-4o-mini, FactAgent achieved F1 scores of 0.600--0.617 on multi-hop reasoning tasks (HoVER 2--3 hop), 0.548--0.681 on FEVEROUS structured data tasks, and 0.770 on scientific claim verification (SciFact-Open), representing a 12.3\% relative improvement over baseline methods. Notably, FactAgent integrates MBFC-based source credibility filtering---a design choice independently adopted in Factible---finding that credibility filtering significantly reduces usable links (to 9.6--15.8\% on HoVER) but improves verdict quality by excluding unreliable sources. Their ablation studies showed that generating 3--4 queries per subclaim provides optimal balance between reasoning depth and evidence precision, aligning with Factible's configuration choices.

The MAD-Fact framework \citep{ning2025madfact} provides particularly relevant methodological insights. MAD-Fact implements a three-tier multi-agent debate system comprising: (1) a \textit{Clerk Agent} for atomic claim decomposition, (2) a \textit{Jury} of Evaluator Agents with heterogeneous role assignments (Public, Critic, News Author, Scientist, etc.) that assess factuality through structured debate, and (3) a \textit{Judge Agent} that aggregates verdicts via majority voting. On fact-checking benchmarks including FacToolQA and FELM-WK, MAD-Fact achieved F1 scores of 0.88 (Label=True) and 0.67 (Label=False), consistently outperforming single-model baselines.

Three findings from MAD-Fact are particularly instructive:
\begin{itemize}
    \item \textbf{Debate dynamics}: Multi-agent systems exhibited human-like traits during debates---agents adhered to their viewpoints, actively corrected peers' mistakes, and engaged in self-reflection. This emergent behavior contributed to superior performance over single-agent systems.
    \item \textbf{Retrieval integration}: The ``Autonomous Retrieval and Free Debate'' rule, where agents decide based on confidence whether to invoke external retrieval, balanced knowledge utilization with computational efficiency.
    \item \textbf{Homogeneous vs.\ heterogeneous initialization}: Interestingly, initializing all agents with the same model (GPT-4o-mini) outperformed heterogeneous initialization with multiple model families, as cross-model agents exhibited higher frequencies of misleading each other and struggled to reach consensus.
\end{itemize}

Factible adopts a comparable multi-agent pipeline---comprising claim extraction, evidence retrieval, and verdict determination---but addresses a significantly more challenging domain: unstructured YouTube content with noisy automatic transcriptions and no gold evidence. Despite this increased difficulty, the 73.0\% accuracy demonstrates that multi-agent approaches can generalize beyond curated datasets to real-world multimedia platforms.

\subsubsection{Domain Difficulty Considerations}

The primary challenge in cross-system comparison lies not in algorithmic sophistication but in domain difficulty. Most comparable systems operate on closed corpora with clean text, gold evidence, and pre-extracted claims. In contrast, Factible handles:

\begin{itemize}
    \item \textbf{Noisy input}: Automatic transcription errors, missing punctuation, and speaker attribution ambiguity
    \item \textbf{Open-domain retrieval}: Web-scale evidence search without annotator-provided ground truth
    \item \textbf{Real-world claims}: Naturally occurring statements rather than synthetic mutations
    \item \textbf{Temporal dynamics}: Time-sensitive claims that may have been true at recording but become outdated
\end{itemize}

Table~\ref{tab:system-comparison} summarizes key characteristics across systems. The performance gap between FEVER's baseline 31.87\% on structured Wikipedia and Factible's 73.0\% on unstructured YouTube does not indicate superior accuracy in absolute terms, but rather reflects fundamentally different difficulty profiles and evaluation methodologies. The system's ability to maintain reasonable accuracy while handling YouTube's compounding challenges represents meaningful progress toward bringing automated fact-checking to platforms where misinformation most commonly spreads.

\begin{table}[H]
\centering
\caption[Comparison of fact-checking systems]{Comparison of fact-checking systems across key dimensions. Performance metrics are reported as originally published and are not directly comparable due to differing task definitions and evaluation protocols.}
\label{tab:system-comparison}
\small
\begin{tabular}{lccccc}
\toprule
\textbf{System} & \textbf{Corpus Type} & \textbf{Evidence} & \textbf{Claims} & \textbf{Metric} & \textbf{Performance} \\
\midrule
FEVER \citep{thorne2018fever} & Wikipedia & Gold & Synthetic & Accuracy & 31.87\% \\
SciFact \citep{wadden2020scifact} & Scientific & Gold & Natural & F1 & 46.5\% \\
RARR \citep{gao2023rarr} & QA datasets & Retrieved & Natural & F1 & 43--68\% \\
FactAgent \citep{factagent2025} & Wikipedia & Retrieved & Natural & F1 & 51--77\% \\
MAD-Fact \citep{ning2025madfact} & Fact-check & Retrieved & Natural & F1 & 67--88\% \\
\textbf{Factible (ours)} & YouTube/Web & Retrieved & Natural & Accuracy & 73.0\% \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Implications for System Design}

The comparative analysis yields several design implications validated by this implementation:

\begin{enumerate}
    \item \textbf{Atomic decomposition is essential}: Following FactScore, SAFE, and MAD-Fact, decomposing claims into atomic, independently verifiable units enables fine-grained evaluation and targeted verdict assignment.

    \item \textbf{Retrieval quality bounds verification quality}: As RARR demonstrated, even sophisticated reasoning cannot compensate for poor evidence retrieval. Factible's web search integration addresses this but introduces new challenges around source credibility assessment.

    \item \textbf{Source credibility filtering is critical}: FactAgent's finding that MBFC-based credibility filtering reduces usable links to 9.6--15.8\% on complex multi-hop tasks while improving verdict quality validates the independent adoption of this approach in Factible. The trade-off between evidence quantity and quality favors aggressive filtering.

    \item \textbf{Multi-agent approaches mitigate single-model bias}: Single-model verification is vulnerable to systematic errors from hallucination or knowledge gaps. Factible's multi-stage pipeline, like FactAgent's four-agent architecture, distributes verification responsibility across specialized agents while maintaining computational efficiency.

    \item \textbf{Domain adaptation requires careful evaluation}: Performance numbers from clean-text benchmarks do not transfer to noisy multimedia domains. Future work should develop standardized benchmarks for video fact-checking that account for automatic transcription errors and real-world claim distributions.
\end{enumerate}

%-------------------------------------------------------
\subsection{Summary of Key Findings}
%-------------------------------------------------------

The experimental evaluation demonstrates that Factible achieves reliable fact-checking performance across diverse video content:

\begin{enumerate}
    \item \textbf{High-precision claim extraction:} 81.3\% precision with strong importance ranking (MAP 0.870), meaning users can trust that presented claims are legitimate, high-priority factual statements.

    \item \textbf{Robust evidence retrieval:} 94.7\% success rate with 83.6\% of sources from high-reliability origins, demonstrating effective query generation and source filtering.

    \item \textbf{Consistent verdict accuracy:} 73.0\% overall accuracy, with 70\% of videos (21/30) achieving $\geq$75\% accuracy.

    \item \textbf{Identified challenges:} The 4 low-accuracy videos (13.3\%) involve politically contested claims with legitimate disagreement, where evidence synthesis rather than retrieval poses the challenge.

    \item \textbf{Practical efficiency:} 82.2 seconds mean latency at \$0.003 per video enables cost-effective, near-interactive use.
\end{enumerate}

These results position Factible as a reliable tool for preliminary fact-checking of YouTube content. The system performs well on scientific, health, and clearly verifiable claims, while contested political topics with conflicting evidence sources represent the primary remaining challenge for future work.
