%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CHAPTER 5: CONCLUSIONS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}

This section presents the conclusions of this thesis, evaluates the achievement of initial objectives, reflects on the methodology employed, and discusses ethical and sustainability considerations.

%-------------------------------------------------------
\subsection{Contributions and Achievements}
%-------------------------------------------------------

This thesis has presented Factible, a multi-agent system for automated fact-checking of YouTube videos. The work addresses critical gaps identified in the State of the Art---particularly the lack of end-to-end usability, dependence on structured sources, and the production readiness gap---demonstrating the practical viability of LLM-based verification systems for real-world deployment.

\subsubsection{Technical Contributions}

\begin{enumerate}
    \item \textbf{End-to-End Video Fact-Checking Pipeline}: Unlike prior research that focuses on isolated components \citep{hassan2017claimbuster, thorne2018fever}, Factible implements a complete pipeline from YouTube URL to verified claims with evidence-backed verdicts. The system handles transcript extraction, claim detection, query generation, evidence retrieval, and verdict synthesis autonomously, addressing the end-to-end usability gap identified in Section 2 \citep{lin2025factaudit}.

    \item \textbf{Novel Claim Extraction Approach}: A novel prompting approach that prioritizes claims based on their impact on the video's central argument, achieving 81.3\% precision and 0.870 MAP score for claim extraction.

    \item \textbf{Open-Web Evidence Retrieval with Credibility Assessment}: Moving beyond Wikipedia and closed knowledge bases \citep{aly2021feverous, thorne2018fever}, the system retrieves evidence from the live web using search engines, incorporating source credibility evaluation using Media Bias/Fact Check data and domain heuristics. This enables verification of claims about current events, specialized domains, and topics not well covered in encyclopedic sources.

    \item \textbf{Three-Level Parallel Architecture}: An efficient execution model that parallelizes processing across claims, queries, and search results, achieving mean processing time of 82.2 seconds per video while maintaining cost under \$0.01 per video. This addresses the performance and scalability limitations noted in the literature \citep{lin2025factaudit}.

    \item \textbf{Modular Multi-Agent Architecture}: Five specialized agents with structured Pydantic schemas enable transparency, maintainability, and independent optimization. This design facilitates experimentation with different models and techniques, demonstrating the practical benefits of multi-agent approaches \citep{factagent2025, chen2024local}.

    \item \textbf{Production-Ready Implementation}: A web-based interface with real-time SSE streaming makes the verification process transparent and accessible to non-expert users, bridging the gap between research prototypes and usable products \citep{lin2025factaudit}. The interface displays complete evidence chains including stance classifications, source reliability ratings, political bias indicators when available, evidence summaries, and confidence levels---enabling users to evaluate the system's reasoning rather than accepting verdicts blindly.
\end{enumerate}

\subsubsection{Research Contributions}

\begin{enumerate}
    \item \textbf{Annotated Evaluation Dataset}: A corpus of 30 YouTube videos with 503 manually annotated ground truth claims across health, climate, and political topics, providing a foundation for future research on video fact-checking.

    \item \textbf{Comprehensive Evaluation Framework}: A methodology combining quantitative metrics (precision, recall, MAP), semantic similarity matching, verdict accuracy assessment, and system efficiency measurements. This multifaceted approach addresses the evaluation challenges discussed in the literature and provides a realistic assessment of system capabilities \citep{raschka2025llmeval, ruder2025llmeval}.

    \item \textbf{Analysis of Failure Modes}: Systematic identification of where automated fact-checking struggles, particularly with contested political claims where evidence synthesis rather than retrieval poses the challenge.

    \item \textbf{Focus on Video Platform}: Unlike most fact-checking research that focuses on text-based content \citep{thorne2018fever, hassan2017claimbuster}, this work specifically addresses YouTube video verification, handling the complete video-to-verification workflow. This fills a significant gap where video platforms have been largely overlooked despite their scale and influence on information consumption.
\end{enumerate}

\subsubsection{Fulfillment of Objectives}

The main objective---to design, implement, and evaluate a multi-agent system capable of automatically verifying factual claims in YouTube videos---was \textbf{fully achieved}. Evaluation on 30 videos demonstrates 81.3\% claim extraction precision, 94.7\% evidence retrieval success, and 73.0\% verdict accuracy.

All six specific objectives defined in Section 1.2.2 were achieved. The technical and research contributions above address objectives 1--2 (architecture and implementation), 4 (evaluation), and 5 (end-to-end system). Two additional objectives merit explicit mention:

\begin{itemize}
    \item \textbf{LLM usage optimization} (Objective 3): The system uses GPT-4o-mini with deterministic settings and token budgets. Cost, latency, and quality trade-offs were analyzed, achieving \$0.003 average cost per video with 82.2 seconds mean latency. Prompt optimization strategies with structured outputs were implemented across all pipeline components.

    \item \textbf{Ethical and bias considerations} (Objective 6): The system incorporates source transparency, political bias indicators, multi-perspective evidence presentation, and clear confidence levels. Section~\ref{subsec:ethics} provides detailed discussion of ethical considerations and bias mitigation strategies.
\end{itemize}

%-------------------------------------------------------
\subsection{Reflection on Ethical and Sustainability Considerations}
\label{subsec:ethics}
%-------------------------------------------------------

\subsubsection{Sustainability Assessment}

The project's environmental impact was considered throughout development:

\begin{itemize}
    \item \textbf{Computational efficiency}: Using GPT-4o-mini rather than larger models (GPT-4, GPT-4-turbo) significantly reduces energy consumption while maintaining adequate quality. The average cost of \$0.003 per video reflects both financial and environmental efficiency.

    \item \textbf{Optimization strategies}: Token budgets, content trimming, and classical algorithms for non-reasoning tasks minimize unnecessary LLM calls.

    \item \textbf{Local model support}: The system supports Ollama-based local models, enabling zero-cloud-cost operation for users with appropriate hardware, further reducing the system's carbon footprint for some deployments.
\end{itemize}

While LLM usage inherently involves energy consumption, the system's design choices represent a conscious effort to balance capability with environmental responsibility.

\subsubsection{Ethical Considerations}

The system addresses ethical concerns through several mechanisms:

\begin{itemize}
    \item \textbf{Transparency}: All evidence sources are exposed with URLs, reliability ratings, political bias indicators (when available from Media Bias/Fact Check data), and stance classifications, enabling users to verify the system's reasoning and identify potential source perspectives.

    \item \textbf{Multi-perspective presentation}: Evidence is organized by stance (supports, refutes, mixed, unclear), avoiding false certainty and acknowledging when claims are genuinely contested.

    \item \textbf{Confidence levels}: Verdicts include confidence ratings (low, medium, high), helping users calibrate trust appropriately.

    \item \textbf{Tool positioning}: The system is positioned as a fact-checking assistant rather than an authoritative source, emphasizing its role in supporting rather than replacing human judgment.
\end{itemize}

\subsubsection{Potential Risks and Mitigation}

Several risks warrant ongoing attention:

\begin{itemize}
    \item \textbf{Over-reliance on automation}: Users might accept system verdicts uncritically. Mitigation: Clear messaging that the system provides preliminary assessment, not definitive truth.

    \item \textbf{Bias amplification}: LLMs may perpetuate biases from training data. Mitigation: Source diversity in evidence retrieval and transparent presentation of multiple perspectives.

    \item \textbf{Weaponization concerns}: The system could theoretically be misused to generate plausible-sounding refutations of true claims. Mitigation: Open-source release enables scrutiny, and the evidence-based approach makes manipulation detectable.
\end{itemize}

\subsubsection{Sustainable Development Goals}

As outlined in Section 1.3.2, the project contributes to SDG 4 (Quality Education) by serving as a media literacy tool that promotes critical thinking, SDG 16 (Peace, Justice and Strong Institutions) by supporting informed public discourse through transparent verification mechanisms, and SDG 10 (Reduced Inequalities) by democratizing access to fact-checking capabilities through its open-source, low-cost design.

%-------------------------------------------------------
\subsection{Concluding Remarks}
%-------------------------------------------------------

This thesis addresses a critical gap in automated fact-checking research: the lack of end-to-end verification systems for video platforms. It demonstrates that automated fact-checking of YouTube videos is not only feasible but can achieve practically useful performance levels. Factible represents a step toward democratizing access to fact-checking capabilities, providing users with tools to critically evaluate video content that might otherwise go unexamined.

The system's strengths---high precision claim extraction, robust evidence retrieval, transparent verdict presentation, and practical efficiency---make it suitable for preliminary fact-checking of YouTube content. Its limitations---particularly with contested political claims---highlight areas where human judgment remains essential and automated systems should complement rather than replace expert fact-checkers. Section~\ref{sec:future-work} outlines directions for addressing these limitations and extending the system's capabilities.

As misinformation continues to evolve in volume and sophistication, automated fact-checking systems will become increasingly important. The modular, open-source nature of Factible provides a foundation for continued research and development in this critical domain. By making both the system and its evaluation transparent, this work aims to advance the broader goal of building more informed and resilient information ecosystems.

The complete source code, evaluation dataset, and documentation are publicly available at \url{https://github.com/begoechavarren/factible}, enabling researchers and practitioners to build upon this work toward more informed and resilient information ecosystems.
