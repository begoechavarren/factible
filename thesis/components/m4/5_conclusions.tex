%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CHAPTER 5: CONCLUSIONS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}

This section presents the conclusions of this thesis, evaluates the achievement of initial objectives, reflects on the methodology employed, and discusses ethical and sustainability considerations.

%-------------------------------------------------------
\subsection{Summary of Contributions}
%-------------------------------------------------------

This thesis has presented Factible, a multi-agent system for automated fact-checking of YouTube videos. The work addresses critical gaps identified in the State of the Art---particularly the lack of end-to-end usability, dependence on structured sources, and the production readiness gap---demonstrating the practical viability of LLM-based verification systems for real-world deployment.

\subsubsection{Technical Contributions}

\begin{enumerate}
    \item \textbf{End-to-End Video Fact-Checking Pipeline}: Unlike prior research that focuses on isolated components \citep{hassan2017claimbuster, thorne2018fever}, Factible implements a complete pipeline from YouTube URL to verified claims with evidence-backed verdicts. The system handles transcript extraction, claim detection, query generation, evidence retrieval, and verdict synthesis autonomously, addressing the end-to-end usability gap identified in Section 2 \citep{lin2025factaudit}.

    \item \textbf{Thesis-First Claim Extraction}: A novel prompting approach that prioritizes claims based on their impact on the video's central argument, achieving 81.3\% precision and 0.870 MAP score for claim extraction.

    \item \textbf{Open-Web Evidence Retrieval with Credibility Assessment}: Moving beyond Wikipedia and closed knowledge bases \citep{aly2021feverous, thorne2018fever}, the system retrieves evidence from the live web using search engines, incorporating source credibility evaluation using Media Bias/Fact Check data and domain heuristics. This enables verification of claims about current events, specialized domains, and topics not well covered in encyclopedic sources.

    \item \textbf{Three-Level Parallel Architecture}: An efficient execution model that parallelizes processing across claims, queries, and search results, achieving mean processing time of 129.6 seconds per video while maintaining cost under \$0.01 per video. This addresses the performance and scalability limitations noted in the literature \citep{lin2025factaudit}.

    \item \textbf{Modular Multi-Agent Architecture}: Five specialized agents with structured Pydantic schemas enable transparency, maintainability, and independent optimization. This design facilitates experimentation with different models and techniques, demonstrating the practical benefits of multi-agent approaches \citep{factagent2025, chen2024local}.

    \item \textbf{Production-Ready Implementation}: A web-based interface with real-time SSE streaming makes the verification process transparent and accessible to non-expert users, bridging the gap between research prototypes and usable products \citep{lin2025factaudit}.
\end{enumerate}

\subsubsection{Research Contributions}

\begin{enumerate}
    \item \textbf{Annotated Evaluation Dataset}: A corpus of 30 YouTube videos with 503 manually annotated ground truth claims across health, climate, and political topics, providing a foundation for future research on video fact-checking.

    \item \textbf{Comprehensive Evaluation Framework}: A methodology combining quantitative metrics (precision, recall, MAP), semantic similarity matching, verdict accuracy assessment, and system efficiency measurements. This multifaceted approach addresses the evaluation challenges discussed in the literature and provides a realistic assessment of system capabilities \citep{raschka2025llmeval, ruder2025llmeval}.

    \item \textbf{Analysis of Failure Modes}: Systematic identification of where automated fact-checking struggles, particularly with contested political claims where evidence synthesis rather than retrieval poses the challenge.

    \item \textbf{Focus on Video Platform}: Unlike most fact-checking research that focuses on text-based content \citep{thorne2018fever, hassan2017claimbuster}, this work specifically addresses YouTube video verification, handling the complete video-to-verification workflow. This fills a significant gap where video platforms have been largely overlooked despite their scale and influence on information consumption.
\end{enumerate}

%-------------------------------------------------------
\subsection{Achievement of Objectives}
%-------------------------------------------------------

This section evaluates the degree to which the initial objectives defined in Section 1 were achieved.

\subsubsection{Main Objective}

\textbf{Objective}: Design, implement, and evaluate a multi-agent system capable of automatically verifying factual claims in YouTube videos through retrieval and analysis of web-based evidence, generating substantiated verdicts with confidence estimates.

\textbf{Achievement}: \textbf{Fully achieved}. The system processes YouTube videos end-to-end, extracting claims, retrieving evidence from multiple web sources, and generating verdicts with confidence levels. Evaluation on 30 videos demonstrates 81.3\% claim extraction precision, 94.7\% evidence retrieval success, and 73.3\% verdict accuracy.

\subsubsection{Specific Objectives}

\begin{enumerate}
    \item \textbf{Multi-agent architecture design}: \textbf{Achieved}. Five specialized components with well-defined Pydantic schemas operate through a modular pipeline, enabling independent testing and optimization.

    \item \textbf{System component implementation}: \textbf{Achieved}. All five components (Transcriptor, Claim Extractor, Query Generator, Online Search, Output Generator) are fully implemented and operational.

    \item \textbf{LLM usage optimization}: \textbf{Partially achieved}. The system uses GPT-4o-mini effectively with deterministic settings and token budgets. Full comparison with open-source models (Qwen, DeepSeek) was not completed due to scope constraints, though the infrastructure supports such comparisons.

    \item \textbf{System evaluation}: \textbf{Achieved}. Comprehensive evaluation including precision/recall metrics, verdict accuracy, evidence retrieval statistics, latency, cost analysis, and qualitative case studies.

    \item \textbf{End-to-end system implementation}: \textbf{Achieved}. REST API with SSE streaming, React web interface, and documentation are complete. Source code is publicly available.

    \item \textbf{Ethical and bias considerations}: \textbf{Achieved}. The system incorporates source transparency, multi-perspective evidence presentation, and clear confidence levels. Discussion of limitations and bias mitigation strategies is included.
\end{enumerate}

%-------------------------------------------------------
\subsection{Critical Assessment of Methodology}
%-------------------------------------------------------

\subsubsection{Strengths of the Approach}

The Design and Creation methodology \citep{oates2006, hevner2004} proved appropriate for this project:

\begin{itemize}
    \item \textbf{Iterative development} enabled rapid identification and resolution of component issues, particularly in prompt engineering and evidence extraction.
    \item \textbf{Modular architecture} facilitated independent testing and optimization of each pipeline component.
    \item \textbf{Practical focus} ensured the system addresses real-world requirements (latency, cost, usability) beyond academic benchmarks.
    \item \textbf{Comprehensive evaluation} combining quantitative metrics with qualitative analysis provided meaningful insights into system behavior.
\end{itemize}

\subsubsection{Limitations of the Approach}

Several limitations should be acknowledged:

\begin{itemize}
    \item \textbf{Single annotator for ground truth}: All 503 ground truth claims were annotated by a single person (the author), introducing potential bias. Inter-annotator agreement studies would strengthen the evaluation.

    \item \textbf{Dataset size}: While 30 videos with 503 claims is comparable to prior work, a larger dataset would enable more robust statistical conclusions and better representation of edge cases.

    \item \textbf{English-only scope}: The system currently supports only English content, limiting applicability in multilingual contexts where misinformation is equally prevalent.

    \item \textbf{Temporal snapshot}: Evaluation was conducted at a specific point in time (December 2025). Both web search results and LLM model versions change, affecting reproducibility.

    \item \textbf{Limited model comparison}: Full comparison between commercial and open-source LLMs was not completed, leaving optimization opportunities unexplored.
\end{itemize}

%-------------------------------------------------------
\subsection{Reflection on Ethical and Sustainability Considerations}
%-------------------------------------------------------

\subsubsection{Sustainability Assessment}

The project's environmental impact was considered throughout development:

\begin{itemize}
    \item \textbf{Computational efficiency}: Using GPT-4o-mini rather than larger models (GPT-4, GPT-4-turbo) significantly reduces energy consumption while maintaining adequate quality. The average cost of \$0.003 per video reflects both financial and environmental efficiency.

    \item \textbf{Optimization strategies}: Token budgets, content trimming, and classical algorithms for non-reasoning tasks minimize unnecessary LLM calls.

    \item \textbf{Local model support}: The system supports Ollama-based local models, enabling zero-cloud-cost operation for users with appropriate hardware, further reducing the system's carbon footprint for some deployments.
\end{itemize}

While LLM usage inherently involves energy consumption, the system's design choices represent a conscious effort to balance capability with environmental responsibility.

\subsubsection{Ethical Considerations}

The system addresses ethical concerns through several mechanisms:

\begin{itemize}
    \item \textbf{Transparency}: All evidence sources are exposed with URLs, reliability ratings, and stance classifications, enabling users to verify the system's reasoning.

    \item \textbf{Multi-perspective presentation}: Evidence is organized by stance (supports, refutes, mixed, unclear), avoiding false certainty and acknowledging when claims are genuinely contested.

    \item \textbf{Confidence levels}: Verdicts include confidence ratings (low, medium, high), helping users calibrate trust appropriately.

    \item \textbf{Tool positioning}: The system is positioned as a fact-checking assistant rather than an authoritative source, emphasizing its role in supporting rather than replacing human judgment.
\end{itemize}

\subsubsection{Potential Risks and Mitigation}

Several risks warrant ongoing attention:

\begin{itemize}
    \item \textbf{Over-reliance on automation}: Users might accept system verdicts uncritically. Mitigation: Clear messaging that the system provides preliminary assessment, not definitive truth.

    \item \textbf{Bias amplification}: LLMs may perpetuate biases from training data. Mitigation: Source diversity in evidence retrieval and transparent presentation of multiple perspectives.

    \item \textbf{Weaponization concerns}: The system could theoretically be misused to generate plausible-sounding refutations of true claims. Mitigation: Open-source release enables scrutiny, and the evidence-based approach makes manipulation detectable.
\end{itemize}

\subsubsection{Sustainable Development Goals Impact}

The project contributes positively to three SDGs:

\begin{itemize}
    \item \textbf{SDG 4 (Quality Education)}: The system serves as a media literacy tool, helping users develop critical thinking skills by exposing the fact-checking process.

    \item \textbf{SDG 16 (Peace, Justice and Strong Institutions)}: By providing transparent verification mechanisms, the system supports informed public discourse and combats misinformation that erodes institutional trust.

    \item \textbf{SDG 10 (Reduced Inequalities)}: The open-source, low-cost design democratizes access to fact-checking capabilities previously available only to well-resourced organizations.
\end{itemize}

%-------------------------------------------------------
\subsection{Lessons Learned}
%-------------------------------------------------------

Several key lessons emerged from this project:

\begin{enumerate}
    \item \textbf{Prompt engineering is critical but brittle}: Small changes in prompt wording can significantly affect LLM outputs. The thesis-first approach required extensive iteration to achieve stable, high-quality claim extraction.

    \item \textbf{Evidence retrieval is easier than evidence synthesis}: The system achieves 94.7\% evidence retrieval success but only 73.3\% verdict accuracy. The challenge lies in correctly interpreting and synthesizing conflicting sources, not in finding them.

    \item \textbf{Contested claims remain difficult}: Political and socially contentious topics with legitimate disagreement present fundamental challenges for automated fact-checking that may require different approaches than straightforward factual verification.

    \item \textbf{Practical constraints matter}: Latency, cost, and usability considerations significantly shaped design decisions. Academic metrics alone are insufficient for evaluating systems intended for real-world use.

    \item \textbf{Modular design pays dividends}: The five-component architecture enabled independent development, testing, and optimization, proving essential for managing complexity.
\end{enumerate}

%-------------------------------------------------------
\subsection{Concluding Remarks}
%-------------------------------------------------------

This thesis has demonstrated that automated fact-checking of YouTube videos is not only feasible but can achieve practically useful performance levels. Factible represents a step toward democratizing access to fact-checking capabilities, providing users with tools to critically evaluate video content that might otherwise go unexamined.

The system's strengths---high precision claim extraction, robust evidence retrieval, transparent verdict presentation, and practical efficiency---make it suitable for preliminary fact-checking of YouTube content. Its limitations---particularly with contested political claims---highlight areas where human judgment remains essential and automated systems should complement rather than replace expert fact-checkers. Section~\ref{sec:future-work} outlines directions for addressing these limitations and extending the system's capabilities.

As misinformation continues to evolve in volume and sophistication, automated fact-checking systems will become increasingly important. The modular, open-source nature of Factible provides a foundation for continued research and development in this critical domain. By making both the system and its evaluation transparent, this work aims to advance the broader goal of building more informed and resilient information ecosystems.

The complete source code, evaluation dataset, and documentation are publicly available at \url{https://github.com/begoechavarren/factible}, enabling researchers and practitioners to build upon this work.
