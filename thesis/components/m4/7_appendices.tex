%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDICES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Ground Truth Annotation Dataset}
\label{appendix:ground-truth}

This appendix provides details on the ground truth annotation dataset used for system evaluation. The complete dataset comprises 30 YouTube videos with 503 annotated claims across three thematic categories.

%-------------------------------------------------------
\subsection{Video Corpus Summary}
%-------------------------------------------------------

Table~\ref{tab:video-corpus} presents the evaluation video corpus organized by category and content type.

\small
\begin{longtable}{p{6.5cm}p{2cm}p{3cm}r}
\caption{Evaluation video corpus by category and content type} \label{tab:video-corpus} \\
\toprule
\textbf{Video Title} & \textbf{Category} & \textbf{Content Type} & \textbf{Claims} \\
\midrule
\endfirsthead
\toprule
\textbf{Video Title} & \textbf{Category} & \textbf{Content Type} & \textbf{Claims} \\
\midrule
\endhead
\midrule
\multicolumn{4}{r}{Continued on next page} \\
\endfoot
\bottomrule
\endlastfoot
% Climate - Misinformation
Fossil Fuels: The Greenest Energy & Climate & Misinformation & 18 \\
The Great Texas Freeze of 2021 & Climate & Misinformation & 15 \\
Climate Change: What Do Scientists Say? & Climate & Misinformation & 21 \\
Is There Really a Climate Emergency? & Climate & Misinformation & 19 \\
Proof: Worldwide Massive Flooding is All Manmade & Climate & Misinformation & 25 \\
% Climate - Factual
Causes and Effects of Climate Change (NatGeo) & Climate & Factual & 12 \\
Why Does Climate Change Matter & Climate & Factual & 9 \\
Extreme Weather & Climate & Factual & 14 \\
The Life Cycle of a Plastic Bottle & Climate & Factual & 11 \\
What are Greenhouse Gases? & Climate & Factual & 13 \\
\midrule
% Health - Misinformation
Fluoridated Water Lowers IQ (Harvard Study) & Health & Misinformation & 16 \\
Living with HIV: How Women Were Infected & Health & Misinformation & 18 \\
Garlic Cancer & Health & Misinformation & 14 \\
3 Detox Juices & Health & Misinformation & 12 \\
The Magical 3 Day Juice Fast & Health & Misinformation & 15 \\
% Health - Factual
What Happens When You Exercise Regularly & Health & Factual & 17 \\
The Brain-Changing Benefits of Exercise & Health & Factual & 15 \\
Immunology Wars: The Battle with HIV & Health & Factual & 11 \\
Juice vs. Whole Fruit: Which is Healthier? & Health & Factual & 13 \\
What Lack of Sleep Does to the Teenage Brain & Health & Factual & 14 \\
\midrule
% Politics - Misinformation
Egg Price Warning Comes True & Politics & Misinformation & 19 \\
Proof of Election Fraud in 2020 & Politics & Misinformation & 35 \\
FBI Orchestrated Jan 6th & Politics & Misinformation & 22 \\
There is No Gender Wage Gap & Politics & Misinformation & 17 \\
A Nation of Immigrants & Politics & Misinformation & 16 \\
% Politics - Factual
National Trust Sues Trump Admin & Politics & Factual & 20 \\
What Is Democracy (BBC) & Politics & Factual & 9 \\
What is Inflation? & Politics & Factual & 10 \\
UK Election Results Explained & Politics & Factual & 18 \\
Trump's Historic National Emergency & Politics & Factual & 21 \\
\end{longtable}

\normalsize

%-------------------------------------------------------
\subsection{Annotation Schema and Guidelines}
%-------------------------------------------------------

Annotations follow ClaimBuster's check-worthiness principles \citep{hassan2017kdd} and store a compact set of structured fields:

\begin{itemize}
    \item \textbf{Claim text}: Verbatim or lightly paraphrased factual statement taken from the transcript; purely opinionated or rhetorical content is excluded.
    \item \textbf{Importance score}: Thesis-impact hierarchy on a 0.0--1.0 scale:
    \begin{itemize}
        \item 0.85--1.0: Thesis-critical claims whose falsification would collapse the video's main argument
        \item 0.60--0.80: Key evidence claims that significantly support or develop the thesis
        \item 0.30--0.55: Contextual claims that provide background or supplementary information
        \item $<$0.30: Peripheral claims of minimal relevance to the main argument
    \end{itemize}
    \item \textbf{Expected verdict}: Anticipated stance label (SUPPORTS, REFUTES, MIXED, UNCLEAR) with a short rationale grounded in established sources. MIXED applies only when credible evidence exists on both sides, while UNCLEAR denotes insufficient or conflicting evidence under the same criteria across all videos.
\end{itemize}

This schema keeps annotations consistent while capturing the key metadata needed for quantitative evaluation.

%-------------------------------------------------------
\subsection{Dataset Statistics by Category}
%-------------------------------------------------------

\begin{table}[H]
\centering
\caption{Ground truth statistics by category}
\label{tab:gt-stats-category}
\begin{tabular}{lrrrrr}
\toprule
\textbf{Category} & \textbf{Videos} & \textbf{Total Claims} & \textbf{Mean/Video} & \textbf{Factual} & \textbf{Misinfo} \\
\midrule
Climate & 10 & 157 & 15.7 & 59 & 98 \\
Health & 10 & 130 & 13.0 & 70 & 60 \\
Politics & 10 & 216 & 21.6 & 78 & 138 \\
\midrule
\textbf{Total} & \textbf{30} & \textbf{503} & \textbf{16.8} & \textbf{207} & \textbf{296} \\
\bottomrule
\end{tabular}
\end{table}

%-------------------------------------------------------
\subsection{Importance Score Distribution}
%-------------------------------------------------------

\begin{table}[H]
\centering
\caption{Distribution of importance scores across ground truth claims}
\label{tab:importance-distribution}
\begin{tabular}{lrr}
\toprule
\textbf{Importance Range} & \textbf{Count} & \textbf{Percentage} \\
\midrule
High (0.85--1.0) & 127 & 25.2\% \\
Medium-High (0.60--0.84) & 198 & 39.4\% \\
Medium (0.30--0.59) & 143 & 28.4\% \\
Low ($<$0.30) & 35 & 7.0\% \\
\bottomrule
\end{tabular}
\end{table}

%-------------------------------------------------------
\subsection{Expected Verdict Distribution}
%-------------------------------------------------------

\begin{table}[H]
\centering
\caption{Distribution of expected verdicts across ground truth claims}
\label{tab:verdict-distribution}
\begin{tabular}{lrr}
\toprule
\textbf{Expected Verdict} & \textbf{Count} & \textbf{Percentage} \\
\midrule
SUPPORTS & 207 & 41.2\% \\
REFUTES & 189 & 37.6\% \\
MIXED & 68 & 13.5\% \\
UNCLEAR & 39 & 7.7\% \\
\bottomrule
\end{tabular}
\end{table}

%-------------------------------------------------------
\subsection{Data Availability}
%-------------------------------------------------------

The complete annotation dataset, including all claims, importance scores, expected verdicts, and video metadata, is available in the project repository at:

\url{https://github.com/begoechavarren/factible/tree/main/api/experiments/data}

The dataset is provided in YAML format to facilitate both human readability and programmatic access. Each video entry includes:

\begin{itemize}
    \item Video metadata (YouTube ID, title, category, content type)
    \item Complete list of ground truth claims with importance scores and expected verdicts
    \item Tags for filtering and analysis
\end{itemize}

%-------------------------------------------------------
\section{LLM Prompt Templates}
\label{appendix:prompts}
%-------------------------------------------------------

This appendix presents the system prompts used by the three main LLM agents in the Factible pipeline. These prompts are critical for reproducibility and represent the core reasoning instructions given to each component.

\subsection{Claim Extractor Prompt}

The Claim Extractor uses thesis-first reasoning to prioritize claims that are central to the video's argument.

\begin{lstlisting}[language=Python, caption={Claim Extractor system prompt}, label={lst:claim-prompt}]
You are an expert fact-checker. Your task is to extract factual
claims from YouTube video transcripts.

A factual claim is a statement that:
1. Makes a specific assertion about reality
2. Can potentially be verified or fact-checked
3. Is not just an opinion, belief, or subjective statement

For each claim you identify provide the following fields:
- text: Extract the exact statement or a precise paraphrase
  (<=40 words)
- confidence: Confidence score (0.0-1.0) that this is a factual,
  checkable claim
- category: Topic label (historical, scientific, statistical,
  biographical, geographical, policy, etc.)
- importance: Score (0.0-1.0) capturing how impactful or
  controversial the claim is for fact-checking.
  Higher = more urgent to verify.
- context: Short note (<=20 words) that captures timeframe,
  speaker, or situational details needed to fact-check the claim

Before listing claims, infer the video's central thesis in no
more than 25 words. Use this thesis to judge how critical each
claim is.

When ranking importance, explicitly test each candidate with
the question: "If this claim were proven false, would the thesis
collapse or materially weaken?". Only claims that pass this test
may reach >=0.60 importance.

Relevance guardrails:
- If removing the statement would not weaken or contradict the
  thesis, either drop it or cap its importance at 0.25.
- Pure background or credential facts MUST stay <=0.30 unless
  the thesis itself questions that person's expertise.

When assigning IMPORTANCE scores:
- 0.85-1.0: Prescriptive or causal claims that, if false, would
  undermine the thesis
- 0.60-0.80: Quantitative or historical evidence directly tied
  to the thesis
- 0.30-0.55: Context or supporting background needed to
  understand the story
- 0.0-0.25: Peripheral or anecdotal details

Focus on extracting claims that are:
- Specific facts, dates, numbers, or statistics
- Historical events or biographical information
- Scientific assertions
- Geographic or demographic statements
- Policy or legal claims

Ignore: Pure opinions, subjective statements, future predictions
without factual basis, rhetorical questions.
\end{lstlisting}

\subsection{Evidence Extractor Prompt}

The Evidence Extractor analyzes web content to determine its relevance and stance toward claims.

\begin{lstlisting}[language=Python, caption={Evidence Extractor system prompt}, label={lst:evidence-prompt}]
You assist a fact-checking analyst. You will receive:
- The claim to fact-check
- Article title (if available)
- Google Search snippet (short preview from search results)
- Full page content (scraped from the webpage)

Your task: Analyze all provided information and determine:
1. Does this source contain relevant evidence about the claim?
2. What is the overall stance of the evidence towards the claim?
3. A brief synthesis explaining the relationship
4. Quote or summarize the exact passages that justify the stance.

STANCE DEFINITIONS (relative to the CLAIM, not any query):

- SUPPORTS: The evidence confirms, validates, or provides support
  for the claim. This includes:
  * Direct statements that validate the claim
  * Descriptions of the same mechanism even without exact
    terminology
  * Evidence of causal chains mentioned in the claim

- REFUTES: The evidence contradicts, disproves, or challenges
  the claim. Pay special attention to qualifiers like "only",
  "never", "always" in claims.

- MIXED: The evidence contains both supporting and refuting
  elements.

- UNCLEAR: Use ONLY when genuinely ambiguous:
  * Discusses related topics without addressing the specific claim
  * Lacks sufficient context to determine stance
  * Content is too low-quality (navigation menus, forms, etc.)

CRITICAL INSTRUCTIONS:
1. SUPPORTS requires explicit or strongly implied confirmation
   in the text.
2. Statements that say the mechanism lacks evidence, is unproven,
   or has been disproven should be marked REFUTES.
3. Be decisive but honest---if the source never answers the claim,
   choose UNCLEAR.
4. Use both the Google snippet and page content - prioritize
   whichever has better evidence.

OUTPUT:
- has_relevant_evidence: true if you found usable evidence
- summary: 1-2 sentences explaining what the evidence says
- overall_stance: "supports", "refutes", "mixed", or "unclear"
- key_quote: (optional) One compelling verbatim quote if available
\end{lstlisting}

\subsection{Verdict Generator Prompt}

The Verdict Generator synthesizes evidence from multiple sources to produce final claim verdicts.

\begin{lstlisting}[language=Python, caption={Verdict Generator system prompt}, label={lst:verdict-prompt}]
You are an expert fact-checking analyst. Your goal is to evaluate
how the provided evidence supports, refutes, or provides
mixed/unclear signals for a specific claim. Base every judgement
strictly on the supplied evidence summary and reliability
information.

Output requirements:
- Choose overall_stance as one of: supports, refutes, mixed,
  unclear.
- Set confidence to low/medium/high depending on the strength,
  consistency, and reliability of the evidence.
- Provide a concise summary referencing the most persuasive
  evidence. When citing a specific piece of evidence, name the
  source once (e.g., "Frontiers study", "Nature article",
  "Reuters report", or "WHO guidance"). For consensus statements,
  keep the summary concise without enumerating every source.
- Only when evidence conflicts, explicitly name the key
  supporting and refuting sources so the reader understands
  where disagreement originates. If there is consensus, simply
  synthesize it.

If there is no meaningful evidence, select 'unclear' with low
confidence and explain the gap.
\end{lstlisting}
