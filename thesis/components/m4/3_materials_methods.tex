%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CHAPTER 3: MATERIALS AND METHODS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Materials and Methods}

This section presents the comprehensive technical implementation of Factible, a multi-agent system for automated fact-checking of YouTube videos. The complete source code is publicly available at \url{https://github.com/begoechavarren/factible}. The system implements an end-to-end pipeline that processes video content through five specialized components, leveraging large language models (LLMs) for reasoning tasks while employing classical algorithms for deterministic operations. The implementation follows design-science principles \citep{hevner2004, oates2006} and focuses on building practical artifacts that are iteratively evaluated and refined.

%-------------------------------------------------------
\subsection{System Architecture Overview}
%-------------------------------------------------------

\subsubsection{High-Level Architecture}

Factible implements an end-to-end automated fact-checking pipeline for YouTube videos using a multi-agent architecture. Recent research on LLM agents demonstrates that multi-agent collaboration can enhance factuality and reasoning by allowing specialized agents to coordinate on tasks \citep{wu2023autogen}. FactAgent further shows that decomposing fact-checking into dedicated agents for input ingestion, query generation, evidence retrieval, and verdict prediction yields higher accuracy and transparency \citep{factagent2025}. The Factible architecture follows this line of work by processing video content through five specialized, modular components that operate sequentially with three levels of internal parallelization.

The system processes a YouTube video URL through five sequential stages, each with specialized responsibilities. The pipeline begins with transcript extraction, proceeds through claim and query generation, conducts online evidence retrieval, and culminates in structured verdict synthesis. This modular design enables independent optimization of each component while maintaining clear data contracts between stages.

The five stages are:

\begin{enumerate}
    \item \textbf{Transcriptor}: Extracts video transcripts via YouTube Transcript API, preserving timestamped segments for claim localization. The component includes automatic fallback to proxy service when rate-limited, ensuring robust transcript retrieval across different access conditions.

    \item \textbf{Claim Extractor} (LLM Agent): Employs thesis-first reasoning to infer the video's central argument before extracting factual and verifiable claims. Each claim receives an importance score based on its impact on the video's thesis. Post-processing uses fuzzy string matching---a technique that finds approximate text matches using similarity scores rather than exact character sequences---to locate claims within the transcript for timestamp mapping.

    \item \textbf{Query Generator} (LLM Agent): Generates diverse search queries across four strategic types---direct, alternative, source-seeking, and contextual. Each query receives a priority score (1--5) based on evidence likelihood, enabling filtering of low-priority queries.

    \item \textbf{Online Search}: Executes a four-step evidence retrieval pipeline for each query: (i) Google Search via Serper API, (ii) website reliability assessment using Media Bias/Fact Check (MBFC) data combined with domain heuristics \citep{mbfc2024methodology}, (iii) content fetching via Selenium WebDriver with JavaScript rendering support, and (iv) LLM-based evidence extraction with stance classification (supports, refutes, mixed, unclear).

    \item \textbf{Output Generator} (LLM Agent): Synthesizes evidence into structured verdicts by building evidence bundles grouped by stance, generating natural language summaries with confidence levels, calculating algorithmic evidence quality scores, and mapping claims to video timestamps for interactive navigation.
\end{enumerate}

Figure \ref{fig:pipeline-architecture} illustrates the complete pipeline architecture with data flow and parallelization points across all five stages.

\begin{figure}[H]
\centering
\begin{tikzpicture}[
    node distance=0.7cm,
    box/.style={rectangle, draw=blue!60, fill=blue!5, thick, minimum width=12cm, minimum height=1.2cm, align=center, font=\small},
    arrow/.style={-{Stealth[length=3mm]}, thick},
    parallel/.style={draw=orange!70, dashed, thick}
]

% Stage 1
\node[box] (s1) {
    \textbf{Stage 1: Transcriptor}\\
    YouTube Transcript API $\rightarrow$ TranscriptData (text + segments + video\_id)
};

% Stage 2
\node[box, below=of s1] (s2) {
    \textbf{Stage 2: Claim Extractor (LLM)}\\
    Thesis-first reasoning $\rightarrow$ ExtractedClaims (sorted by importance)
};

% Stage 3
\node[box, below=of s2] (s3) {
    \textbf{Stage 3: Query Generator (LLM)}\\
    Generate diverse queries $\rightarrow$ GeneratedQueries (filtered by priority)
};

% Stage 4
\node[box, below=of s3] (s4) {
    \textbf{Stage 4: Online Search}\\
    \textit{Google Search $\rightarrow$ Reliability $\rightarrow$ Fetch $\rightarrow$ Extract Evidence}
};

% Stage 5
\node[box, below=of s4] (s5) {
    \textbf{Stage 5: Output Generator (LLM)}\\
    Evidence synthesis $\rightarrow$ FactCheckRunOutput (reports sorted by quality)
};

% Arrows
\draw[arrow] (s1) -- (s2);
\draw[arrow] (s2) -- node[right, font=\scriptsize, orange] {Level 1: N claims $\parallel$} (s3);
\draw[arrow] (s3) -- node[right, font=\scriptsize, orange] {Level 2: M queries/claim $\parallel$} (s4);
\draw[arrow] (s4) -- node[right, font=\scriptsize, orange] {Level 3: K results/query $\parallel$} (s5);

\end{tikzpicture}
\caption{High-level pipeline architecture showing the five main stages and three parallelization levels. Parallelization occurs at claims (Level 1), queries per claim (Level 2), and search results per query (Level 3). Verdict generation happens within Level 1 after each claim's evidence is collected.}
\label{fig:pipeline-architecture}
\end{figure}

\subsubsection{Design Principles}

The system adheres to several key design principles derived from software engineering best practices and GenAI application development \citep{hevner2004}:

\begin{enumerate}
    \item \textbf{Modularity}: Each component is isolated with well-defined inputs and outputs using Pydantic schemas, enabling independent optimization, testing, and replacement.

    \item \textbf{Structured Outputs}: All LLM interactions use Pydantic AI with typed output schemas, ensuring type safety, automatic validation, and consistent data structures across the pipeline.

    \item \textbf{Transparency}: The full evidence chain is preserved and exposed to users---sources, reliability ratings, stances, and reasoning are all traceable from final verdict back to original source.

    \item \textbf{Progressive Enhancement}: The pipeline operates with graceful degradation (e.g., fallback to snippet if scraping fails, fallback to proxy if rate-limited) rather than failing entirely.

    \item \textbf{Cost-Conscious Design}: Configurable limits (\texttt{max\_claims}, \texttt{max\_queries}, \texttt{max\_results}) prevent runaway API costs and control latency during development and production.

    \item \textbf{Reproducibility}: Deterministic LLM outputs (\texttt{temperature=0.0}), structured YAML configurations, and comprehensive experiment tracking enable reproducible research.

    \item \textbf{Separation of Concerns}: Classical algorithms handle tasks like reliability scoring, deduplication, and quality calculation, reserving LLM calls for tasks requiring reasoning and language understanding.
\end{enumerate}

%-------------------------------------------------------
\subsection{Technology Stack}
%-------------------------------------------------------

\subsubsection{Core Technologies}

Table \ref{tab:tech-stack} presents the core technologies employed in the implementation.

\begin{table}[H]
\centering
\caption{Core technology stack}
\label{tab:tech-stack}
\begin{tabular}{llll}
\toprule
\textbf{Category} & \textbf{Technology} & \textbf{Version} & \textbf{Purpose} \\
\midrule
Language & Python & 3.12 & Core implementation with type hints \\
LLM Framework & Pydantic AI & $\geq$1.0.0 & Agent orchestration, structured outputs \\
Data Validation & Pydantic & $\geq$2.0.0 & Schema definitions, runtime validation \\
Web Framework & FastAPI & $\geq$0.115.0 & REST API with SSE streaming \\
HTTP Server & Uvicorn & $\geq$0.32.0 & High-performance ASGI server \\
Async HTTP & httpx & $\geq$0.28.1 & Async HTTP client \\
Web Scraping & Selenium & $\geq$4.15.2 & JavaScript-rendered content extraction \\
YouTube & youtube-transcript-api & $\geq$1.2.2 & Transcript extraction \\
Domain Info & python-whois & $\geq$0.8.0 & Domain age lookup \\
CLI & Typer & $\geq$0.15.0 & Experiment runner CLI \\
Analysis & pandas, matplotlib & - & Data analysis and visualization \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Large Language Models}

The system supports multiple LLM providers to enable comparison of cost-quality trade-offs. Table \ref{tab:llm-models} shows the available models and their configurations.

\begin{table}[H]
\centering
\caption{LLM providers and pricing}
\label{tab:llm-models}
\begin{tabular}{lllll}
\toprule
\textbf{Provider} & \textbf{Model} & \textbf{Context} & \textbf{Pricing (per 1M tokens)} & \textbf{Use Case} \\
\midrule
OpenAI & gpt-4o-mini & 128K & \$0.15 / \$0.60 & Default \\
OpenAI & gpt-4o & 128K & \$5.00 / \$15.00 & High-quality \\
OpenAI & gpt-4-turbo & 128K & \$10.00 / \$30.00 & Premium \\
Ollama & qwen3:8b & 40K & Free (local) & Budget/offline \\
Ollama & qwen3:4b & 256K & Free (local) & Budget/offline \\
\bottomrule
\end{tabular}
\end{table}

Large language models such as GPT-4 offer multimodal capabilities and demonstrate human-level performance across diverse benchmarks \citep{openai2024gpt4}. Despite these advances, models still suffer from hallucinations and are constrained by limited context windows, underscoring the need for careful configuration and reliability safeguards \citep{openai2024gpt4}. The model management layer therefore makes it straightforward to switch providers, tune limits, and keep outputs deterministic through shared configuration.

The same layer also exposes a zero-cost, offline alternative through Ollama with Qwen 3 variants (qwen3:8b, qwen3:4b). Switching between OpenAI and local providers requires only a configuration change, enabling experiments that balance latency, quality, and hardware constraints. GPT-4o-mini remains the default selection for this thesis because cloud inference provides lower latency than consumer hardware while maintaining strong reasoning quality.

\subsubsection{External Services}

The system integrates with external services for search and transcript extraction:

\begin{itemize}
    \item \textbf{Serper API}: Google Search wrapper providing organic search results with approximately 2,500 queries per month on the free tier.
    \item \textbf{YouTube oEmbed API}: Video metadata retrieval without authentication.
    \item \textbf{Webshare Proxy}: Rate limit bypass for transcript extraction with configurable proxy locations.
\end{itemize}

%-------------------------------------------------------
\subsection{Pipeline Components}
%-------------------------------------------------------

\subsubsection{Transcriptor}

The Transcriptor component extracts YouTube video transcripts with precise timestamp information for later claim-to-video mapping.

\paragraph{Implementation Details}

The transcriptor uses the \texttt{youtube-transcript-api} library to fetch available transcripts, with preference for English. When rate-limited by YouTube, it automatically falls back to a proxy service (Webshare). Key features include:

\begin{itemize}
    \item \textbf{Timestamped Segments}: Each segment preserves \texttt{start} time and \texttt{duration} in seconds.
    \item \textbf{Character Position Mapping}: Enables mapping claim text positions back to video timestamps.
    \item \textbf{Title Fetching}: Uses YouTube oEmbed API to retrieve video title for context.
    \item \textbf{Proxy Fallback}: Automatic retry through Webshare proxy when rate-limited.
\end{itemize}

\paragraph{Output and Timestamp Mapping}

Each run returns the video identifier, the full transcript text, and a list of timestamped segments containing text, start time, and duration. A lightweight mapping pass scans the segments sequentially while maintaining a cumulative character counter so that claim positions can be translated into timestamps. This enables the interface to jump straight to the moment where a claim appeared without duplicating transcript parsing logic elsewhere.

%---

\subsubsection{Claim Extractor}

The Claim Extractor identifies factual, verifiable claims from video transcripts using LLM-based extraction with thesis-relative importance ranking. This approach builds on prior work in automated claim detection: supervised models trained on annotated political debates have been used to detect check-worthy claims \citep{hassan2015claimbuster}, and end-to-end systems like ClaimBuster monitor public discourse and prioritize factual statements for manual fact-checking \citep{li2017claimbuster}. These systems show that focusing on salient, verifiable claims improves the efficiency of fact-checking pipelines.

\paragraph{LLM Configuration}

The claim extractor uses deterministic settings for reproducibility: temperature set to 0.0 to ensure consistent outputs across multiple runs, max tokens limited to 1,200 to control response length and latency, and automatic retry logic (3 attempts) to handle transient LLM failures gracefully.

\paragraph{Prompt Engineering Strategy: Thesis-First Approach}

The claim extractor employs a novel \textit{thesis-first approach} with multi-step reasoning designed to prioritize claims most critical to the video's central argument:

\textbf{Step 1: Thesis Inference} --- Before listing claims, the LLM infers the video's central thesis in no more than 25 words (e.g., ``Climate change alarmism is driven more by politics and media than by settled science'').

\textbf{Step 2: Importance Ranking with Thesis Impact Test} --- Claims are scored based on their impact on the video's thesis using the question: ``If this claim were proven false, would the thesis collapse or materially weaken?'' Table \ref{tab:importance-scoring} presents the scoring guidelines.

\begin{table}[H]
\centering
\caption{Claim importance scoring guidelines}
\label{tab:importance-scoring}
\small
\begin{tabular}{p{2.5cm}p{5.5cm}p{5cm}}
\toprule
\textbf{Score Range} & \textbf{Description} & \textbf{Examples} \\
\midrule
0.85--1.0 & Prescriptive/causal claims undermining thesis & Policy proposals, causal mechanisms \\
0.60--0.80 & Quantitative/historical evidence tied to thesis & Statistics, dates, expert citations \\
0.30--0.55 & Context/supporting background & Definitions, general facts \\
0.0--0.25 & Peripheral/anecdotal details & Personal stories, credentials \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Step 3: Relevance Guardrails} --- Pure credential facts are capped at 0.30 unless the thesis questions expertise; statements not affecting the thesis are capped at 0.25; pure opinions are excluded; and paraphrases and duplicate numbers are removed.

\paragraph{Prompt Engineering Techniques}

The system employs several established prompting techniques across its components:

\begin{itemize}
    \item \textbf{Role/system prompting}: Each component receives a focused system prompt that spells out its role, constraints, and expected outputs for consistent behavior.
    \item \textbf{Structured outputs}: Responses are constrained to typed schemas so downstream stages always receive validated fields.
    \item \textbf{Chain-of-thought reasoning}: The thesis-first steps require the model to state the thesis and then rate claims against it, which improves prioritization.
    \item \textbf{Dynamic instruction injection}: Runtime parameters such as \texttt{max\_claims} are inserted directly into prompts so component behavior adapts without duplicating templates.
    \item \textbf{Deterministic generation}: Temperature stays at 0.0 across the pipeline for reproducible, debuggable outputs.
    \item \textbf{Zero-shot prompting}: Components receive task instructions without example I/O pairs, minimizing prompt length and avoiding format bias.
\end{itemize}

\paragraph{Post-Processing: Fuzzy Claim Localization}

After LLM extraction, each claim is located in the original transcript using fuzzy string matching. The algorithm normalizes both the claim text and transcript, then applies a sliding window approach (with $\pm 2$ words tolerance around the expected claim length) to find the best matching region. Similarity is computed using Python's sequence matching algorithm, which calculates the ratio of matching characters between two strings. A minimum similarity score of 0.5 is required for a valid match. This process yields the character-level start and end positions within the transcript, along with the match confidence score, enabling precise timestamp mapping from transcript positions back to video timestamps.

%---

\subsubsection{Query Generator}

The Query Generator produces diverse, prioritized search queries tailored to each claim. A single LLM call emits direct restatements, synonym-heavy variations, source-focused prompts, and broader context queries together with priority tags so downstream steps can respect latency budgets. Guardrails ensure the suggestions include relevant entities, time periods, and qualifiers while capping each claim at a manageable number of high-yield searches.

\paragraph{Query Type Taxonomy}

The system generates four types of queries with different search strategies, as shown in Table \ref{tab:query-types}.

\begin{table}[H]
\centering
\caption{Query type taxonomy}
\label{tab:query-types}
\small
\begin{tabular}{p{2.3cm}p{4cm}p{3cm}p{4cm}}
\toprule
\textbf{Type} & \textbf{Description} & \textbf{Strategy} & \textbf{Example} \\
\midrule
DIRECT & Exact claim phrasing & Verbatim search & ``unemployment rose 15\% Q3 2024'' \\
ALTERNATIVE & Rephrased with synonyms & Semantic variation & ``jobless rate increase third quarter'' \\
SOURCE & Target authoritative sources & Source-seeking & ``BLS unemployment statistics Q3'' \\
CONTEXT & Broader context & Background search & ``economic indicators fall 2024'' \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Priority System}

Queries are prioritized 1--5 based on likelihood of finding reliable, definitive information: priority 1 queries are always included, priority 2 by default, priority 3 if budget allows, and priorities 4--5 rarely or only for completeness.

\paragraph{Context-Aware Query Generation}

Beyond factual accuracy, the query generator is designed to detect misleadingly framed claims---statements that may be technically accurate but presented without essential context. The system prompt instructs the LLM to respect temporal context when choosing keywords (e.g., including relevant years or qualifiers to avoid mixing eras), and to explicitly generate queries seeking counter-arguments or opposing views. This approach helps surface evidence that may qualify, limit, or contextualize the original claim, enabling more nuanced verdict generation.

%---

\subsubsection{Online Search}

The Online Search component implements a multi-step pipeline to retrieve, assess, and extract evidence from web sources with adaptive quality filtering. Unlike the other LLM-based components, Online Search orchestrates multiple classical algorithms alongside a single LLM call for evidence extraction. This hybrid approach balances speed, reliability, and reasoning capabilities.

The reliability assessment combines domain-level heuristics with the Media Bias/Fact Check (MBFC) methodology, which employs a comprehensive weighted scoring system to evaluate media outlets' ideological bias and factual reliability \citep{mbfc2024methodology}. These scores, combined with stance-aware evidence extraction instructions, keep unreliable or speculative passages from flowing downstream.

Figure \ref{fig:online-search-pipeline} illustrates the four-step Online Search pipeline executed for each query.

\begin{figure}[H]
\centering
\begin{tikzpicture}[
    node distance=1.2cm,
    box/.style={rectangle, draw=green!60, fill=green!5, thick, minimum width=10cm, minimum height=1cm, align=center, font=\small},
    arrow/.style={-{Stealth[length=3mm]}, thick},
]

% Step 1
\node[box] (step1) {
    \textbf{Step 1: Google Search}\\
    Serper API $\rightarrow$ GoogleSearchHits (title, URL, snippet)
};

% Step 2
\node[box, below=of step1] (step2) {
    \textbf{Step 2: Reliability Assessment}\\
    MBFC + Domain Heuristics $\rightarrow$ SiteReliability (rating, score, bias)
};

% Step 3
\node[box, below=of step2] (step3) {
    \textbf{Step 3: Content Fetching}\\
    Selenium WebDriver $\rightarrow$ PageText (with JS rendering)
};

% Step 4
\node[box, below=of step3] (step4) {
    \textbf{Step 4: Evidence Extraction (LLM)}\\
    Analyze content $\rightarrow$ EvidenceSummary + Stance (supports/refutes/mixed/unclear)
};

% Arrows
\draw[arrow] (step1) -- (step2);
\draw[arrow] (step2) -- (step3);
\draw[arrow] (step3) -- (step4);

% Parallelization note
\node[below=0.3cm of step4, font=\scriptsize, orange, align=center] {
    \textit{All K results per query execute Steps 2-4 in parallel (Level 3)}
};

\end{tikzpicture}
\caption{Online Search pipeline showing the four sequential steps executed for each search result. Steps 2--4 run in parallel across all K results per query (Level 3 parallelization).}
\label{fig:online-search-pipeline}
\end{figure}

\paragraph{Step 1: Google Search (Serper API)}

The Google search client wraps the Serper API for asynchronous search execution. The implementation uses persistent HTTP connections for performance and returns structured search hits containing title, URL, and snippet fields. Each query can retrieve up to 10 results, with the limit parameter controlling the exact number returned. The async design enables parallel query execution across multiple claims simultaneously.

\paragraph{Step 2: Website Reliability Assessment}

The reliability checker uses a multi-factor scoring system with first-match priority, combining external datasets with algorithmic heuristics:

\begin{itemize}
    \item \textbf{Media Bias/Fact Check (MBFC) Dataset}: The system loads timestamped JSON snapshots containing nearly 10,000 news sources with credibility ratings (dataset extracted December 2025). Credibility mappings are: high $\rightarrow$ 0.85, medium $\rightarrow$ 0.60, low $\rightarrow$ 0.30, very low $\rightarrow$ 0.15.

    \item \textbf{TLD Reputation}: High-trust top-level domains (\texttt{.gov}, \texttt{.edu}, \texttt{.int}) receive a base score of 0.90, reflecting their institutional authority.

    \item \textbf{Domain Age via WHOIS}: Domains $\geq$10 years old receive a +0.10 bonus (established presence), while domains $<$1 year old receive a --0.15 penalty (recent creation may indicate lower trust).
\end{itemize}

The output includes a categorical rating (high, medium, low, unknown), numerical score (0.0--1.0), reasoning for the assessment, and political bias classification when available from MBFC data.

\paragraph{Step 3: Content Fetching (Selenium)}

Content fetching uses headless Chrome with smart waits: the scraper first grabs paragraph elements immediately, then allows extra rendering time only when less than 100 characters were captured. Images are disabled, page-load and wait timeouts cap the work (20 seconds and 12 seconds, respectively), and the cleaned text is trimmed to 8,000 characters before being passed to the LLM. Blocking Selenium calls are delegated to background threads so multiple results can be processed in parallel without freezing the async loop.

\paragraph{Step 4: Evidence Extraction (LLM)}

The evidence extractor analyzes retrieved content against claims using structured stance definitions:

\begin{itemize}
    \item \textbf{SUPPORTS}: Evidence confirms or validates the claim through direct statements, semantic equivalents, or mechanism descriptions.
    \item \textbf{REFUTES}: Evidence contradicts or disproves the claim through counter-evidence or statements that evidence is unproven/disproven.
    \item \textbf{MIXED}: Both supporting and refuting elements present.
    \item \textbf{UNCLEAR}: Genuinely ambiguous content that discusses related topics without addressing the specific claim.
\end{itemize}

Critical prompt instructions ensure that mere discussion equals UNCLEAR, that mechanisms are recognized even without exact terminology, and that both Google snippets and page content are considered with better evidence prioritized. Importantly, the evidence extractor pays special attention to qualifiers such as ``only'', ``never'', ``always'', and temporal scope limitations (e.g., ``since X date'')---this enables detection of claims that may be technically accurate but misleadingly framed due to omitted context or overgeneralization.

\paragraph{Adaptive Credibility Filtering}

The search orchestrator implements adaptive credibility filtering as a key innovation for ensuring evidence quality. The algorithm operates in three phases:

\begin{enumerate}
    \item \textbf{Initial Batch}: Fetch 2$\times$ the desired limit to provide filtering margin
    \item \textbf{Quality Check}: If $>$50\% of results are unreliable, fetch an additional batch to increase the pool of high-quality sources
    \item \textbf{Intelligent Filtering}: Sort all results by reliability score and select the top reliable sources, with a minimum guarantee ensuring at least some results are returned even if reliability is universally low
\end{enumerate}

Additional filtering mechanisms include stance filtering (removing unclear results if $>$50\% have definitive stances) and URL deduplication using sets to prevent duplicate sources across different queries for the same claim.

%---

\subsubsection{Output Generator}

The Output Generator synthesizes evidence into coherent verdicts with confidence levels, quality scoring, and timestamp mapping.

\paragraph{Two-Step Process}

The Output Generator employs a hybrid approach combining algorithmic evidence organization with LLM-based synthesis:

\textbf{Step 1: Build Evidence Bundle (Algorithmic)} --- The system groups evidence by stance (supports, refutes, mixed, unclear), deduplicates sources by URL, and sorts within each group by reliability rating (high first), then numerical score, with alphabetic tie-breaking for consistency.

\textbf{Step 2: Generate Verdict (LLM)} --- Organized evidence is formatted into a structured prompt containing stance labels, source counts, reliability ratings, and evidence summaries. The system prompt instructs the LLM to synthesize concise verdicts, naming sources explicitly only when clarifying contrasting perspectives or when evidence directly conflicts. This reduces verbosity while maintaining attribution transparency.

\paragraph{Evidence Quality Score (Algorithmic)}

The quality score is calculated algorithmically without LLM involvement for consistency and speed. The scoring formula combines three components with different weights: a base score of 0.3 for having any evidence, an actionable stance bonus of up to 0.3 (scaled by the number of supports/refutes/mixed sources, saturating at 3 sources), and a reliability bonus of up to 0.4 (scaled by the number of high/medium reliability sources, saturating at 3 sources). This design prioritizes both actionable stances and source reliability, with the maximum achievable score of 1.0 indicating high-quality, decisive evidence from multiple reliable sources.

Table \ref{tab:quality-score} summarizes the quality score components.

\begin{table}[H]
\centering
\caption{Evidence quality score breakdown}
\label{tab:quality-score}
\begin{tabular}{lll}
\toprule
\textbf{Component} & \textbf{Weight} & \textbf{Criteria} \\
\midrule
Base & 0.3 & Having any evidence \\
Actionable & 0.3 & Up to 3 supports/refutes/mixed sources \\
Reliability & 0.4 & Up to 3 high/medium reliability sources \\
\textbf{Maximum} & \textbf{1.0} & \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Integrated Verdict Generation (within Level 1)}

Verdicts are generated immediately after each claim's evidence collection completes, within the same parallel execution context as the claim processing. This design choice eliminates the latency overhead of waiting for all claims to finish evidence collection before beginning verdict synthesis. Each claim's verdict generation executes as soon as its evidence is ready, allowing early-finishing claims to produce results while slower claims continue processing. After all parallel claim tasks complete, the reports are sorted by evidence quality score (descending) to prioritize high-confidence verdicts in the user interface.

%-------------------------------------------------------
\subsection{Latency Optimization Strategies}
%-------------------------------------------------------

The pipeline ships with a concrete set of latency optimizations. The following subsections describe what was actually built: tuned model choices, strict token budgets, trimmed inputs, single-call components, parallel execution, real-time streaming updates, and classical fallbacks for non-reasoning tasks. Together these measures reduced mean end-to-end latency to 129.6 seconds for the evaluation set.

\subsubsection{Model Selection}

The default model (gpt-4o-mini) is selected for its balanced performance across speed, cost-effectiveness, and large context window (128K tokens). This model provides sufficient reasoning capabilities for fact-checking tasks while maintaining low latency and competitive pricing (\$0.15 per million input tokens, \$0.60 per million output tokens). For budget-conscious deployments or offline operation, local Ollama models (qwen3:8b, qwen3:4b) offer zero-cost inference at the expense of potential quality degradation.

\subsubsection{Output Constraints}

Each component has carefully tuned \texttt{max\_tokens} limits to minimize generation latency and API costs without sacrificing information quality. Claims are limited to approximately 40 words (sufficient for most factual assertions), context descriptions to 20 words (brief background), and evidence summaries to 1--2 sentences (key findings only). These constraints are enforced through explicit prompt instructions and validated against output token limits.

\subsubsection{Content Trimming}

Input token counts are minimized through aggressive content trimming strategies. Web content is trimmed to 6,000--8,000 characters before being passed to the Evidence Extractor, removing excessive context while retaining the most relevant portions (typically the first several paragraphs of an article). Evidence prompts include only Google snippets and extracted page text, explicitly excluding raw HTML, JavaScript, CSS, and other non-content elements that would inflate token counts without improving extraction quality.

\subsubsection{Combined Operations}

The pipeline minimizes LLM API calls by combining operations into single requests wherever possible. Each component makes exactly one LLM call per input unit (one call for claim extraction, one per claim for query generation, one per search result for evidence extraction, and one per claim for verdict synthesis), with no multi-turn conversations that would multiply request counts.

\subsubsection{Three-Level Async Architecture}

The system implements three levels of nested parallelization to maximize throughput while maintaining dependency ordering:

\begin{itemize}
    \item \textbf{Level 1 (Claims)}: After extracting N claims from the transcript, all claims are processed in parallel. Each claim independently proceeds through query generation, evidence search, and verdict generation. Critically, each claim's verdict is generated immediately after its evidence collection completes, rather than waiting for all claims to finish---this optimization reduces perceived latency by producing results progressively.

    \item \textbf{Level 2 (Queries per Claim)}: Within each claim's processing, the Query Generator produces M queries. These queries are executed in parallel, enabling simultaneous search across different query formulations (direct, alternative, source-seeking, contextual).

    \item \textbf{Level 3 (Search Results per Query)}: Within each query's execution, the Online Search component retrieves K results from Google. The four-step pipeline (reliability assessment, content fetching, and evidence extraction) runs in parallel for all K results, with each result processed independently.
\end{itemize}

This design achieves maximum theoretical parallelization of $N \times M \times K$ operations during the search phase, bounded only by system resources and API rate limits.

\subsubsection{Real-Time Streaming}

Server-Sent Events (SSE) provide progressive updates as the pipeline executes, improving perceived responsiveness for users. Each update labels the active stage (e.g., ``claim extraction'', ``processing\_claim\_2'', ``generating\_report'') alongside a percentage so the UI can display the exact component currently running. Extracted claims are streamed immediately after the Claim Extractor finishes, enabling preview and early user feedback while later stages gather evidence.

\subsubsection{Classical Methods for Non-Reasoning Tasks}

Table \ref{tab:classical-methods} shows operations handled by classical algorithms rather than LLMs.

\begin{table}[H]
\centering
\caption{Operations using classical methods}
\label{tab:classical-methods}
\begin{tabular}{lll}
\toprule
\textbf{Operation} & \textbf{Method} & \textbf{Rationale} \\
\midrule
Reliability scoring & Rule-based + MBFC lookup & Faster, deterministic, no API cost \\
Claim localization & Fuzzy string matching & No LLM needed for text search \\
Evidence quality score & Algorithmic calculation & Consistent, fast, reproducible \\
URL deduplication & Hash set & O(1) lookup \\
Stance filtering & Threshold-based & Simple percentage check \\
\bottomrule
\end{tabular}
\end{table}

%-------------------------------------------------------
\subsection{Experimentation and Evaluation Framework}
%-------------------------------------------------------

Evaluating LLM-based fact-checking systems presents unique challenges: outputs are non-deterministic, external dependencies (web search) introduce variability, and traditional benchmarks risk overfitting \citep{ruder2025llmeval}. To address these challenges, a three-component experimentation framework was developed to capture complete execution traces, support batch experimentation, and compute performance metrics.

\subsubsection{Framework Architecture}

The framework follows a linear data flow through three stages:

\begin{enumerate}
    \item \textbf{Experiment Runner}: Executes the fact-checking pipeline on configured videos, invoking the tracking module for each run.
    \item \textbf{Tracking Module}: Captures all execution data---inputs, outputs, LLM calls, timing, and costs---saving structured artifacts for later analysis.
    \item \textbf{Evaluator}: Computes performance metrics including comparisons against ground truth annotations, system efficiency measurements, and source quality assessments.
\end{enumerate}

\subsubsection{Tracking Module}

The tracking module implements a singleton pattern with context manager support, enabling any pipeline component to log data without explicit parameter passing. When initialized, the tracker creates a timestamped run directory and registers itself as the global tracker. The context manager protocol ensures automatic saving on exit.

Each run generates structured artifacts containing: run configuration and parameters, complete records of all LLM calls with prompts, responses, latency and cost, final extracted claims and fact-check verdicts, aggregated timing and cost metrics, and the original video transcript for reference.

LLM call tracking is achieved via a decorator that instruments the Pydantic AI agent methods, automatically recording component name, model, timestamp, latency, token counts, and calculated cost for every inference call.

\subsubsection{Evaluator}

The evaluator computes performance metrics using modular components for each evaluation dimension:

\begin{itemize}
    \item \textbf{Claim extraction metrics}: Precision@k, Recall, F1, and MAP computed using semantic similarity matching between extracted and ground truth claims.
    \item \textbf{Verdict accuracy}: Comparison of system stances against ground truth labels.
    \item \textbf{Evidence retrieval metrics}: Success rate and source reliability distribution based on MBFC credibility ratings.
    \item \textbf{System efficiency}: Latency and cost aggregation across all pipeline components.
    \item \textbf{LLM-as-judge evaluation} (optional): LLM-as-judge is an evaluation paradigm where a language model assesses the quality of outputs from another model (or the same model), providing scores or judgments on dimensions difficult to capture with traditional metrics \citep{raschka2025llmeval}. In this system, LLM-as-judge can evaluate claim relevance, evidence quality, and verdict coherence.
\end{itemize}

Evaluations execute in parallel across multiple videos for efficiency. Results include per-video reports and aggregate statistics (means, standard deviations, distributions) across all evaluated videos.

%-------------------------------------------------------
\subsection{API Layer and User Interface}
%-------------------------------------------------------

\subsubsection{FastAPI Setup}

The API uses FastAPI with CORS middleware configured for local frontend development, supporting common development server ports. API routes are organized under a versioned prefix to enable future API evolution without breaking existing clients.

\subsubsection{Streaming Endpoint with SSE}

The streaming endpoint exposes the fact-checking run as a server-sent event stream. Progress callbacks enqueue updates that the API emits in real time, allowing clients to maintain a single open connection while receiving incremental status messages. Events follow a fixed sequence (transcript extraction, claim extraction, per-claim processing, verdict generation, completion) so the frontend can map each update to user-facing milestones without polling.

\subsubsection{User Interface}

A web-based frontend provides an accessible interface for end users to interact with the fact-checking pipeline. The interface is built with React and communicates with the backend via the streaming API endpoint. Figure~\ref{fig:ui-screenshots} presents the four main interface states.

\begin{figure}[H]
\centering
\begin{subfigure}[b]{0.95\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/main.png}
    \caption{Landing page with YouTube URL input}
    \label{fig:ui-main}
\end{subfigure}

\vspace{0.4cm}

\begin{subfigure}[b]{0.95\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/process.png}
    \caption{Real-time processing with progress updates}
    \label{fig:ui-process}
\end{subfigure}
\caption{User interface: input and processing stages.}
\label{fig:ui-screenshots-1}
\end{figure}

\begin{figure}[H]
\centering
\begin{subfigure}[b]{0.95\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/results.png}
    \caption{Results overview with embedded video player}
    \label{fig:ui-results}
\end{subfigure}

\vspace{0.4cm}

\begin{subfigure}[b]{0.95\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/claim.png}
    \caption{Detailed claim view with verdict and sources}
    \label{fig:ui-claim}
\end{subfigure}
\caption{User interface: results and claim detail views. Each claim shows its verdict, confidence level, and supporting evidence with source reliability ratings.}
\label{fig:ui-screenshots}
\end{figure}

The interface shows transparency by displaying source reliability ratings, confidence levels, and direct links to evidence sources. Users can click ``Jump'' buttons to navigate directly to the video timestamp where each claim was made, enabling quick verification of the original context.

%-------------------------------------------------------
\subsection{Engineering Practices}
%-------------------------------------------------------

The thesis emphasizes data science outcomes, yet maintaining consistent engineering habits keeps experiments reproducible and easier to debug. Practices are kept lightweight and focused on what directly supports the fact-checking pipeline:

\begin{itemize}
    \item \textbf{Typed schemas}: Pydantic models plus strict mypy checks prevent interface drift between components while supplying clear validation errors during runs.
    \item \textbf{Logging \& fallbacks}: Module-level logging records claim/query context, and simple retry rules (e.g., fall back to Google snippets when scraping fails) keep long runs from collapsing on transient issues.
    \item \textbf{Centralized configuration}: Environment variables store secrets, YAML files define experiment batches, and a small Python settings module captures model limits, making it easy to reproduce or tweak runs.
    \item \textbf{Async + background threads}: Claims, queries, and search results fan out via asyncio gather calls, while Selenium/WHOIS work shifts to threads so blocking I/O never stalls the event loop; the same progress callbacks drive the SSE stream for user feedback.
    \item \textbf{Pre-commit hooks}: Lightweight automated checks run before every commit to keep formatting, linting, and type rules consistent.
\end{itemize}
