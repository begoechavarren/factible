%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CHAPTER 2: STATE OF THE ART
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{State of the Art}

Automated fact-checking has evolved significantly since the mid-2010s, progressing from rule-based systems through classical machine learning classifiers and knowledge graph approaches, to neural architectures and contemporary Large Language Models (LLMs). This evolution has been driven by advances in natural language processing, retrieval-augmented generation (RAG), and multi-agent systems that break down fact-checking into specialized, coordinated tasks.

This section examines automated fact-checking systems, emphasizing multi-agent architectures for video content verification. It analyzes the technological foundations of each pipeline component, from claim detection to evidence retrieval to verdict synthesis, and identifies gaps limiting practical deployment, tracing the field's evolution from 2014 to the present.

%-------------------------------------------------------
\subsection{Historical Development of Automated Fact-Checking}
%-------------------------------------------------------

Automated fact-checking emerged as a formal NLP task in the mid-2010s. \citet{vlachos2014fact} provided the foundational task definition, framing fact-checking as a pipeline comprising claim identification, evidence retrieval, and veracity prediction---a structure that remains central to contemporary systems.

Early approaches (2014--2017) relied on two main paradigms. \textbf{Knowledge graph verification} systems, such as those developed by \citet{ciampaglia2015computational}, computed shortest paths through structured knowledge bases like DBpedia to assess claim truthfulness, achieving promising results on well-formed factual statements but struggling with natural language variability. \textbf{Classical machine learning classifiers}, exemplified by ClaimBuster \citep{hassan2017claimbuster}, used SVMs and Random Forests with hand-crafted features including TF-IDF vectors, named entity counts, part-of-speech patterns, and sentiment scores to identify check-worthy claims in political discourse.

The release of benchmark datasets marked a turning point. The LIAR dataset (2017) provided 12,836 PolitiFact claims with six-way veracity labels, while \textbf{FEVER} \citep{thorne2018fever}, with its 185,445 claims verified against Wikipedia, established the dominant evaluation paradigm and formalized the retrieve-then-verify pattern. These datasets enabled systematic comparison of approaches and drove rapid progress.

Pre-transformer neural models (2016--2018) achieved substantial improvements over classical approaches. The Decomposable Attention Model \citep{parikh2016decomposable} and ESIM introduced attention mechanisms for natural language inference, achieving approximately 70\% accuracy on FEVER. However, the introduction of BERT in late 2018 quickly dominated leaderboards, demonstrating that transfer learning from large-scale pretraining could surpass task-specific architectures.

Despite this progress, pre-LLM systems exhibited fundamental limitations that motivate current research directions: (1) \textbf{single-source restriction}---systems verified against only Wikipedia or curated knowledge bases, not diverse web sources; (2) \textbf{no explanation generation}---models predicted labels without human-readable justifications; (3) \textbf{binary/ternary labels}---poor handling of nuanced ``partially true'' claims; (4) \textbf{no multimodal capability}---text-only processing, with video/audio fact-checking treated as a separate field; and (5) \textbf{weak multi-hop reasoning}---claims requiring evidence synthesis across multiple documents frequently failed. These limitations directly inform the design of modern multi-agent LLM architectures, which aim to address each gap through specialized agents, web-scale retrieval, and natural language reasoning.

%-------------------------------------------------------
\subsection{Multi-Agent Architectures for Fact-Checking}
%-------------------------------------------------------

\subsubsection{Foundational Multi-Agent Frameworks}

\textbf{FactAgent} \citep{factagent2025} introduced an agentic workflow that explicitly emulates the methodology of human fact-checkers. Instead of fine-tuning models for specific tasks, FactAgent employs a pre-trained LLM that operates through a structured script: (1) gathering evidence via tools, (2) analysing that evidence, and (3) synthesising a verdict. A key advantage of this approach is its zero-shot nature: the system uses the LLM's existing capabilities without requiring task-specific training data. However, the sequential multi-step process can introduce latency, and errors in early stages may cascade through the pipeline.

\textbf{LoCal} \citep{chen2024local} handles complex claims through a decomposition--reasoning--evaluation loop. It uses specialized agents: a decomposer breaking claims into sub-claims, reasoning agents verifying each with evidence, and evaluators ensuring logical consistency and testing counterfactual scenarios. If inconsistencies arise, agents iteratively revise their reasoning. This targets single-pass verification weaknesses but increases computational cost.

\textbf{Multi-agent debate systems} \citep{ma2025guided, lakara2025madsherlock} use adversarial collaboration where distinct LLM agents take opposing roles: one argues for a claim's truth, another against it, while a judge decides the outcome. This exposes contradictions and forces agents to defend positions with evidence, reducing confirmation bias and hallucinations. The adversarial setup prevents premature convergence on incorrect conclusions, though debates can reach impasses and quality depends on the judge agent's synthesis ability.

\subsubsection{Evolution and Current Landscape}

Multi-agent design for fact-checking emerged recently (2023--2025). Early research used linear pipelines where a single model performed one task at a time. By 2024, works like FactAgent and LoCal began breaking verification into specialized agents \citep{factagent2025, chen2024local}. In 2025, researchers added debate mechanisms, self-reflection, and richer tool use \citep{ma2025guided, icwsm2025workshop, tian2024webagents}. MAD-Sherlock showed that debate-driven systems reduce hallucinations through collaborative verification \citep{lakara2025madsherlock}. Despite progress, recent evaluations reveal latency challenges, high compute requirements, and the need for careful orchestration to avoid loops or premature termination.

%-------------------------------------------------------
\subsection{Automatic Claim Detection in Text}
%-------------------------------------------------------

Identifying which statements warrant fact-checking---known as check-worthiness detection---is crucial for any verification pipeline. For YouTube videos, this means scanning transcripts to flag claims that are both verifiable and important.

\subsubsection{Traditional Supervised Approaches}

Claim detection research began in the mid-2010s, targeting political speech. \textbf{ClaimBuster} \citep{hassan2017claimbuster} pioneered the approach, using supervised models trained on human-labelled debate transcripts to score sentences for verifiable factual claims. Subsequent work refined datasets and models: \textbf{ClaimRank} introduced context-aware modeling using surrounding sentences \citep{gencheva2017claimrank}, while the \textbf{CLEF CheckThat!} lab (2018--present) released multilingual challenges for check-worthiness detection \citep{elsayed2019checkthat}. These efforts established claim detection as a classification problem, with BERT and transformers becoming dominant after 2018. The key insight was that check-worthiness requires assessing both \textbf{importance} and \textbf{verifiability}---a claim like ``Unemployment rose by 15\% last quarter'' is check-worthy because it is both verifiable and consequential.

\subsubsection{LLM-Based Claim Detection}

Recent work explores whether LLMs can identify check-worthy claims without fine-tuning. \citet{sawinski2024comparison} compared fine-tuned BERT variants with GPT-3/GPT-4 in zero-shot mode, finding that simple prompts still underperform fine-tuned models due to inconsistent internal definitions of ``worthiness'' and sensitivity to prompt wording.

However, careful prompt design can substantially improve LLM performance. \citet{li2023llmclaimdetection} demonstrated that LLMs with verbose few-shot prompts can effectively replace traditional claim detectors in automated pipelines. \citet{ni2024threestep} proposed a three-step prompting approach where the LLM analyzes text in stages---highlighting factual statements, applying check-worthiness criteria, and ranking by importance---similar to chain-of-thought reasoning \citep{ni2024verifiable}.

\subsubsection{Current Challenges}

Key challenges include: (1) \textbf{definition ambiguity}: what constitutes a ``check-worthy'' claim varies by context; (2) \textbf{scalability}: scanning long transcripts with LLMs is slow and expensive; (3) \textbf{false positives}: overly aggressive detection wastes resources; and (4) \textbf{domain adaptation}: models trained on political debates may not work for scientific or economic content.

%-------------------------------------------------------
\subsection{RAG Approaches in Automated Fact-Checking}
%-------------------------------------------------------

A core pillar for automated fact-checking is retrieval-augmented generation (RAG), which combines text generation with external information retrieval to ensure outputs are based on verifiable sources.

\subsubsection{From RAG to Fact-Checking}

\citet{lewis2020retrieval} formalized RAG for general knowledge-intensive tasks, showing that augmenting generation with external knowledge retrieval significantly improves performance. For fact-checking, this paradigm is essential: systems must fetch reliable sources that support or refute claims. The \textbf{FEVER} dataset exemplified this retrieve-then-verify pattern: given a claim, retrieve relevant documents (e.g., Wikipedia pages), then determine if they support or refute the claim \citep{thorne2018fever}. Modern systems use LLMs for both retrieval and verification, conditioning their reasoning on retrieved evidence.

RAG addresses a critical limitation of pure LLM approaches: parametric knowledge can be outdated, incomplete, or hallucinated. By retrieving external information (from document indexes, web search, or APIs), RAG systems access current information, provide source attribution, and ground reasoning in verifiable evidence. This is crucial for fact-checking, where claims often reference recent events, specific statistics, or specialized knowledge not in LLM training data.

\subsubsection{Tool Use and Web Retrieval}

Integrating tool use with LLMs has enabled more sophisticated retrieval. The \textbf{ReAct pattern} (Reason and Act) interleaves tool use with chain-of-thought reasoning, letting LLMs decide when to call external tools like search engines \citep{yao2023react}. \textbf{WebGPT} demonstrated training LLMs to use web browsers to answer questions and cite sources, improving factual accuracy by teaching models to navigate search results and synthesize information from multiple pages \citep{nakano2022webgpt}. Similarly, \textbf{Toolformer} showed that LLMs can be fine-tuned to call external tools like search engines or calculators, reducing hallucinations by grounding answers in retrieved evidence \citep{schick2023toolformer}.

Recent work has applied these techniques to fact-checking. \textbf{Chern et al. (2023)} proposed using Google Search, Google Scholar, and other tools to verify LLM-generated text against external sources \citep{chern2023framework}, while \textbf{Cheung and Lam (2023)} combined search-engine retrieval with LLaMA to predict claim veracity \citep{cheung2023llmfactcheck}. These tool-augmented methods address LLMs' inherent knowledge limitations, which can be outdated or incomplete \citep{chern2023framework}.

\subsubsection{Open Web versus Closed Knowledge Bases}

Most academic fact-checking systems restrict retrieval to trusted corpora (primarily Wikipedia) to simplify evaluation and ensure evidence quality. This yields high precision but severely limits real-world coverage \citep{gao2023rarr}. The FEVER dataset, while influential, exemplifies this by assuming all claims can be checked against a June 2017 Wikipedia snapshot, which breaks down for recent events, specialized domains, or claims requiring sources like World Bank reports or CDC guidelines.

\textbf{Tian et al. (2024)} integrated web-retrieval agents into an LLM pipeline and demonstrated improved misinformation detection \citep{tian2024webagents}. However, open-web retrieval introduces challenges: (1) \textbf{source credibility}: not all websites are reliable; (2) \textbf{information quality}: web content varies in accuracy; (3) \textbf{ranking complexity}: identifying relevant sources among millions of candidates; and (4) \textbf{dynamic nature}: content changes, affecting reproducibility.

Current best practices include prioritizing sources with high domain authority (established news organizations, academic institutions, government agencies), cross-referencing multiple independent sources, explicitly evaluating source credibility using metadata (publication date, author credentials, institutional affiliation), and maintaining transparency by exposing retrieved sources to users. Systems like FactAgent incorporate evidence retrieval as a dedicated step, using search tools to query the web and filtering results based on relevance and credibility \citep{factagent2025}.

\subsubsection{Query Optimization for Fact-Checking}

A critical but often overlooked component of RAG systems is query formulation---the same claim can be verified or refuted depending on how search queries are constructed \citep{ma2025guided}. Effective strategies include \textbf{keyword extraction} (identifying salient terms and entities), \textbf{query expansion} (generating variants to capture different phrasings, e.g., ``unemployment rate increased'' alongside ``jobless claims rose'') \citep{lewis2020retrieval}, and \textbf{temporal awareness} (incorporating time constraints for period-specific claims).

Recent multi-agent systems often dedicate a specialized agent to query generation, recognizing that poor queries degrade overall performance regardless of verification model quality. FactAgent includes explicit query formulation as one of its agent steps, using the LLM to generate search-optimized queries \citep{factagent2025}.

%-------------------------------------------------------
\subsection{LLM-Based Claim Verification Methods}
%-------------------------------------------------------

Once claims are identified and evidence retrieved, systems must determine veracity, labelling claims as supported, refuted, mixed, or not enough evidence. Traditional approaches treated this as textual entailment, using classifiers to determine if evidence entails or contradicts claims. With LLMs, a new approach emerged: using models to perform verification through natural language reasoning.

\subsubsection{Prompting Strategies}

\textbf{Zero-shot and few-shot prompting} involves providing an LLM with a claim and evidence, asking it to decide veracity and explain why: ``Claim: X. Evidence: [text]. Based on the evidence, is the claim true or false?'' \citep{zhang2023siren}. In zero-shot mode, the LLM relies on internal reasoning and evidence interpretation. In few-shot mode, the prompt includes examples of claims with evidence and the correct verdict to guide the model.

GPT-4 and similar models show surprising capability at this task, often correctly interpreting whether evidence supports statements. However, LLMs can be overly agreeable, sometimes hallucinating justifications or defaulting to ``Supported'' even when evidence is insufficient \citep{zhang2023siren}. This confirmation bias stems from models' training to be helpful and provide answers, even when saying ``I don't know'' would be more appropriate.

Careful prompt engineering can mitigate this bias. Effective strategies include explicitly instructing the model to answer ``Not Enough Evidence'' or ``Mixed'' when appropriate, adding system messages emphasizing accuracy over helpfulness, requesting citation of specific evidence sentences, using temperature settings near zero to reduce randomness, and implementing multi-pass verification where the model critiques its own reasoning.

\subsubsection{Advanced Reasoning Strategies}

More sophisticated approaches use \textbf{chain-of-thought reasoning} or implement the LLM as an agent in a loop. The \textbf{ReAct pattern} has the LLM explicitly reason step-by-step while using tools \citep{yao2023react}---for verification, this involves breaking claims into parts, querying search engines, evaluating evidence, and synthesizing conclusions. FactAgent exemplifies this approach: the LLM follows a script where each step is explicit and logged for transparency \citep{factagent2025}. Chain-of-thought provides transparency and debuggability, though multiple LLM calls can be slow and errors compound across stages.

Complementary techniques address reliability concerns. \textbf{SelfCheckGPT} detects hallucinations by generating multiple independent answers and checking consistency---hallucinated information varies across samples while grounded information remains stable \citep{manakul2023selfcheckgpt}. \textbf{LLM-as-a-judge} approaches use one model to generate answers and another to verify them \citep{raschka2025llmeval, ruder2025llmeval}, while \textbf{cross-model checking} uses different models that can abstain when they disagree \citep{aly2021feverous}. Many pipelines also incorporate \textbf{stance detection} to classify whether evidence supports, refutes, or is neutral toward claims \citep{thorne2018fever}. While these techniques improve reliability, they add computational overhead.

\subsubsection{Current Capabilities and Limitations}

Carefully prompted LLMs can achieve near state-of-the-art performance on tasks like FEVER \citep{thorne2018fever}. However, they still make mistakes, especially on ambiguous or complex claims requiring specialized knowledge or multi-step reasoning. Designing prompts or agent behaviors to be appropriately skeptical remains important, requiring calibration to avoid being too trusting or too skeptical.

%-------------------------------------------------------
\subsection{Evaluation of Fact-Checking Systems}
%-------------------------------------------------------

Evaluating automated fact-checking systems requires assessing accuracy, explanation quality, evidence usage, and practical usability.

\subsubsection{Accuracy and Retrieval Metrics}

Standard classification metrics include accuracy, F1-score, and precision/recall. The FEVER challenge introduced the \textbf{FEVER score}, which requires both correct labels and proper evidence, penalizing systems that get labels right without grounding \citep{thorne2018fever}. For evidence retrieval, key metrics include \textbf{Recall@k}, \textbf{Precision}, and \textbf{Mean Average Precision (MAP)} \citep{thorne2018fever}. End-to-end evaluations typically credit systems only when they retrieve human-identified evidence, though multiple valid sources may exist for a claim.

\subsubsection{Explanation Quality and Consistency}

Explanations should be \textbf{faithful} (reflect actual reasoning) and \textbf{factually consistent} with evidence. While automatic metrics like BLEU or ROUGE exist, they don't measure factuality well \citep{raschka2025llmeval}. \textbf{LLM-as-a-judge} has become popular for scoring explanation coherence and factuality, though these judges require validation against human assessments \citep{raschka2025llmeval, ruder2025llmeval}. \textbf{Stance consistency} checks whether evidence stances align with final verdicts, while systems like LoCal evaluate logical consistency by checking if composed solutions imply claim veracity \citep{chen2024local}.

\subsubsection{Human and Computational Evaluation}

Human judgment remains the gold standard, with experts rating verdict correctness and lay users evaluating explanation clarity \citep{ma2025guided, wardle2017information}. For practical deployment, computational efficiency also matters: latency, throughput, and cost metrics are critical for operational systems, though rarely reported in academic papers \citep{factagent2025, tian2024webagents}.

%-------------------------------------------------------
\subsection{Current Limitations and Research Opportunities}
%-------------------------------------------------------

Despite rapid progress, automated fact-checking systems have significant limitations constraining practical deployment.

\subsubsection{Lack of End-to-End Usability}

Most research prototypes focus on isolated components rather than seamless end-to-end tools. Some excel at claim detection but assume manual verification \citep{hassan2017claimbuster}, while others verify claims but require human identification. Even ClaimBuster, dubbed ``end-to-end'', only highlighted claims without verification \citep{hassan2017claimbuster}. Complete systems need to integrate detection, verification, and source tracing, but existing systems typically address only one or two steps \citep{lin2025factaudit}. For YouTube videos, true end-to-end systems should handle transcription extraction, claim detection, evidence retrieval, verification, and user-friendly presentation, but this integration is rarely achieved \citep{lin2025factaudit}.

\subsubsection{Dependence on Structured Sources}

Much research restricts evidence to structured knowledge bases, primarily Wikipedia. While this yields cleaner evaluation, it severely limits applicability \citep{aly2021feverous}. Real misinformation often requires specialized sources not available in Wikipedia, and systems benchmarked on FEVER tend to be overfitted \citep{thorne2018fever}. In practice, fact-checkers must handle the open web including news sites, scientific papers, and government databases, introducing challenges of source credibility and information quality \citep{aly2021feverous}.

\subsubsection{Production Readiness and Scalability}

Most solutions remain research-grade implementations rather than production-ready systems. Code is typically provided as research artifacts without the robustness or user interfaces needed for public deployment \citep{lin2025factaudit}. Even public tools like Google Fact Check Explorer only search existing fact-checks rather than performing new verification \citep{google2024factcheck}. Scalability poses additional challenges: if verification takes minutes per claim, videos with 20 claims become impractical for interactive use. Cost is also a factor---using commercial LLMs for every step can be prohibitively expensive at scale \citep{lin2025factaudit}.

\subsubsection{Trust, Transparency, and Model Selection}

Users may be reluctant to trust AI verdicts without understanding their derivation, and many systems have been criticized as ``black boxes'' \citep{wardle2017information}. While multi-agent systems and chain-of-thought approaches attempt to address this via explicit reasoning traces, they face risks of hallucinated explanations. Additionally, despite many LLM options (commercial models like GPT-4 or Claude; open-source models like LLaMA, Qwen, DeepSeek), there's limited systematic comparison of their suitability for fact-checking tasks---research tends to use whichever model is most accessible without careful comparison of trade-offs \citep{raschka2025llmeval}.

\subsubsection{The Video Content Gap}

Most fact-checking research focuses exclusively on text-based content, primarily analyzing written articles, social media posts, or pre-extracted claims from political debates \citep{thorne2018fever, hassan2017claimbuster}. YouTube, despite having over 2 billion monthly active users and serving as a primary information source for millions, remains largely unaddressed by automated fact-checking systems. Existing research assumes pre-processed text, leaving the video-to-claim pipeline unsolved. Video content introduces unique challenges including automatic speech recognition errors, missing punctuation and formatting, temporal context loss, and high computational costs. Given YouTube's central role in information consumption and its documented contribution to misinformation spread \citep{benkler2018network}, this gap represents a critical limitation in automated fact-checking capabilities.
