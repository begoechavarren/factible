\documentclass[12pt,a4paper]{article}

% Essential packages
\usepackage{graphicx}
\usepackage{setspace}
\usepackage[numbers,sort&compress]{natbib}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{color}
\usepackage{hyperref}
\usepackage{amsmath,amssymb}
\usepackage{geometry}
\usepackage{longtable}
\usepackage{caption}
\usepackage{tikz}
\usepackage{adjustbox}
\usetikzlibrary{arrows.meta, positioning, shapes.geometric, calc}

% Margin settings
\geometry{
    a4paper,
    left=3cm,
    right=2.5cm,
    top=2.5cm,
    bottom=2.5cm
}

% Hyperref configuration
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=blue,
    pdftitle={State of the Art TFM - Multi-Agent System for Fact-Checking},
    pdfauthor={Begoña Echavarren Sánchez}
}

% Spacing
\onehalfspacing

\begin{document}

\begin{titlepage}
    \begin{center}
    \vspace*{0.5cm}

    \includegraphics[width=0.6\textwidth]{../../assets/uoc_logo.png}

    \vspace{1.5cm}

    {\Huge \textbf{State of the Art Report}}

    \vspace{2cm}

    {\LARGE \textbf{Multi-Agent System for Automated Fact-Checking of YouTube Videos}}

    \vspace{2.5cm}

    \textbf{\large Begoña Echavarren Sánchez}

    \vspace{0.5cm}

    \textit{Tutor: Josep-Anton Mir Tutusaus}

    \vspace{2cm}

    {\large Master's Degree in Data Science}

    {\large Universitat Oberta de Catalunya}

    \vspace{1cm}

    {\large PEC 2 - State of the Art}

    \vspace{0.5cm}

    {\large November 2025}

    \end{center}
\end{titlepage}

\newpage
\pagenumbering{arabic}

\hypersetup{linkcolor=black}
\tableofcontents
\hypersetup{linkcolor=blue}

\newpage

\section{Introduction}

The proliferation of misinformation on digital platforms has become one of the defining challenges of the modern information age. YouTube, with more than 2 billion monthly active users and over 500 hours of video uploaded every minute, has emerged as a primary source of information for millions of people worldwide. However, the absence of effective verification mechanisms allows false or misleading claims to spread at massive scale, with documented impacts on public health, democratic processes, and social cohesion \citep{aly2021feverous, benkler2018network, wardle2017information}.

Automated fact-checking has evolved significantly since the mid-2010s, transitioning from rule-based systems and supervised classifiers to sophisticated architectures leveraging Large Language Models (LLMs). This evolution has been driven by advances in natural language processing, the emergence of retrieval-augmented generation (RAG) paradigms, and, more recently, the development of multi-agent systems that decompose the complex fact-checking workflow into specialized, coordinated sub-tasks.

This state of the art review examines the current landscape of automated fact-checking systems, with particular emphasis on multi-agent architectures and their application to video content verification. We analyse the technological foundations that enable each component of the fact-checking pipeline—from claim detection to evidence retrieval to verdict synthesis—and identify critical gaps that limit the practical deployment of existing systems. The review covers literature primarily from 2017 to 2025, focusing on systems that combine LLMs with information retrieval and structured reasoning.

\section{Multi-Agent Architectures for Fact-Checking}

\subsection{Foundational Multi-Agent Frameworks}

\textbf{FactAgent} \citep{hysonlab2025factagent} introduced an agentic workflow that explicitly emulates the methodology of human fact-checkers. Rather than fine-tuning models for specific tasks, FactAgent employs a pre-trained LLM that operates through a structured script: (1) gathering evidence via tools or internal knowledge, (2) analysing that evidence, and (3) synthesising a verdict. Each step is performed sequentially with explicit reasoning traces, enabling transparency and debuggability. A key advantage of this approach is its zero-shot nature—the system leverages the LLM's existing capabilities without requiring task-specific training data. However, the sequential multi-step process can introduce latency, and errors in early stages may cascade through the pipeline.

\textbf{LoCal} \citep{chen2024local} addresses complex claims requiring multi-hop reasoning through a decomposition–reasoning–evaluation loop. The system employs multiple specialised agents: a decomposer that breaks complex claims into simpler sub-claims, reasoning agents that verify each sub-claim by retrieving evidence, and evaluator agents that ensure logical consistency and test counterfactual scenarios. If inconsistencies are detected, agents iteratively revise their reasoning. This approach explicitly targets the brittleness of single-pass verification, aiming to ensure verdicts are logically sound and robust against perturbations, albeit at the cost of additional computational expense.

\textbf{Multi-agent debate systems} \citep{ma2025guided, lakara2025madsherlock} introduce adversarial collaboration where distinct LLM agents are assigned opposing roles—one arguing for a claim's veracity, another against it—with a judge agent ultimately deciding the outcome. This debate mechanism surfaces contradictions and forces each agent to defend its position with evidence, thereby reducing confirmation bias and hallucinations. The adversarial setup can improve factual reliability by preventing premature convergence on incorrect conclusions, although debates can reach stalemates and overall quality depends heavily on the judge agent's ability to synthesise competing arguments.

\subsection{Evolution and Current Landscape}

The shift to multi-agent design for fact-checking is recent (2023–2025). Early fact-checking research typically followed linear pipelines where a single model performed one task at a time. By 2024, works such as FactAgent and LoCal began modularising verification steps into specialised agents \citep{hysonlab2025factagent, chen2024local}. In 2025, researchers integrated debate mechanisms, self-reflection capabilities, and richer tool use to improve reliability \citep{ma2025guided, icwsm2025workshop, tian2024webagents}. MAD-Sherlock demonstrated that debate-driven agent systems reduce hallucinations through collaborative verification \citep{lakara2025madsherlock}. Despite clear progress, empirical evaluations still reveal latency challenges, high compute requirements, and the need for rigorous orchestration strategies to avoid loops or premature termination in complex verification scenarios.

\section{Automatic Claim Detection in Text}

Identifying which statements in content are ``claims'' worthy of fact-checking is a crucial first step in any verification pipeline. In the context of YouTube videos, this means scanning transcripts to flag factual assertions that are verifiable and sufficiently important to check. This task is known as claim check-worthiness detection.

\subsection{Traditional Supervised Approaches}

Academic interest in claim detection began in the mid-2010s, primarily targeting political speech. \textbf{ClaimBuster} was pioneering work, touted as the ``first-ever end-to-end fact-checking system'' \citep{hassan2021claimbuster}. ClaimBuster provided a supervised model trained on human-labelled debate transcripts to score each sentence for likelihood of containing a verifiable factual claim. The system used feature engineering and early neural networks (pre-transformer era) to detect check-worthy statements, achieving sufficient accuracy to be integrated by fact-checking organisations such as Duke Reporters' Lab \citep{hassan2021claimbuster}. ClaimBuster effectively automated the initial triage step, helping journalists prioritise which statements from live debates or speeches merited human verification.

Subsequent research created refined datasets and models. The \textbf{ClaimRank} dataset expanded on U.S. presidential debate data and introduced context-aware modelling, considering surrounding sentences to improve detection \citep{gencheva2017claimrank}. The \textbf{CLEF CheckThat!} lab (2018--2022) annually released challenges with datasets of social media posts or political statements in multiple languages, labelled for check-worthiness \citep{thorne2018fever}. These efforts established claim detection as a text classification or ranking problem, with BERT and other transformer models becoming dominant after 2018. The key insight from this era was that check-worthiness is not just about identifying factual statements, but also about assessing their \textbf{importance} and \textbf{verifiability}. A statement like ``the sky is blue'' is factual but not check-worthy due to its obviousness. Conversely, ``unemployment rose by 15\% last quarter'' is both factual and check-worthy because it is verifiable and consequential. Training supervised models to capture this nuance required carefully annotated datasets with clear labelling guidelines.

\subsection{LLM-Based Claim Detection}

Recent work has explored whether large language models can identify check-worthy claims without fine-tuning. \textbf{Sawinski et al. (2023) and Hyben et al. (2023)} compared fine-tuned BERT variants with GPT-3/GPT-4 in zero-shot or few-shot mode for claim detection and check-worthiness classification \citep{sawinski2024comparison}. Their findings indicate that naïve zero-shot LLM prompts still underperform fine-tuned smaller models on established benchmarks. LLMs often have inconsistent internal definitions of ``worthiness'' and are sensitive to prompt wording, whereas fine-tuned models have learned explicit criteria from labelled data.

However, carefully crafted prompts can improve LLM performance substantially. \textbf{Li et al. (2023)} built a fully automated fact-checking prototype that included an LLM-based claim detection module, using GPT-3 with verbose few-shot prompts to identify claims in input text \citep{li2023llmclaimdetection}. While quantitative metrics for this step were not reported, the work demonstrated feasibility of using LLMs as drop-in replacements for dedicated claim detectors.

\textbf{Ni et al. (2024)} proposed a three-step prompting approach to improve consistency in claim identification \citep{ni2024threestep}. The method has the LLM analyse text in stages—first highlighting all factual statements, then applying check-worthiness criteria, and finally ranking by importance—essentially decomposing the decision similar to chain-of-thought reasoning. This approach improved consistency but focused on verifiable claim identification (whether a claim is objectively checkable) rather than worthiness ranking \citep{ni2024verifiable}.

\subsection{Current Challenges and Hybrid Approaches}

Key challenges in claim detection include:

\begin{enumerate}
    \item \textbf{Definition ambiguity} — what constitutes a ``check-worthy'' claim varies by context and application.
    \item \textbf{Scalability} — scanning long transcripts with LLMs can be slow and expensive.
    \item \textbf{False positives} — overly aggressive detection wastes verification resources.
    \item \textbf{Domain adaptation} — models trained on political debates may not generalise to scientific or economic content.
\end{enumerate}

A hybrid approach appears promising for practical systems: lightweight classifiers (fine-tuned transformers) can provide initial filtering for efficiency, with LLMs performing a second pass for nuanced judgement on borderline cases. This two-stage detection aligns with industry practice where automated systems highlight candidates for human fact-checkers to make final decisions \citep{hassan2021claimbuster}. Such architectures balance the speed and consistency of fine-tuned models with the flexibility and reasoning capabilities of LLMs.

\section{Retrieval-Augmented Generation and Information Retrieval}

A core technological pillar for automated fact-checking is retrieval-augmented generation (RAG), which combines text generation models with external information retrieval to ground outputs in verifiable sources rather than relying purely on parametric memory.

\subsection{RAG Fundamentals}

\textbf{Lewis et al. (2020)} formalised RAG by showing that augmenting generation with a non-parametric memory (a document index) significantly improves performance on knowledge-intensive tasks \citep{lewis2020retrieval}. In fact-checking, RAG is nearly essential: to verify a claim, systems must fetch reliable sources that either support or refute it. The original \textbf{FEVER} dataset exemplified this retrieve-then-verify pattern: given a claim, the system retrieved Wikipedia pages, then a classifier determined if the claim was supported or refuted by those pages \citep{thorne2018fever}. Modern systems replace classifiers with LLMs but maintain the same fundamental architecture of conditioning verification on retrieved evidence.

The RAG paradigm addresses a critical limitation of pure LLM-based approaches: parametric knowledge can be outdated, incomplete, or hallucinated. By retrieving and conditioning on external documents, RAG systems can access current information, provide source attribution, and ground their reasoning in verifiable evidence. This is particularly crucial for fact-checking, where claims often reference recent events, specific statistics, or domain-specific knowledge that may not be well represented in an LLM's training data.

\subsection{Tool Use and Web Retrieval}

The integration of tool use with LLMs has enabled more sophisticated retrieval strategies. The \textbf{ReAct pattern} (Reason and Act) interleaves tool use with chain-of-thought reasoning, allowing LLMs to decide when to call external tools like search engines based on their current reasoning state \citep{yao2023react}. This pattern represents a significant advance over fixed retrieve-then-verify pipelines, as the LLM can adaptively determine what information it needs and how to query for it.

\textbf{WebGPT} demonstrated the effectiveness of training LLMs to use web browsers to answer questions and cite sources, greatly improving factual accuracy over vanilla GPT-3 \citep{nakano2022webgpt}. The key innovation was teaching the model not just to search, but to navigate search results, click through to promising sources, and synthesise information from multiple pages while maintaining proper attribution.

Similarly, \textbf{Toolformer} showed that LLMs can be fine-tuned to decide when to call external tools such as search engines or calculators to obtain factual information, reducing hallucinations by grounding answers in retrieved evidence \citep{schick2023toolformer}. The model learns to recognise when its parametric knowledge is insufficient and explicitly invoke tools to fill knowledge gaps.

\textbf{Chern et al. (2023)} proposed a framework using Google Search, Google Scholar, code interpreters, and other tools to fact-check LLM-generated text, verifying outputs against external sources \citep{chern2023framework}. \textbf{Cheung and Lam (2023)} similarly combined search-engine retrieval with LLaMA to predict claim veracity, with the LLM using retrieved web information to make judgements rather than relying solely on training data \citep{cheung2023llmfactcheck}. These tool-augmented methods were motivated by limitations of LLMs' inherent knowledge, which can be outdated or incomplete for real-world claims \citep{chern2023framework}.

\subsection{Open Web versus Closed Knowledge Bases}

Most academic fact-checking systems restrict retrieval to trusted corpora (Wikipedia being predominant) to simplify evaluation and ensure evidence quality. This approach yields high precision in closed-domain settings but severely limits real-world coverage \citep{gao2023rarr}. For example, economic claims might require World Bank reports, medical claims need CDC guidelines, and breaking news requires recent articles—none available in static Wikipedia dumps.

The FEVER dataset, while influential, exemplifies this limitation by assuming all verifiable claims can be checked against a June 2017 Wikipedia snapshot. This assumption breaks down for claims about recent events, specialised domains not well-covered in Wikipedia, or claims requiring synthesis of information across multiple specialised sources.

\textbf{Tian et al. (2024)} integrated web-retrieval agents into an LLM pipeline and demonstrated improved misinformation detection compared to standalone LLMs \citep{tian2024webagents}. However, open-web retrieval introduces significant challenges:

\begin{itemize}
    \item \textbf{Source credibility}: not all websites are equally reliable.
    \item \textbf{Information quality}: web content varies in accuracy and completeness.
    \item \textbf{Ranking complexity}: systems must identify the most relevant sources among millions of candidates.
    \item \textbf{Dynamic nature}: content changes over time, affecting reproducibility.
\end{itemize}

Current best practices for web retrieval in fact-checking include prioritising sources with high domain authority (established news organisations, academic institutions, government agencies), cross-referencing multiple independent sources to corroborate claims, explicitly evaluating source credibility using metadata (publication date, author credentials, institutional affiliation), and maintaining transparency by exposing retrieved sources to users. Systems like FactAgent incorporate evidence retrieval as a dedicated step, using search tools to query the web and then filtering results based on relevance and credibility \citep{hysonlab2025factagent}.

\subsection{Query Optimization for Fact-Checking}

An often overlooked but critical component of RAG systems is query formulation. The same claim can be verified or refuted depending on how search queries are constructed. Research on question generation for information retrieval has shown that query quality significantly impacts downstream task performance \citep{ma2025guided, hassan2021claimbuster}.

Effective query optimisation for fact-checking involves several key strategies. \textbf{Keyword extraction} identifies the most salient terms in a claim that are likely to appear in relevant sources, filtering out stop words and focusing on entities and key concepts. \textbf{Query expansion} generates multiple query variants to capture different phrasings or perspectives on the claim—for example, expanding ``unemployment rate increased'' to also search for ``jobless claims rose'' or ``labour market deterioration'' \citep{lewis2020retrieval}. \textbf{Entity recognition} identifies named entities (people, organisations, locations) that should be included in queries, as these often serve as strong signals for retrieval systems. \textbf{Temporal awareness} incorporates time constraints when claims reference specific periods, such as adding the relevant year when verifying recent events.

Recent multi-agent systems often dedicate a specialised agent to query generation, recognising that this step significantly impacts the quality of retrieved evidence \citep{hysonlab2025factagent}. Poor queries may miss relevant sources or retrieve irrelevant information, degrading overall system performance regardless of verification model quality. FactAgent, for instance, includes explicit query formulation as one of its agent steps, using the LLM to generate search-optimised queries from claims \citep{hysonlab2025factagent}.

\section{LLM-Based Claim Verification Methods}

Once claims are identified and relevant evidence retrieved, systems must determine veracity—typically labelling claims as supported (true), refuted (false), or not enough evidence. Traditional approaches treated this as recognising textual entailment, using neural classifiers to determine if evidence entails or contradicts claims. With LLMs, a new paradigm has emerged: using models to perform verification through natural language reasoning.

\subsection{Prompting Strategies}

\textbf{Zero-shot and few-shot prompting} involves providing an LLM with a claim and retrieved evidence, asking it to decide veracity and explain why. For example: ``Claim: X. Evidence: [text]. Based on the evidence, is the claim true or false?'' \citep{zhang2023siren}. In zero-shot mode, the LLM relies on internal reasoning and evidence interpretation. In few-shot mode, the prompt includes examples of claims with evidence and the correct verdict to guide the model's style and criteria.

GPT-4 and similar models show surprising capability at this task, often correctly interpreting whether evidence supports statements. However, LLMs can be overly agreeable, sometimes hallucinating justifications or defaulting to ``Supported'' even when evidence is insufficient \citep{zhang2023siren}. This confirmation bias appears to stem from models' training to be helpful and provide answers, even when saying ``I don't know'' would be more appropriate.

Careful prompt engineering can mitigate these issues. Effective strategies include:

\begin{itemize}
    \item Explicitly instructing the model to answer ``Not Enough Evidence'' when information is insufficient.
    \item Adding system messages emphasising the importance of accuracy over helpfulness.
    \item Requesting that the model cite specific evidence sentences supporting its verdict.
    \item Using temperature settings near zero to reduce randomness and increase consistency.
    \item Implementing multi-pass verification where the model first generates a verdict and then critiques its own reasoning.
\end{itemize}

Despite these techniques, pure prompting still lags behind specialised models on complex datasets, particularly for subtle cases requiring deep domain knowledge or multi-hop reasoning.

\subsection{Chain-of-Thought Reasoning and Agent Loops}

More advanced approaches use \textbf{chain-of-thought reasoning} or implement the LLM as an agent in a loop. The \textbf{ReAct pattern} has the LLM explicitly reason step-by-step while using tools \citep{yao2023react}. For verification, this might involve: (1) breaking the claim into parts, (2) querying a search engine for each part, (3) evaluating each piece of evidence, and (4) synthesising a conclusion.

\textbf{FactAgent's} structured workflow exemplifies this: the LLM follows a script where each step (search, read results, extract evidence, cross-check, formulate verdict) is explicit and logged for transparency \citep{hysonlab2025factagent}. This orchestrated approach is more flexible than fixed pipelines—if initial evidence is inconclusive, the agent can trigger refined searches. A major benefit is zero-shot operation without training, mimicking human fact-checker processes \citep{hysonlab2025factagent}.

The chain-of-thought approach provides several advantages for verification:

\begin{itemize}
    \item \textbf{Transparency}: each reasoning step is explicit and inspectable.
    \item \textbf{Debuggability}: when errors occur, it is possible to identify which step failed.
    \item \textbf{Adaptability}: the system can adjust its strategy based on intermediate results.
    \item \textbf{Explainability}: the reasoning trace serves as a natural language explanation for the verdict.
\end{itemize}

However, the downside is efficiency: multiple LLM calls for each sub-step can be slow and expensive, and errors compound across stages. If the claim decomposition is incorrect, subsequent reasoning will be compromised regardless of retrieval and evaluation quality.

\subsection{Self-Consistency and Verification}

\textbf{SelfCheckGPT} introduced self-consistency checking for hallucination detection \citep{manakul2023selfcheckgpt}. The method generates multiple independent answers to the same query and checks if factual assertions agree across responses. If answers diverge on details, those details likely represent hallucinations or false facts \citep{manakul2023selfcheckgpt, wardle2017information}. While SelfCheckGPT does not use external evidence, the principle extends to verification: an LLM can be prompted to explicitly double-check its own claims.

The self-consistency approach operates on the principle that hallucinated information will vary across samples (since it is essentially random), while information grounded in the model's training data will be consistent. By generating multiple explanations for why a claim is true or false and checking for factual consistency across explanations, systems can identify low-confidence or potentially hallucinated components of their reasoning.

\textbf{LLM-as-a-judge} approaches use one model to generate answers and another (or the same model in a different mode) to verify them. For fact-checking, this could mean using GPT-4 to generate a verdict, then prompting it (or another model) to validate: ``Given the claim, evidence, and explanation, is the explanation correct and does it truly support the claim?'' This metacognitive step can catch errors before presenting results to users \citep{raschka2025llmeval, ruder2025llmeval}.

\textbf{Cross-model checking} uses different models or the same model with different knowledge to verify outputs. For example, a powerful GPT-4 might generate a verdict, then a smaller model fine-tuned on FEVER validates it against evidence. If they disagree, the system might abstain or ask GPT-4 to reconsider \citep{aly2021feverous}. This ensemble approach can improve reliability by detecting cases where one model's reasoning is flawed. Another relevant task is \textbf{stance detection}: classifying evidence snippets as supporting, refuting, or not mentioning the claim. Many verification pipelines have dedicated stance models, which can also be implemented via prompting \citep{thorne2018fever}.

\subsection{Current Capabilities and Limitations}

Evaluation results from various benchmarks suggest that carefully prompted LLMs (especially GPT-4-class models) can achieve near state-of-the-art performance on tasks like FEVER \citep{thorne2018fever}. However, they still make mistakes, especially on ambiguous or complex claims requiring specialised knowledge or multi-step reasoning.

A noted limitation is the tendency to default to ``Supported''—models sometimes erroneously agree claims are true if any related evidence is found (confirmation bias), rather than truly verifying the exact claim \citep{zhang2023siren}. For example, given the claim ``Paris has a population of 10 million'' and evidence stating ``Paris metropolitan area has 12 million residents,'' an LLM might incorrectly mark this as supported due to numerical proximity without recognising the distinction between city proper and metropolitan area.

Designing prompts or agent behaviours to be appropriately sceptical and output ``Not Enough Info'' when evidence is lacking remains important. This requires careful calibration—the system should neither be too credulous (accepting weak evidence) nor too sceptical (rejecting valid evidence due to minor inconsistencies).

\section{Evaluation of Fact-Checking Systems}

Evaluating automated fact-checking systems is multifaceted, requiring assessment of accuracy, explanation quality, evidence usage, and practical usability. This section examines evaluation methodologies from recent literature.

\subsection{Veracity Classification Metrics}

When framed as classification (true/false or support/refute), standard metrics include accuracy, F1-score, and precision/recall. The FEVER challenge introduced the \textbf{FEVER score}—accuracy of the claim label and provision of at least one correct supporting evidence \citep{thorne2018fever}. This metric penalises systems that get labels right without proper reasoning or evidence grounding. For multi-class truth scales (e.g. ``true'', ``mostly true'', ``half-true'', ``mostly false'', ``false'', ``pants on fire'' as used by PolitiFact), accuracy within each class or Cohen's kappa for ordinal scales can be used, with disagreement severity varying by class distance.

\subsection{Evidence Retrieval Metrics}

A critical aspect of evaluation is whether systems find appropriate evidence. \textbf{Recall@k} (e.g. Recall@5, Recall@10) measures whether correct evidence appears in the top $k$ retrieved documents or sentences \citep{thorne2018fever}. \textbf{Precision} measures what proportion of selected evidence is actually relevant, indicating how much noise accompanies signal. \textbf{Mean Average Precision (MAP)} provides a single metric that accounts for both the relevance of retrieved documents and their ranking order, rewarding systems that place relevant documents higher in result lists. End-to-end evaluations often credit systems only when they retrieve human-identified supporting or refuting evidence, though this can be restrictive because multiple valid sources may exist for a claim.

\subsection{Explanation Faithfulness and Quality}

When systems produce textual explanations or rationales, evaluation becomes challenging. Ideally, explanations should be \textbf{faithful} (reflect actual reasoning without introducing external information) and \textbf{factually consistent} with evidence. Automatic metrics like BLEU or ROUGE compare to reference explanations but do not measure factuality well \citep{raschka2025llmeval}. More sophisticated approaches include FactCC \citep{skywork2024hallucinations}, Q$^2$ (question-answering-based verification), and entailment checks that determine whether evidence and claim entail the explanation.

\textbf{LLM-as-a-judge} has become increasingly popular: using GPT-4 or similar models to score explanation coherence and factuality. For example, prompting GPT-4 with criteria such as factual accuracy, logical coherence, appropriate use of evidence, and absence of unsupported claims yields scores that correlate reasonably with human judgements when properly calibrated \citep{raschka2025llmeval, ruder2025llmeval}. Nevertheless, judge models may have biases, can be persuaded by confident but incorrect explanations, and may struggle with specialised domains, so validation against human assessments remains essential.

\subsection{Logical Coherence and Consistency}

\textbf{Stance consistency} checks whether evidence stances align with final verdicts. If a system claims a statement is true but all evidence is marked ``Refutes'', that indicates either retrieval problems or reasoning errors. LoCal explicitly evaluates logical consistency by checking if composed solutions imply claim veracity \citep{chen2024local}. Secondary models can perform entailment checks to verify each claim against cited sources, creating a verification layer where the system's reasoning is itself subject to verification.

\subsection{Human Evaluation}

Human judgement remains the gold standard for systems meant for real-world use. Researchers conduct user studies or expert assessments on output samples. Fact-checking experts may rate verdict correctness and reasoning soundness, while lay users can evaluate whether explanations are convincing and understandable. Human evaluation can also assess readability, perceived trust, actionability, and completeness—whether the system addressed all relevant aspects of the claim \citep{ma2025guided, wardle2017information}.

\subsection{Computational Performance Metrics}

For practical deployment, computational efficiency matters. Relevant metrics include latency (time from claim input to verdict output), throughput (number of claims processed per unit time), monetary cost (API or infrastructure expenditure), and resource utilisation (memory, CPU, GPU). Although rarely reported in academic papers, these metrics are critical for operational systems, particularly those targeting near-real-time analysis of streaming video content \citep{hysonlab2025factagent, tian2024webagents}.

\section{Current Limitations and Research Opportunities}

Despite rapid progress, contemporary automated fact-checking systems have significant limitations that constrain practical deployment and real-world impact. This section examines these gaps critically and identifies specific research opportunities they present.

\subsection{Lack of End-to-End Usability}

\textbf{Limitation}: most research prototypes focus on isolated problem slices rather than providing seamless end-to-end tools. Some systems excel at claim detection but assume verification is done manually \citep{hassan2021claimbuster}. Others verify given claims but require humans to identify them first. This fragmentation means few truly autonomous fact-checkers exist that users can feed raw content (such as videos) and receive comprehensive verified analyses.

Even ClaimBuster, while dubbed ``end-to-end'', essentially provided claim highlighting and left verification to humans \citep{hassan2021claimbuster}. \textbf{Fact-Audit} points out that complete systems need to integrate detection, verification with reasoning, and source tracing, but existing systems typically address only one or two of these steps \citep{lin2025factaudit}. For YouTube videos specifically, true end-to-end systems should handle transcription extraction, claim detection from long transcripts, evidence retrieval from diverse sources, verification with reasoned explanations, and presentation of results in user-friendly formats—a level of integration rarely achieved in current research \citep{lin2025factaudit}.

\textbf{Research opportunity}: develop complete pipelines that integrate all stages from raw video input to verified claims with user-friendly presentation. This requires not just technical integration but careful attention to error propagation across stages and system-level optimisation.

\subsection{Dependence on Structured or Closed Sources}

\textbf{Limitation}: a large portion of fact-checking research restricts evidence to structured, highly reliable knowledge bases—primarily Wikipedia. While this yields cleaner evaluation and reduces web noise, it severely limits applicability \citep{aly2021feverous}. Real misinformation often involves domains or topics where Wikipedia lacks coverage. Economic claims might require World Bank reports, medical claims need CDC guidelines, and breaking news requires recent articles—none available in static snapshots.

Systems benchmarked on FEVER or similar datasets tend to be over-fitted to Wikipedia as the source of truth, using wiki-specific retrieval heuristics or assuming single correct pages exist for claims \citep{thorne2018fever}. In practice, fact-checkers must handle a heterogeneous open web including news sites, blogs, scientific papers, government databases, and multimedia content. This introduces challenges of source credibility, data variety, and information quality variations. Additionally, many systems do not handle multilingual or non-English content well, limiting global accessibility \citep{aly2021feverous}.

\textbf{Research opportunity}: develop robust open-web retrieval methods with explicit source credibility evaluation, multi-source corroboration strategies, and techniques for synthesising information from conflicting sources. This includes handling diverse evidence formats and supporting multilingual fact-checking.

\subsection{Limited Accessibility for Non-Expert Users}

\textbf{Limitation}: most current solutions are research demonstrations or internal tools, not polished products for public use. Academic papers propose models and report accuracy metrics, but code is often research-grade (notebooks, command-line scripts) that average users cannot operate. The general public, who could benefit most from automated fact-checking, seldom interacts with these systems directly \citep{lin2025factaudit}.

One exception is Google Fact Check Explorer, which is public but only searches existing fact-check articles rather than performing new verification \citep{google2024factcheck}. User experience is often lacking—systems output labels and confidence scores that non-experts find unactionable or unconvincing. Computational accessibility is another concern: many advanced systems require heavy compute not feasible without powerful hardware or costly API access \citep{lin2025factaudit}.

\textbf{Research opportunity}: bridge the gap between research prototypes and usable products through intuitive interfaces, real-time feedback mechanisms, and cost-efficient architectures. This includes exploring cost-quality trade-offs to make systems economically viable for public deployment.

\subsection{Scalability and Real-Time Constraints}

\textbf{Limitation}: checking long videos with many claims stresses any system. If verification takes minutes per claim, a video with 20 claims becomes impractical for interactive use. Current research often does not address runtime performance, evaluating models on single queries or small datasets without considering throughput or latency \citep{lin2025factaudit}. Multi-agent systems can theoretically operate in parallel on different claims, but sequential dependencies (evidence retrieval preceding verification) limit parallelisation opportunities. Cost is also a scalability factor: using commercial LLMs like GPT-4 for every step can be prohibitively expensive at scale.

\textbf{Research opportunity}: perform systematic analysis of computational efficiency and cost–quality trade-offs in multi-agent systems. Questions such as which steps benefit most from powerful models, whether claim difficulty can guide routing, and how to parallelise agent operations effectively remain largely unexplored.

\subsection{Trust and Transparency Issues}

\textbf{Limitation}: users and fact-checkers may be reluctant to trust AI verdicts without understanding how they were derived. Many deep learning systems have been criticised as ``black boxes'' \citep{wardle2017information}. Multi-agent systems and chain-of-thought approaches attempt to address this via explicit reasoning traces, but if explanations are generated by the same model making judgements, there is risk of post-hoc rationalisation or hallucinated justifications.

Maintaining clear separation between evidence and reasoning—ensuring systems quote actual sources rather than fabricating them—is critical yet challenging. LLMs have a documented tendency to confidently present false information, which in fact-checking contexts could undermine the system's purpose.

\textbf{Research opportunity}: develop verification mechanisms for system outputs themselves, such as secondary validation of cited sources, consistency checks between evidence and explanations, and methods to detect when LLMs hallucinate rather than reason from evidence. Communicating uncertainty and limitations transparently to users is equally important.

\subsection{Lack of Comparative LLM Evaluation}

\textbf{Limitation}: despite the proliferation of LLM options (commercial models such as GPT-4 or Claude; open-source models like LLaMA, Qwen, DeepSeek), there is limited systematic comparison of their suitability for different fact-checking tasks. Research tends to use whichever model is most accessible without rigorous comparison of trade-offs in cost, accuracy, and latency across the full pipeline \citep{raschka2025llmeval}.

\textbf{Research opportunity}: conduct comprehensive comparative evaluations of different LLMs across fact-checking pipeline components, measuring not just accuracy but also cost, latency, consistency, and propensity toward hallucination. This would inform practical decisions about which models to use for which tasks.

\section{Positioning of This Work}

This thesis addresses the critical gaps identified above by developing a complete multi-agent system for YouTube video fact-checking. The work makes the following contributions to the field.

\subsection{End-to-End Video Fact-Checking System}

We implement a complete pipeline integrating all stages—from transcription extraction to claim detection, query generation, evidence retrieval, and verdict synthesis—that processes raw YouTube URLs autonomously. Unlike research prototypes that assume pre-processed inputs, this system handles the full workflow from video to verified claims, addressing the end-to-end usability gap \citep{lin2025factaudit}.

\subsection{Open-Web Evidence Retrieval}

Moving beyond Wikipedia and closed knowledge bases, the system retrieves evidence from the live web using search engines, incorporates source credibility evaluation, and handles multi-source corroboration. This addresses the limitation of systems constrained to structured sources and enables verification of claims about recent events, specialised domains, and topics not well covered in encyclopaedic sources \citep{aly2021feverous, thorne2018fever}.

\subsection{Practical User Interface}

We develop a web-based interface with real-time streaming of intermediate results using Server-Sent Events, making the verification process transparent and accessible to non-expert users. This bridges the gap between research prototypes and usable products \citep{lin2025factaudit}, demonstrating that academic advances can be packaged for practical use.

\subsection{Systematic LLM Comparison}

The thesis conducts comparative evaluation of different LLM configurations (commercial versus open-source models, different model sizes), analysing trade-offs in cost, latency, and quality across pipeline components. This fills a gap in the literature where such trade-offs are rarely studied explicitly, providing practical guidance for deployment decisions \citep{raschka2025llmeval}.

\subsection{Modular Multi-Agent Architecture}

We implement five specialised agents with structured data schemas (using Pydantic) that enable transparency, maintainability, and future extensibility. Each agent can be evaluated and optimised independently, and the modular design facilitates experimentation with different models and techniques. This demonstrates the practical benefits of multi-agent approaches in a production-oriented context \citep{hysonlab2025factagent, chen2024local, ma2025guided}.

\subsection{Comprehensive Evaluation Framework}

The evaluation combines quantitative metrics (technical performance, LLM-specific metrics such as faithfulness and consistency), LLM-as-a-judge evaluation, and qualitative case studies across different video topics. This multifaceted approach addresses the evaluation challenges discussed earlier and provides a realistic assessment of system capabilities and limitations \citep{raschka2025llmeval, ruder2025llmeval}.

By combining state-of-the-art techniques from multi-agent systems, RAG, and LLM-based verification with explicit focus on practical deployment, this work advances automated fact-checking from research prototype toward usable tool. The system is designed not to replace human fact-checkers but to empower both professionals and lay users by automating information gathering and initial verification, allowing human judgement to focus on complex cases requiring expertise, contextual understanding, and ethical consideration.

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
