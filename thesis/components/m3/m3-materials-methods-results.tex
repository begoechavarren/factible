\documentclass[12pt,a4paper]{article}

% Essential packages
\usepackage{graphicx}
\usepackage{setspace}
\usepackage[numbers,sort&compress]{natbib}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{color}
\usepackage{hyperref}
\usepackage{amsmath,amssymb}
\usepackage{geometry}
\usepackage{longtable}
\usepackage{caption}
\usepackage{tikz}
\usepackage{adjustbox}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{float}
\usepackage{subcaption}
\usetikzlibrary{arrows.meta, positioning, shapes.geometric, calc, fit, backgrounds}

% Code listing configuration
\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{gray!10},
    keywordstyle=\color{blue},
    commentstyle=\color{green!50!black},
    stringstyle=\color{red!60!black},
    numbers=left,
    numberstyle=\tiny\color{gray},
    numbersep=5pt,
    showstringspaces=false,
    tabsize=2
}

\lstdefinelanguage{Python}{
    keywords={def, class, return, if, else, elif, for, while, with, as, import, from, async, await, try, except, finally, raise, True, False, None, and, or, not, in, is, lambda, yield},
    keywordstyle=\color{blue}\bfseries,
    ndkeywords={self, cls, BaseModel, Field, Literal, Optional, Union, list, dict, str, int, float, bool},
    ndkeywordstyle=\color{purple},
    sensitive=true,
    comment=[l]{\#},
    morestring=[b]",
    morestring=[b]',
}

% Margin settings
\geometry{
    a4paper,
    left=3cm,
    right=2.5cm,
    top=2.5cm,
    bottom=2.5cm
}

% Hyperref configuration
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=blue,
    pdftitle={Materials and Methods - Multi-Agent System for Fact-Checking},
    pdfauthor={Begoña Echavarren Sánchez}
}

% Spacing
\onehalfspacing

\begin{document}

\begin{titlepage}
    \begin{center}
    \vspace*{0.5cm}

    \includegraphics[width=0.6\textwidth]{../../assets/uoc_logo.png}

    \vspace{1.5cm}

    {\Huge \textbf{Materials, Methods and Results}}

    \vspace{2cm}

    {\LARGE \textbf{Multi-Agent System for Automated Fact-Checking of YouTube Videos}}

    \vspace{2.5cm}

    \textbf{\large Begoña Echavarren Sánchez}

    \vspace{0.5cm}

    \textit{Tutor: Josep-Anton Mir Tutusaus}

    \vspace{2cm}

    {\large Master's Degree in Data Science}

    {\large Universitat Oberta de Catalunya}

    \vspace{1cm}

    {\large PEC 3 - Implementation}

    \vspace{0.5cm}

    {\large December 2025}

    \end{center}
\end{titlepage}

\newpage
\pagenumbering{arabic}

\hypersetup{linkcolor=black}
\tableofcontents
\hypersetup{linkcolor=blue}

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SECTION 1: MATERIALS AND METHODS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Materials and Methods}

This section presents the comprehensive technical implementation of Factible, a multi-agent system for automated fact-checking of YouTube videos. The system implements an end-to-end pipeline that processes video content through five specialized components, leveraging large language models (LLMs) for reasoning tasks while employing classical algorithms for deterministic operations. The implementation follows design-science principles \citep{hevner2004design, oates2006researching}, emphasizing the creation of artifacts that extend human capabilities through systematic evaluation and iterative refinement.

%-------------------------------------------------------
\subsection{System Architecture Overview}
%-------------------------------------------------------

\subsubsection{High-Level Architecture}

Factible implements an end-to-end automated fact-checking pipeline for YouTube videos using a multi-agent architecture. Recent research on LLM agents demonstrates that multi-agent collaboration can enhance factuality and reasoning by allowing specialized agents to converse and coordinate on tasks \citep{wu2023autogen}. FactAgent further shows that decomposing fact-checking into dedicated agents for input ingestion, query generation, evidence retrieval, and verdict prediction yields higher accuracy and transparency \citep{zhang2025factagent}. The Factible architecture follows this line of work by processing video content through five specialized, modular components that operate sequentially with three levels of internal parallelization.

The system processes a YouTube video URL through five sequential stages, each with specialized responsibilities. The pipeline begins with transcript extraction, proceeds through claim and query generation, conducts online evidence retrieval, and culminates in structured verdict synthesis. This modular design enables independent optimization of each component while maintaining clear data contracts between stages.

The five stages are:

\begin{enumerate}
    \item \textbf{Transcriptor}: Extracts video transcripts via YouTube Transcript API, preserving timestamped segments for claim localization. The component includes automatic fallback to proxy service when rate-limited, ensuring robust transcript retrieval across different access conditions.

    \item \textbf{Claim Extractor} (LLM Agent): Employs thesis-first reasoning to infer the video's central argument before extracting factual and verifiable claims. Each claim receives an importance score based on its impact on the video's thesis. Post-processing uses fuzzy string matching to locate claims within the transcript for timestamp mapping.

    \item \textbf{Query Generator} (LLM Agent): Generates diverse search queries across four strategic types---direct, alternative, source-seeking, and contextual. Each query receives a priority score (1--5) based on evidence likelihood, enabling budget-conscious filtering of low-priority queries.

    \item \textbf{Online Search}: Executes a four-step evidence retrieval pipeline for each query: (i) Google Search via Serper API, (ii) website reliability assessment using Media Bias/Fact Check (MBFC) data combined with domain heuristics \citep{mbfc2024methodology}, (iii) content fetching via Selenium WebDriver with JavaScript rendering support, and (iv) LLM-based evidence extraction with stance classification (supports, refutes, mixed, unclear).

    \item \textbf{Output Generator} (LLM Agent): Synthesizes evidence into structured verdicts by building evidence bundles grouped by stance, generating natural language summaries with confidence levels, calculating algorithmic evidence quality scores, and mapping claims to video timestamps for interactive navigation.
\end{enumerate}

Figure \ref{fig:pipeline-architecture} illustrates the complete pipeline architecture with data flow and parallelization points across all five stages.

\begin{figure}[H]
\centering
\begin{tikzpicture}[
    node distance=0.7cm,
    box/.style={rectangle, draw=blue!60, fill=blue!5, thick, minimum width=12cm, minimum height=1.2cm, align=center, font=\small},
    arrow/.style={-{Stealth[length=3mm]}, thick},
    parallel/.style={draw=orange!70, dashed, thick}
]

% Stage 1
\node[box] (s1) {
    \textbf{Stage 1: Transcriptor}\\
    YouTube Transcript API $\rightarrow$ TranscriptData (text + segments + video\_id)
};

% Stage 2
\node[box, below=of s1] (s2) {
    \textbf{Stage 2: Claim Extractor (LLM)}\\
    Thesis-first reasoning $\rightarrow$ ExtractedClaims (sorted by importance)
};

% Stage 3
\node[box, below=of s2] (s3) {
    \textbf{Stage 3: Query Generator (LLM)}\\
    Generate diverse queries $\rightarrow$ GeneratedQueries (filtered by priority)
};

% Stage 4
\node[box, below=of s3] (s4) {
    \textbf{Stage 4: Online Search}\\
    \textit{Google Search $\rightarrow$ Reliability $\rightarrow$ Fetch $\rightarrow$ Extract Evidence}
};

% Stage 5
\node[box, below=of s4] (s5) {
    \textbf{Stage 5: Output Generator (LLM)}\\
    Evidence synthesis $\rightarrow$ FactCheckRunOutput (reports sorted by quality)
};

% Arrows
\draw[arrow] (s1) -- (s2);
\draw[arrow] (s2) -- node[right, font=\scriptsize, orange] {Level 1: N claims $\parallel$} (s3);
\draw[arrow] (s3) -- node[right, font=\scriptsize, orange] {Level 2: M queries/claim $\parallel$} (s4);
\draw[arrow] (s4) -- node[right, font=\scriptsize, orange] {Level 3: K results/query $\parallel$} (s5);

\end{tikzpicture}
\caption{High-level pipeline architecture showing the five main stages and three parallelization levels. Parallelization occurs at claims (Level 1), queries per claim (Level 2), and search results per query (Level 3). Verdict generation happens within Level 1 after each claim's evidence is collected.}
\label{fig:pipeline-architecture}
\end{figure}

\subsubsection{Design Principles}

The system adheres to several key design principles derived from software engineering best practices and GenAI application development, aligned with the design-science paradigm in information systems research \citep{hevner2004design}:

\begin{enumerate}
    \item \textbf{Modularity}: Each component is isolated with well-defined inputs and outputs using Pydantic schemas, enabling independent optimization, testing, and replacement.

    \item \textbf{Structured Outputs}: All LLM interactions use Pydantic AI with typed output schemas, ensuring type safety, automatic validation, and consistent data structures across the pipeline.

    \item \textbf{Transparency}: The full evidence chain is preserved and exposed to users---sources, reliability ratings, stances, and reasoning are all traceable from final verdict back to original source.

    \item \textbf{Progressive Enhancement}: The pipeline operates with graceful degradation (e.g., fallback to snippet if scraping fails, fallback to proxy if rate-limited) rather than failing entirely.

    \item \textbf{Cost-Conscious Design}: Configurable limits (\texttt{max\_claims}, \texttt{max\_queries}, \texttt{max\_results}) prevent runaway API costs during development and production.

    \item \textbf{Reproducibility}: Deterministic LLM outputs (\texttt{temperature=0.0}), structured YAML configurations, and comprehensive experiment tracking enable reproducible research.

    \item \textbf{Separation of Concerns}: Classical algorithms handle tasks like reliability scoring, deduplication, and quality calculation, reserving LLM calls for tasks requiring reasoning and language understanding.
\end{enumerate}

\subsubsection{Component Isolation Pattern}

Each component follows a consistent directory structure that promotes modularity and maintainability. This standardized organization enables independent testing of each component in isolation, seamless swapping of LLM models for experimentation, automated metrics collection via decorators, and clear interface contracts through Pydantic schemas. The typical structure includes a public exports file (\texttt{\_\_init\_\_.py}), main logic file with tracking decorators (\texttt{component\_name.py}), and schema definitions file (\texttt{schemas.py}) containing all input and output Pydantic models. This separation of concerns facilitates parallel development and reduces coupling between pipeline stages.

%-------------------------------------------------------
\subsection{Technology Stack}
%-------------------------------------------------------

\subsubsection{Core Technologies}

Table \ref{tab:tech-stack} presents the core technologies employed in the implementation.

\begin{table}[H]
\centering
\caption{Core technology stack}
\label{tab:tech-stack}
\begin{tabular}{llll}
\toprule
\textbf{Category} & \textbf{Technology} & \textbf{Version} & \textbf{Purpose} \\
\midrule
Language & Python & 3.12 & Core implementation with type hints \\
LLM Framework & Pydantic AI & $\geq$1.0.0 & Agent orchestration, structured outputs \\
Data Validation & Pydantic & $\geq$2.0.0 & Schema definitions, runtime validation \\
Web Framework & FastAPI & $\geq$0.115.0 & REST API with SSE streaming \\
HTTP Server & Uvicorn & $\geq$0.32.0 & High-performance ASGI server \\
Async HTTP & httpx & $\geq$0.28.1 & Async HTTP client \\
Web Scraping & Selenium & $\geq$4.15.2 & JavaScript-rendered content extraction \\
YouTube & youtube-transcript-api & $\geq$1.2.2 & Transcript extraction \\
Domain Info & python-whois & $\geq$0.8.0 & Domain age lookup \\
CLI & Typer & $\geq$0.15.0 & Experiment runner CLI \\
Analysis & pandas, matplotlib & - & Data analysis and visualization \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Large Language Models}

The system supports multiple LLM providers to enable comparison of cost-quality trade-offs. Table \ref{tab:llm-models} shows the available models and their configurations.

\begin{table}[H]
\centering
\caption{LLM providers and pricing}
\label{tab:llm-models}
\begin{tabular}{lllll}
\toprule
\textbf{Provider} & \textbf{Model} & \textbf{Context} & \textbf{Pricing (per 1M tokens)} & \textbf{Use Case} \\
\midrule
OpenAI & gpt-4o-mini & 128K & \$0.15 / \$0.60 & Default \\
OpenAI & gpt-4o & 128K & \$5.00 / \$15.00 & High-quality \\
OpenAI & gpt-4-turbo & 128K & \$10.00 / \$30.00 & Premium \\
Ollama & qwen3:8b & 40K & Free (local) & Budget/offline \\
Ollama & qwen3:4b & 256K & Free (local) & Small footprint \\
\bottomrule
\end{tabular}
\end{table}

Large language models such as GPT-4 offer multimodal capabilities and demonstrate human-level performance across diverse benchmarks \citep{openai2024gpt4}. Despite these advances, models still suffer from hallucinations and are constrained by limited context windows, underscoring the need for careful configuration and reliability safeguards \citep{openai2024gpt4}. Our model management layer therefore emphasizes deterministic outputs, context-aware trimming, and tool-assisted generation.

\subsubsection{External Services}

The system integrates with external services for search and transcript extraction:

\begin{itemize}
    \item \textbf{Serper API}: Google Search wrapper providing organic search results with approximately 2,500 queries per month on the free tier.
    \item \textbf{YouTube oEmbed API}: Video metadata retrieval without authentication.
    \item \textbf{Webshare Proxy}: Rate limit bypass for transcript extraction with configurable proxy locations.
\end{itemize}

%-------------------------------------------------------
\subsection{Pipeline Components}
%-------------------------------------------------------

\subsubsection{Transcriptor}

The Transcriptor component extracts YouTube video transcripts with precise timestamp information for later claim-to-video mapping.

\paragraph{Implementation Details}

The transcriptor uses the \texttt{youtube-transcript-api} library to fetch available transcripts, with preference for English (\texttt{["en", "en-US"]}). When rate-limited by YouTube, it automatically falls back to a proxy service (Webshare). Key features include:

\begin{itemize}
    \item \textbf{Timestamped Segments}: Each segment preserves \texttt{start} time and \texttt{duration} in seconds.
    \item \textbf{Character Position Mapping}: Enables mapping claim text positions back to video timestamps.
    \item \textbf{Title Fetching}: Uses YouTube oEmbed API to retrieve video title for context.
    \item \textbf{Proxy Fallback}: Automatic retry through Webshare proxy when rate-limited.
\end{itemize}

\paragraph{Output Schema}

\begin{lstlisting}[language=Python, caption={Transcriptor output schemas}]
class TranscriptSegment(BaseModel):
    """A single timestamped segment from a YouTube transcript."""
    text: str = Field(description="The text content of this segment")
    start: float = Field(ge=0.0, description="Start time in seconds")
    duration: float = Field(ge=0.0, description="Duration in seconds")

class TranscriptData(BaseModel):
    """Complete transcript data with timestamped segments."""
    text: str = Field(description="Full transcript as plain text")
    segments: list[TranscriptSegment]
    video_id: str = Field(description="YouTube video ID")
\end{lstlisting}

\paragraph{Timestamp Mapping Algorithm}

The timestamp mapping function enables the system to locate where in the video each claim originates. The algorithm iterates through transcript segments sequentially, maintaining a cumulative character counter. When a character position falls within a segment's range, the function returns the corresponding video timestamp (start time and duration). This mapping is crucial for the user interface, allowing users to jump directly to the video moment where a specific claim was made.

%---

\subsubsection{Claim Extractor}

The Claim Extractor identifies factual, verifiable claims from video transcripts using LLM-based extraction with thesis-relative importance ranking. Our approach builds on prior work in automated claim detection: supervised models trained on annotated political debates have been used to detect check-worthy claims \citep{hassan2015claimbuster}, and end-to-end systems like ClaimBuster monitor public discourse and prioritize factual statements for manual fact-checking \citep{li2017claimbuster}. These systems show that focusing on salient, verifiable claims improves the efficiency of fact-checking pipelines.

\paragraph{LLM Configuration}

The claim extractor uses deterministic settings for reproducibility: temperature set to 0.0 to ensure consistent outputs across multiple runs, max tokens limited to 1,200 to control response length and latency, and automatic retry logic (3 attempts) to handle transient LLM failures gracefully.

\paragraph{Prompt Engineering Strategy: Thesis-First Approach}

The claim extractor employs a novel \textit{thesis-first approach} with multi-step reasoning designed to prioritize claims most critical to the video's central argument:

\textbf{Step 1: Thesis Inference} --- Before listing claims, the LLM infers the video's central thesis in no more than 25 words (e.g., ``Climate change alarmism is driven more by politics and media than by settled science'').

\textbf{Step 2: Importance Ranking with Thesis Impact Test} --- Claims are scored based on their impact on the video's thesis using the question: ``If this claim were proven false, would the thesis collapse or materially weaken?'' Table \ref{tab:importance-scoring} presents the scoring guidelines.

\begin{table}[H]
\centering
\caption{Claim importance scoring guidelines}
\label{tab:importance-scoring}
\begin{tabular}{lll}
\toprule
\textbf{Score Range} & \textbf{Description} & \textbf{Examples} \\
\midrule
0.85--1.0 & Prescriptive/causal claims undermining thesis & Policy proposals, causal mechanisms \\
0.60--0.80 & Quantitative/historical evidence tied to thesis & Statistics, dates, expert citations \\
0.30--0.55 & Context/supporting background & Definitions, general facts \\
0.0--0.25 & Peripheral/anecdotal details & Personal stories, credentials \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Step 3: Relevance Guardrails} --- Pure credential facts are capped at 0.30 unless the thesis questions expertise; statements not affecting the thesis are capped at 0.25; pure opinions are excluded; and paraphrases and duplicate numbers are removed.

\paragraph{Dynamic Instructions via Pydantic AI Dependencies}

The agent uses Pydantic AI's dependency injection mechanism to inject runtime constraints into the system prompt. This design pattern enables dynamic instruction generation based on runtime parameters: when a \texttt{max\_claims} limit is specified, the instruction explicitly constrains the output size; otherwise, it defaults to requesting only the highest-impact claims. This approach provides flexibility for experimentation while maintaining type safety through Pydantic validation.

\paragraph{Post-Processing: Fuzzy Claim Localization}

After LLM extraction, each claim is located in the original transcript using fuzzy string matching. The algorithm normalizes text, applies a sliding window with $\pm 2$ words around the claim length, computes similarity using \texttt{difflib.SequenceMatcher}, and requires a minimum score of 0.5 for a match. This produces \texttt{transcript\_char\_start}, \texttt{transcript\_char\_end}, and \texttt{transcript\_match\_score} fields for timestamp mapping.

\paragraph{Output Schema}

\begin{lstlisting}[language=Python, caption={Claim extractor output schemas}]
class Claim(BaseModel):
    """A single claim or fact extracted from text."""
    text: str  # Claim text (<=40 words recommended)
    confidence: float = Field(ge=0.0, le=1.0)
    category: str  # historical, scientific, statistical, etc.
    importance: float = Field(default=0.5, ge=0.0, le=1.0)
    context: str | None = Field(default=None)
    # Post-processing fields
    transcript_char_start: int | None = None
    transcript_char_end: int | None = None
    transcript_match_score: float | None = None

class ExtractedClaims(BaseModel):
    """Collection of claims extracted from a transcript."""
    claims: list[Claim]  # Sorted by importance (descending)
    total_count: int
\end{lstlisting}

Standard fact-checking datasets such as FEVER \citep{thorne2018fever} and FEVEROUS \citep{aly2021feverous} are used as external references to evaluate claim extraction performance. FEVER contains 185,445 claims derived from Wikipedia sentences labeled as Supported, Refuted, or NotEnoughInfo, while FEVEROUS extends this to 87,026 claims with both unstructured text and structured table evidence.

%---

\subsubsection{Query Generator}

The Query Generator produces diverse, prioritized search queries optimized for evidence retrieval. Research on LLMs shows that interleaving reasoning and acting enables models to plan and perform external searches more effectively; the ReAct prompting technique encourages models to produce intermediate reasoning steps and task-specific actions, leading to improved factuality and reduced hallucination \citep{yao2023react}. Retrieval-augmented generation methods combine parametric language models with non-parametric memory to retrieve relevant documents \citep{lewis2020retrieval}, while frameworks like RARR perform post-generation research and revision to align outputs with supporting evidence \citep{gao2022rarr}.

\paragraph{Query Type Taxonomy}

The system generates four types of queries with different search strategies, as shown in Table \ref{tab:query-types}.

\begin{table}[H]
\centering
\caption{Query type taxonomy}
\label{tab:query-types}
\begin{tabular}{llll}
\toprule
\textbf{Type} & \textbf{Description} & \textbf{Strategy} & \textbf{Example} \\
\midrule
DIRECT & Exact claim phrasing & Verbatim search & ``unemployment rose 15\% Q3 2024'' \\
ALTERNATIVE & Rephrased with synonyms & Semantic variation & ``jobless rate increase third quarter'' \\
SOURCE & Target authoritative sources & Source-seeking & ``BLS unemployment statistics Q3'' \\
CONTEXT & Broader context & Background search & ``economic indicators fall 2024'' \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Priority System}

Queries are prioritized 1--5 based on likelihood of finding reliable, definitive information: priority 1 queries are always included, priority 2 by default, priority 3 if budget allows, and priorities 4--5 rarely or only for completeness.

\paragraph{Output Schema}

\begin{lstlisting}[language=Python, caption={Query generator output schemas}]
class SearchQuery(BaseModel):
    query: str           # Search query text
    query_type: str      # "direct", "alternative", "source", "context"
    priority: int        # 1-5 (1 = highest priority)

class GeneratedQueries(BaseModel):
    original_claim: str           # Reference to source claim
    queries: list[SearchQuery]    # Filtered and sorted queries
    total_count: int              # Original count before filtering
\end{lstlisting}

%---

\subsubsection{Online Search}

The Online Search component implements a multi-step pipeline to retrieve, assess, and extract evidence from web sources with adaptive quality filtering. Unlike the other LLM-based components, Online Search orchestrates multiple classical algorithms alongside a single LLM call for evidence extraction. This hybrid approach balances speed, reliability, and reasoning capabilities.

The reliability assessment combines domain-level heuristics with the Media Bias/Fact Check (MBFC) methodology, which employs a comprehensive weighted scoring system to evaluate media outlets' ideological bias and factual reliability \citep{mbfc2024methodology}. To mitigate hallucinations and ensure evidence quality, we draw on research like SelfCheckGPT, which detects hallucinations by comparing multiple sampled responses and ranks passages by factuality \citep{manakul2023selfcheckgpt}. Our multi-agent retrieval strategy is further informed by systems such as FactAgent \citep{zhang2025factagent} and LoCal \citep{chen2024local}, where decomposing, reasoning, and evaluating agents iteratively refine answers and outperform baselines.

Figure \ref{fig:online-search-pipeline} illustrates the four-step Online Search pipeline executed for each query.

\begin{figure}[H]
\centering
\begin{tikzpicture}[
    node distance=1.2cm,
    box/.style={rectangle, draw=green!60, fill=green!5, thick, minimum width=10cm, minimum height=1cm, align=center, font=\small},
    arrow/.style={-{Stealth[length=3mm]}, thick},
]

% Step 1
\node[box] (step1) {
    \textbf{Step 1: Google Search}\\
    Serper API $\rightarrow$ GoogleSearchHits (title, URL, snippet)
};

% Step 2
\node[box, below=of step1] (step2) {
    \textbf{Step 2: Reliability Assessment}\\
    MBFC + Domain Heuristics $\rightarrow$ SiteReliability (rating, score, bias)
};

% Step 3
\node[box, below=of step2] (step3) {
    \textbf{Step 3: Content Fetching}\\
    Selenium WebDriver $\rightarrow$ PageText (with JS rendering)
};

% Step 4
\node[box, below=of step3] (step4) {
    \textbf{Step 4: Evidence Extraction (LLM)}\\
    Analyze content $\rightarrow$ EvidenceSummary + Stance (supports/refutes/mixed/unclear)
};

% Arrows
\draw[arrow] (step1) -- (step2);
\draw[arrow] (step2) -- (step3);
\draw[arrow] (step3) -- (step4);

% Parallelization note
\node[below=0.3cm of step4, font=\scriptsize, orange, align=center] {
    \textit{All K results per query execute Steps 2-4 in parallel (Level 3)}
};

\end{tikzpicture}
\caption{Online Search pipeline showing the four sequential steps executed for each search result. Steps 2--4 run in parallel across all K results per query (Level 3 parallelization).}
\label{fig:online-search-pipeline}
\end{figure}

\paragraph{Step 1: Google Search (Serper API)}

The Google search client wraps the Serper API for asynchronous search execution. The implementation uses persistent HTTP connections for performance and returns structured search hits containing title, URL, and snippet fields. Each query can retrieve up to 10 results, with the limit parameter controlling the exact number returned. The async design enables parallel query execution across multiple claims simultaneously.

\paragraph{Step 2: Website Reliability Assessment}

The reliability checker uses a multi-factor scoring system with first-match priority, combining external datasets with algorithmic heuristics:

\begin{itemize}
    \item \textbf{Media Bias/Fact Check (MBFC) Dataset}: The system loads timestamped JSON snapshots containing over 10,000 news sources with credibility ratings. Credibility mappings are: high $\rightarrow$ 0.85, medium $\rightarrow$ 0.60, low $\rightarrow$ 0.30, very low $\rightarrow$ 0.15.

    \item \textbf{TLD Reputation}: High-trust top-level domains (\texttt{.gov}, \texttt{.edu}, \texttt{.int}) receive a base score of 0.90, reflecting their institutional authority.

    \item \textbf{Domain Age via WHOIS}: Domains $\geq$10 years old receive a +0.10 bonus (established presence), while domains $<$1 year old receive a --0.15 penalty (recent creation may indicate lower trust).
\end{itemize}

The output includes a categorical rating (high, medium, low, unknown), numerical score (0.0--1.0), reasoning for the assessment, and political bias classification when available from MBFC data.

\paragraph{Step 3: Content Fetching (Selenium)}

Content fetching uses Selenium WebDriver in headless Chrome mode with smart wait strategies to handle JavaScript-heavy sites. The implementation employs a two-phase extraction strategy: first attempting quick extraction of paragraph elements, then waiting for JavaScript rendering if initial content is insufficient ($<$100 characters). This adaptive approach balances speed for static sites with completeness for dynamic content.

Configuration optimizations include disabling images to reduce load time, setting page load timeouts (20 seconds), and limiting wait times for dynamic content (12 seconds). Content is trimmed to a maximum of 8,000 characters to control LLM input costs while preserving sufficient context for evidence extraction. Async integration wraps blocking Selenium calls in \texttt{asyncio.to\_thread()} for non-blocking operation, enabling parallel processing of multiple search results.

\paragraph{Step 4: Evidence Extraction (LLM)}

The evidence extractor analyzes retrieved content against claims using structured stance definitions:

\begin{itemize}
    \item \textbf{SUPPORTS}: Evidence confirms or validates the claim through direct statements, semantic equivalents, or mechanism descriptions.
    \item \textbf{REFUTES}: Evidence contradicts or disproves the claim through counter-evidence or statements that evidence is unproven/disproven.
    \item \textbf{MIXED}: Both supporting and refuting elements present.
    \item \textbf{UNCLEAR}: Genuinely ambiguous content that discusses related topics without addressing the specific claim.
\end{itemize}

Critical prompt instructions ensure that mere discussion equals UNCLEAR, that mechanisms are recognized even without exact terminology, and that both Google snippets and page content are considered with better evidence prioritized.

\paragraph{Adaptive Credibility Filtering}

The search orchestrator implements adaptive credibility filtering as a key innovation for ensuring evidence quality. The algorithm operates in three phases:

\begin{enumerate}
    \item \textbf{Initial Batch}: Fetch 2× the desired limit to provide filtering headroom
    \item \textbf{Quality Check}: If $>$50\% of results are unreliable, fetch an additional batch to increase the pool of high-quality sources
    \item \textbf{Intelligent Filtering}: Sort all results by reliability score and select the top reliable sources, with a minimum guarantee ensuring at least some results are returned even if reliability is universally low
\end{enumerate}

Additional filtering mechanisms include stance filtering (removing unclear results if $>$50\% have definitive stances) and URL deduplication using hash sets to prevent duplicate sources across different queries for the same claim.

%---

\subsubsection{Output Generator}

The Output Generator synthesizes evidence into coherent verdicts with confidence levels, quality scoring, and timestamp mapping.

\paragraph{Two-Step Process}

The Output Generator employs a hybrid approach combining algorithmic evidence organization with LLM-based synthesis:

\textbf{Step 1: Build Evidence Bundle (Algorithmic)} --- The system groups evidence by stance (supports, refutes, mixed, unclear), deduplicates sources by URL, and sorts within each group by reliability rating (high first), then numerical score, with alphabetic tie-breaking for consistency. This deterministic organization ensures reproducible output ordering.

\textbf{Step 2: Generate Verdict (LLM)} --- Organized evidence is formatted into a structured prompt containing stance labels, source counts, reliability ratings, and evidence summaries. The system prompt instructs the LLM to synthesize concise verdicts, naming sources explicitly only when clarifying contrasting perspectives or when evidence directly conflicts. This reduces verbosity while maintaining attribution transparency.

\paragraph{Evidence Quality Score (Algorithmic)}

The quality score is calculated algorithmically without LLM involvement for consistency and speed. The scoring formula combines three components with different weights: a base score of 0.3 for having any evidence, an actionable stance bonus of up to 0.3 (scaled by the number of supports/refutes/mixed sources, saturating at 3 sources), and a reliability bonus of up to 0.4 (scaled by the number of high/medium reliability sources, saturating at 3 sources). This design prioritizes both actionable stances and source reliability, with the maximum achievable score of 1.0 indicating high-quality, decisive evidence from multiple reliable sources.

Table \ref{tab:quality-score} summarizes the quality score components.

\begin{table}[H]
\centering
\caption{Evidence quality score breakdown}
\label{tab:quality-score}
\begin{tabular}{lll}
\toprule
\textbf{Component} & \textbf{Weight} & \textbf{Criteria} \\
\midrule
Base & 0.3 & Having any evidence \\
Actionable & 0.3 & Up to 3 supports/refutes/mixed sources \\
Reliability & 0.4 & Up to 3 high/medium reliability sources \\
\textbf{Maximum} & \textbf{1.0} & \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Integrated Verdict Generation (within Level 1)}

Verdicts are generated immediately after each claim's evidence collection completes, within the same parallel execution context as the claim processing. This design choice eliminates the latency overhead of waiting for all claims to finish evidence collection before beginning verdict synthesis. Each claim's verdict generation executes as soon as its evidence is ready, allowing early-finishing claims to produce results while slower claims continue processing. After all parallel claim tasks complete, the reports are sorted by evidence quality score (descending) to prioritize high-confidence verdicts in the user interface.

\paragraph{Output Schema}

\begin{lstlisting}[language=Python, caption={Output generator schemas}]
VerdictConfidence = Literal["low", "medium", "high"]

class ClaimFactCheckReport(BaseModel):
    """Structured report ready to present to end users."""
    claim_text: str
    claim_confidence: float
    claim_category: str
    overall_stance: EvidenceStance
    verdict_confidence: VerdictConfidence
    verdict_summary: str
    evidence_by_stance: dict[EvidenceStance, list[EvidenceSourceSummary]]
    total_sources: int = Field(ge=0)
    evidence_quality_score: float = Field(ge=0.0, le=1.0)
    timestamp_hint: float | None = None
    timestamp_confidence: float | None = None
\end{lstlisting}

%-------------------------------------------------------
\subsection{LLM Configuration and Model Management}
%-------------------------------------------------------

\subsubsection{Model Abstraction Layer}

The system uses an enum-based model configuration with Pydantic validation for type-safe model management. Each model is defined through a \texttt{ModelConfig} schema containing provider name (OpenAI or Ollama), model identifier, input and output pricing per million tokens, and context window size. The \texttt{ModelChoice} enumeration defines available models as named constants (e.g., \texttt{OPENAI\_GPT4O\_MINI}, \texttt{OLLAMA\_QWEN3\_8B}), each associated with its configuration. This abstraction enables centralized model definitions, compile-time checking of model names, automatic price tracking for cost estimation, and easy addition of new models through enum extension.

\subsubsection{Model Instantiation with Caching}

Ollama model instances are cached using Python's \texttt{@lru\_cache} decorator to enable connection reuse across multiple agent invocations. Without caching, each agent run would create a new provider connection, incurring initialization overhead. The cache is unbounded (\texttt{maxsize=None}), ensuring that once a model is instantiated, subsequent requests reuse the existing connection. This optimization is particularly important for Ollama models where the provider connection establishes communication with the local inference server at \texttt{http://127.0.0.1:11434/v1}.

\subsubsection{Per-Component Model Configuration}

A centralized configuration file defines which model each component uses, enabling easy model swapping for experiments. By default, all components use \texttt{OPENAI\_GPT4O\_MINI} for consistency, but this can be overridden on a per-component basis. For example, the Evidence Extractor could be downgraded to a local Ollama model to reduce costs while keeping the Claim Extractor on GPT-4o-mini for quality. This granular control enables component-specific optimization, A/B testing of model choices, and precise cost-quality trade-off analysis.

\subsubsection{Common Model Settings}

All components use \texttt{temperature=0.0} for deterministic outputs, enabling reproducibility, consistency in structured outputs, and meaningful A/B comparisons. Table \ref{tab:model-settings} shows the token limits per component.

\begin{table}[H]
\centering
\caption{Component model settings}
\label{tab:model-settings}
\begin{tabular}{llll}
\toprule
\textbf{Component} & \textbf{Temperature} & \textbf{Max Tokens} & \textbf{Rationale} \\
\midrule
Claim Extractor & 0.0 & 1,200 & Deterministic, structured claims \\
Query Generator & 0.0 & 600 & Concise queries, no explanations \\
Evidence Extractor & 0.0 & 1,100 & Summary + key quote \\
Output Generator & 0.0 & 900 & Concise verdict synthesis \\
\bottomrule
\end{tabular}
\end{table}

%-------------------------------------------------------
\subsection{Latency Optimization Strategies}
%-------------------------------------------------------

The system implements multiple latency optimization strategies following established principles for LLM applications \citep{openai2024latency}. Research into LLM latency shows that prompt size, completion length, and model size are the dominant factors in inference time; reducing input and output token counts directly lowers compute and memory overhead \citep{graphsignal2024latency, openai2024latency}. Additional techniques such as defining clear output boundaries, setting token limits, and adjusting sampling temperature can reduce generated tokens, and caching prompts allows reuse of computations for identical prefixes \citep{openai2024latency}. Choosing smaller model weights yields faster inference speeds. These optimizations can be grouped into seven core principles: processing tokens faster, generating fewer tokens, using fewer input tokens, making fewer requests, parallelizing operations, reducing perceived wait time, and using classical methods where LLMs are not required \citep{openai2024latency}.

\subsubsection{Process Tokens Faster: Model Selection}

The default model (gpt-4o-mini) is selected for its balanced performance across speed, cost-effectiveness, and large context window (128K tokens). This model provides sufficient reasoning capabilities for fact-checking tasks while maintaining low latency and competitive pricing (\$0.15 per million input tokens, \$0.60 per million output tokens). For budget-conscious deployments or offline operation, local Ollama models (qwen3:8b, qwen3:4b) offer zero-cost inference at the expense of potential quality degradation. The modular model abstraction layer enables easy comparison of these trade-offs through experiment configuration.

\subsubsection{Generate Fewer Tokens: Output Constraints}

Each component has carefully tuned \texttt{max\_tokens} limits to minimize generation latency and API costs without sacrificing information quality. Claims are limited to approximately 40 words (sufficient for most factual assertions), context descriptions to 20 words (brief background), and evidence summaries to 1--2 sentences (key findings only). These constraints are enforced through explicit prompt instructions and validated against output token limits. By preventing verbose outputs, the system reduces both generation time and downstream processing costs for subsequent pipeline stages.

\subsubsection{Use Fewer Input Tokens: Content Trimming}

Input token counts are minimized through aggressive content trimming strategies. Web content is trimmed to 6,000--8,000 characters before being passed to the Evidence Extractor, removing excessive context while retaining the most relevant portions (typically the first several paragraphs of an article). Evidence prompts include only Google snippets and extracted page text, explicitly excluding raw HTML, JavaScript, CSS, and other non-content elements that would inflate token counts without improving extraction quality. This targeted trimming reduces LLM input costs by an order of magnitude compared to naive full-page submission.

\subsubsection{Make Fewer Requests: Combined Operations}

The pipeline minimizes LLM API calls by combining operations into single requests wherever possible. Each component makes exactly one LLM call per input unit (one call for claim extraction, one per claim for query generation, one per search result for evidence extraction, and one per claim for verdict synthesis), with no multi-turn conversations that would multiply request counts. The Query Generator produces all queries for a claim in a single batch call rather than generating queries iteratively. Similarly, the Output Generator synthesizes verdicts with all available evidence in a single call. This design reduces API overhead, improves latency, and simplifies cost tracking.

\subsubsection{Parallelize: 3-Level Async Architecture}

The system implements three levels of nested parallelization to maximize throughput while maintaining dependency ordering. This architecture enables the pipeline to process multiple claims, queries, and search results simultaneously, dramatically reducing total execution time compared to sequential processing.

The parallelization hierarchy operates as follows:

\begin{itemize}
    \item \textbf{Level 1 (Claims)}: After extracting N claims from the transcript, all claims are processed in parallel. Each claim independently proceeds through query generation, evidence search, and verdict generation. Critically, each claim's verdict is generated immediately after its evidence collection completes, rather than waiting for all claims to finish---this optimization reduces perceived latency by producing results progressively.

    \item \textbf{Level 2 (Queries per Claim)}: Within each claim's processing, the Query Generator produces M queries. These queries are executed in parallel, enabling simultaneous search across different query formulations (direct, alternative, source-seeking, contextual).

    \item \textbf{Level 3 (Search Results per Query)}: Within each query's execution, the Online Search component retrieves K results from Google. The four-step pipeline (reliability assessment, content fetching, and evidence extraction) runs in parallel for all K results, with each result processed independently.
\end{itemize}

This design achieves maximum theoretical parallelization of $N \times M \times K$ operations during the search phase, bounded only by system resources and API rate limits. The nesting structure means that at peak execution, the system may be processing dozens of parallel operations across all three levels simultaneously. Blocking I/O operations (Selenium WebDriver for content fetching, WHOIS for domain age lookup) are wrapped in \texttt{asyncio.to\_thread()} to avoid blocking the event loop, ensuring that CPU-bound and I/O-bound operations can execute concurrently.

\subsubsection{Real-Time Streaming}

Server-Sent Events (SSE) provide progressive updates as the pipeline executes, improving perceived responsiveness for users. The streaming endpoint emits progress updates at key milestones, ranging from 5\% (transcript extraction initiated) through 100\% (fact-checking complete). Importantly, extracted claims are streamed to the user interface immediately after the Claim Extractor completes, allowing users to preview claims and begin reviewing the video's assertions before the time-intensive search phase completes. This progressive disclosure pattern reduces perceived latency and enables users to provide early feedback or cancellation if the extracted claims are not aligned with their expectations.

\subsubsection{Classical Methods for Non-Reasoning Tasks}

Table \ref{tab:classical-methods} shows operations handled by classical algorithms rather than LLMs.

\begin{table}[H]
\centering
\caption{Operations using classical methods}
\label{tab:classical-methods}
\begin{tabular}{lll}
\toprule
\textbf{Operation} & \textbf{Method} & \textbf{Rationale} \\
\midrule
Reliability scoring & Rule-based + MBFC lookup & Faster, deterministic, no API cost \\
Claim localization & Fuzzy string matching & No LLM needed for text search \\
Evidence quality score & Algorithmic calculation & Consistent, fast, reproducible \\
URL deduplication & Hash set & O(1) lookup \\
Stance filtering & Threshold-based & Simple percentage check \\
\bottomrule
\end{tabular}
\end{table}

%-------------------------------------------------------
\subsection{Data Schemas and Structured Outputs}
%-------------------------------------------------------

\subsubsection{Pydantic AI Integration}

All LLM agents use Pydantic models as their \texttt{output\_type}, providing comprehensive type safety throughout the pipeline. This integration ensures automatic JSON parsing and validation of LLM outputs, graceful error handling with automatic retries on validation failures, full IDE support with autocomplete for all schema fields, and consistent serialization via \texttt{model\_dump()} for logging and persistence. The agent configuration includes the model selection, output schema type, runtime dependencies type, model settings (temperature, token limits), system prompt, and retry count. This declarative configuration style reduces boilerplate while maintaining explicit control over agent behavior.

\subsubsection{Schema Design Principles}

The schema design follows five guiding principles to balance flexibility, safety, and maintainability. First, schemas prefer flat structures over deeply nested objects to simplify validation and reduce complexity. Second, optional fields use Python's union type syntax (\texttt{str | None}) for clarity. Third, \texttt{Literal} types constrain string fields to predefined values (e.g., stance must be ``supports'', ``refutes'', ``mixed'', or ``unclear''). Fourth, all fields include descriptive names and \texttt{Field} descriptions for documentation and IDE hints. Fifth, numeric fields include validation constraints (e.g., \texttt{Field(ge=0.0, le=1.0)} for scores) to catch invalid data at runtime. These principles ensure robust data contracts across component boundaries.

%-------------------------------------------------------
\subsection{Evaluation Framework}
%-------------------------------------------------------

Evaluating fact-checking systems remains an open challenge because existing benchmarks are narrow and risk overfitting. Commentators have noted a ``benchmark crisis'' where canonical datasets no longer represent broad language understanding \citep{ruder2025evolving}. Practical evaluation strategies include multiple-choice benchmarks, verifiers, public leaderboards, and LLM judges, each with distinct trade-offs \citep{raschka2025evaluation}. Our evaluation framework therefore combines quantitative metrics with qualitative assessments using external datasets such as FEVER \citep{thorne2018fever} and FEVEROUS \citep{aly2021feverous}.

\subsubsection{Experiment Tracker (Singleton Pattern)}

The \texttt{ExperimentTracker} implements a singleton pattern with context manager support for global access during pipeline execution. This design enables any component to log metrics, timing, and LLM calls without explicit parameter passing. When initialized, the tracker creates a timestamped run directory, initializes data structures for timing, metrics, and LLM call logs, and registers itself as the current global tracker. The context manager protocol ensures automatic saving on exit and cleanup of the global reference.

A complementary \texttt{timer} context manager provides automatic timing instrumentation. When a code block executes within a timer context, the elapsed duration is automatically logged to the current tracker with a descriptive label. This declarative timing approach eliminates manual timing boilerplate and ensures consistent timing collection across all pipeline stages.

\subsubsection{Pydantic AI Monitoring (Monkey-Patching)}

The \texttt{@track\_pydantic(component\_name)} decorator enables automatic tracking of all LLM calls via runtime monkey-patching of \texttt{Agent.run()} and \texttt{Agent.run\_sync()}. Each call records component name, model, timestamp, latency, input prompt, estimated tokens, output, and calculated cost in USD.

\subsubsection{Metrics Aggregation}

The \texttt{metrics.json} file provides aggregated experiment metrics for quantitative analysis and comparison across runs. Metrics include total pipeline execution time, per-step timing breakdowns (transcript, claims, queries, search, output), total LLM call count, estimated total cost in USD, and success/failure status. This structured format enables automated analysis scripts to compare different parameter configurations, identify performance bottlenecks, and track cost-quality trade-offs across the experiment corpus.

\subsubsection{Output Directory Structure}

Each experiment run creates a timestamped directory containing structured artifacts for analysis. The directory includes \texttt{config.json} (run ID, parameters, model configuration), \texttt{llm\_calls.json} (all Pydantic AI calls with full input/output), \texttt{outputs.json} (final claims and fact-check reports), \texttt{metrics.json} (aggregated timing, cost, success status), and \texttt{transcript.txt} (original video transcript for reference). This organization enables reproducible analysis, post-hoc debugging of individual LLM calls, and batch processing of experiment results across parameter sweeps.

%-------------------------------------------------------
\subsection{Experiment Configuration and Runner}
%-------------------------------------------------------

\subsubsection{YAML Configuration}

The YAML configuration defines test videos and experiment parameters with automatic expansion:

\begin{lstlisting}[language=Python, caption={Example YAML configuration}]
videos:
  - id: "climate_what_scientists_say"
    url: "https://www.youtube.com/watch?v=OwqIy8Ikv-c"
    description: "Climate Change: What Do Scientists Say?"
    tags: ["climate", "climate_skepticism", "short", "selected"]

experiments:
  - name: "vary_claims"
    description: "OFAT: Impact of max_claims on quality/cost/time"
    max_claims: [1, 3, 5, 7, 10]  # Auto-expands to 5 experiments
    max_queries_per_claim: 2
    max_results_per_query: 3
    video_filter: ["selected"]
\end{lstlisting}

\subsubsection{Test Video Corpus}

The evaluation dataset includes 19 videos across domains: Climate (10 videos covering climate skepticism, renewable energy, CO2, Texas freeze), Technology (7 videos on 5G health concerns, AI job displacement), and Health (2 videos on EMF radiation, alkaline water). Selection criteria include verifiable factual claims, range of difficulty levels, mix of viewpoints, available English transcripts, and durations from 2--25 minutes.

\subsubsection{Experiment Types}

Three experiment types are supported:

\textbf{Baseline}: Standard configuration (\texttt{max\_claims=5}, \texttt{max\_queries=2}, \texttt{max\_results=3}).

\textbf{OFAT (One-Factor-At-A-Time) Analysis}: Sensitivity analysis varying one parameter while holding others constant---\texttt{vary\_claims} [1, 3, 5, 7, 10], \texttt{vary\_queries} [1, 2, 3, 4, 5], \texttt{vary\_results} [1, 2, 3, 5, 7].

\textbf{Strategic Combinations}: \texttt{minimal} (1/1/1 for fast prototyping), \texttt{deep} (3/4/5 for comprehensive evidence), \texttt{broad} (10/1/2 for coverage-focused).

\subsubsection{Experiment Runner CLI}

The experiment runner provides command-line options:

\begin{lstlisting}[language=bash, caption={Experiment runner CLI usage}]
factible-experiments run                    # Run all experiments
factible-experiments run --experiment vary_claims  # Specific group
factible-experiments run --video fossil_fuels      # Specific video
factible-experiments run --dry-run          # Preview without executing
factible-experiments analyze                # Analyze completed runs
\end{lstlisting}

%-------------------------------------------------------
\subsection{Analysis and Visualization Pipeline}
%-------------------------------------------------------

\subsubsection{Data Loading and Enrichment}

The analysis script loads all experiment runs into a pandas DataFrame, extracting configuration, metrics, and outputs from JSON files. Enrichment adds categorization including experiment type, OFAT parameter, and strategy type.

\subsubsection{Generated Visualizations}

The analysis pipeline generates visualizations including run summary overviews, component breakdowns showing latency and tokens by component, claims analysis with stance distribution, pipeline timing tables, scalability plots (tokens vs. time), OFAT sensitivity analysis, and strategy comparisons.

%-------------------------------------------------------
\subsection{API Layer and Real-Time Streaming}
%-------------------------------------------------------

\subsubsection{FastAPI Setup}

The API uses FastAPI with CORS middleware for frontend development and API versioning:

\begin{lstlisting}[language=Python, caption={FastAPI setup}]
app = FastAPI(
    title="Factible API",
    version="0.1.0",
    description="Automated fact-checking for YouTube videos"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://localhost:5173"],
    allow_methods=["*"], allow_headers=["*"],
)

app.include_router(fact_check_router, prefix="/api/v1")
\end{lstlisting}

\subsubsection{Streaming Endpoint with SSE}

The streaming endpoint (\texttt{POST /api/v1/fact-check/stream}) returns a \texttt{StreamingResponse} with \texttt{text/event-stream} media type. A callback-based progress handler collects updates into an asyncio queue, which are then yielded as SSE events.

\subsubsection{Progress Events}

Table \ref{tab:progress-events} shows the progress events emitted during pipeline execution.

\begin{table}[H]
\centering
\caption{SSE progress events}
\label{tab:progress-events}
\begin{tabular}{llll}
\toprule
\textbf{Stage} & \textbf{Progress} & \textbf{Event Name} & \textbf{Data Payload} \\
\midrule
1 & 5\% & transcript\_extraction & -- \\
2 & 15\% & transcript\_complete & transcript\_length \\
3 & 20\% & claim\_extraction & -- \\
4 & 35\% & claims\_extracted & claims[], total\_claims \\
5 & 90\% & generating\_report & -- \\
6 & 100\% & complete & result (full output) \\
Error & 100\% & error & error message \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Request/Response Schemas}

\begin{lstlisting}[language=Python, caption={API request/response schemas}]
class FactCheckRequest(BaseModel):
    """Request schema for fact-checking a YouTube video."""
    video_url: HttpUrl = Field(..., description="YouTube video URL")
    experiment_name: str = Field(default="default")
    max_claims: int | None = Field(default=5, ge=1, le=20)
    max_queries_per_claim: int = Field(default=2, ge=1, le=5)
    max_results_per_query: int = Field(default=3, ge=1, le=10)

class ProgressUpdate(BaseModel):
    """SSE response schema for progress updates."""
    step: str
    message: str
    progress: int  # 0-100
    data: dict | None
\end{lstlisting}

%-------------------------------------------------------
\subsection{Code Quality and Engineering Practices}
%-------------------------------------------------------

\subsubsection{Type Safety}

The codebase enforces strong type safety through multiple mechanisms. All functions use Python 3.12 type annotations with modern union syntax (\texttt{str | None}) and generic types (\texttt{list[T]}), enabling static analysis and IDE assistance. Pydantic models provide runtime validation for all data structures crossing component boundaries, catching type mismatches and constraint violations at execution time. Additionally, static type checking via mypy with strict mode configuration runs in continuous integration, preventing type errors from reaching production. This layered approach combines the benefits of gradual typing with runtime safety nets.

\subsubsection{Logging}

Module-level loggers provide structured logging throughout the pipeline execution. Each component initializes a logger using Python's standard logging module, configured with consistent formatting for timestamp, level, and message. Log messages use semantic levels (INFO for normal progress, WARNING for recoverable issues, ERROR for failures) and include contextual information such as claim indices, query counts, and reliability scores. The logging output supports both real-time monitoring during development and post-hoc analysis from experiment run files.

\subsubsection{Error Handling Patterns}

Table \ref{tab:error-handling} summarizes error handling strategies.

\begin{table}[H]
\centering
\caption{Error handling patterns}
\label{tab:error-handling}
\begin{tabular}{lll}
\toprule
\textbf{Error Type} & \textbf{Handling} & \textbf{Fallback} \\
\midrule
No transcript & Return empty claims & Continue with empty pipeline \\
LLM failure & Retry 3x & Return empty/unclear \\
Search API failure & Log and continue & Empty results for query \\
Scraping failure & Fall back to snippet & Use Google snippet \\
Reliability failure & Default to ``unknown'' & Conservative rating \\
Verdict failure & Error message in summary & Unclear stance \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Configuration Management}

Configuration is managed through multiple layers optimized for different use cases. Sensitive credentials (API keys for Serper, OpenAI, Webshare) are stored in environment variables loaded from a \texttt{.env} file, following security best practices by keeping secrets out of version control. Experiment parameters (max claims, queries per claim, results per query, video corpus) are defined in YAML files for human readability and version control tracking. Model settings (temperature, token limits, retry counts) are specified as Python constants for type safety and inline documentation. Finally, API server configuration (CORS origins, port, host) uses Pydantic Settings with automatic environment variable override support. This layered approach balances security, flexibility, and maintainability.

\subsubsection{Pre-commit Hooks}

Code quality is enforced automatically through pre-commit hooks that run before each git commit. The ruff linter performs fast Python linting with automatic fixes for common issues (unused imports, trailing whitespace, line length violations) and consistent code formatting following PEP 8 style guidelines. The mypy static type checker validates type annotations across the entire codebase in strict mode, catching potential type errors before runtime. Additional standard hooks validate YAML and TOML file syntax, preventing configuration errors. These automated checks ensure code quality standards are maintained consistently across all contributions without requiring manual review for style issues.

%-------------------------------------------------------
\subsection{Design Patterns and Software Engineering}
%-------------------------------------------------------

\subsubsection{Patterns Summary}

Table \ref{tab:design-patterns} summarizes the design patterns employed.

\begin{table}[H]
\centering
\caption{Design patterns used in the implementation}
\label{tab:design-patterns}
\begin{tabular}{lll}
\toprule
\textbf{Pattern} & \textbf{Usage} & \textbf{Benefit} \\
\midrule
Singleton & ExperimentTracker & Global access during pipeline \\
Context Manager & Tracker, Timer, Fetcher & Resource cleanup, timing \\
Decorator & @track\_pydantic & Cross-cutting concerns \\
Monkey-Patching & Agent monitoring & Non-invasive instrumentation \\
Factory & get\_model() & Model instantiation \\
Strategy & Adaptive filtering & Runtime algorithm selection \\
Observer & Progress callbacks & Decoupled progress reporting \\
Builder & Evidence bundle & Complex object construction \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Async Patterns}

The implementation employs four asynchronous programming patterns to maximize concurrency. First, parallel independent tasks use \texttt{asyncio.gather()} to execute multiple coroutines concurrently and collect their results. Second, thread pools handle blocking I/O operations (Selenium, WHOIS) via \texttt{asyncio.to\_thread()}, preventing them from blocking the event loop. Third, queue-based producer/consumer patterns enable streaming results from background workers to the API endpoint. Fourth, background task spawning via \texttt{asyncio.create\_task()} allows fire-and-forget operations that don't block the main execution flow. These patterns combine to create a highly concurrent pipeline that efficiently utilizes system resources.

\subsubsection{Key Engineering Decisions}

Several architectural decisions significantly shaped the implementation. Pydantic AI was chosen over LangChain for its simpler API surface and native support for type-safe structured outputs through Pydantic models. Selenium was selected over lightweight HTTP libraries like requests specifically to handle JavaScript-rendered content that requires browser execution. The MBFC dataset provides authoritative reliability ratings based on established fact-checking methodology, offering more credible assessments than simple domain heuristics alone. The \texttt{asyncio.to\_thread} pattern enables non-blocking integration of synchronous libraries (Selenium, WHOIS) within the async pipeline. Temperature 0.0 ensures reproducible experiments by eliminating sampling randomness. Finally, token estimation using character count divided by 4 provides a computationally cheap approximation for cost tracking without requiring exact tokenization.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SECTION 2: RESULTS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}

% TODO: This section will be completed with experimental results
% including system performance, LLM comparisons, case studies,
% and qualitative analysis.

\textit{[Results section to be completed with experimental data]}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BIBLIOGRAPHY
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
